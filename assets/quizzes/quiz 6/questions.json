[
    {
        "question_number": "Question 1",
        "question": "You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their\n          own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting a\n          ProvisionedThroughputExceededException exception. You have been contacted to help and upon\n          analysis, you notice that messages are being sent one by one at a high rate.\nWhich of the following options will help with the exception while keeping costs at a minimum?",
        "skipped": true,
        "choices": [
            "Use Exponential Backoff",
            "Increase the number of shards",
            "Use batch messages",
            "Decrease the Stream retention duration"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use batch messages</strong></p>\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS\n            can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website\n            clickstreams, database event streams, financial transactions, social media feeds, IT logs, and\n            location-tracking events. The data collected is available in milliseconds to enable real-time analytics use\n            cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n<p>Amazon Kinesis Data Streams Overview:\n            <img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\"/>\n            via - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a>\n</p>\n<p>When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic\n            PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application\n            must batch records and implement parallel HTTP requests. This will increase the efficiency overall and\n            ensure you are optimally using the shards.</p>\n<p>Incorrect options:</p>\n<p><strong>Use Exponential Backoff</strong> - While this may help in the short term, as soon as the request\n            rate increases, you will see the <code>ProvisionedThroughputExceededException</code> exception again.</p>\n<p><strong>Increase the number of shards</strong> - Increasing shards could be a short term fix but will\n            substantially increase the cost, so this option is ruled out.</p>\n<p><strong>Decrease the Stream retention duration</strong> - This operation may result in data loss and won't\n            help with the exceptions, so this option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/\">https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 2",
        "question": "A company is looking for a technology that allows its mobile app users to connect through a Google login and\n          have the capability to turn on AWS Multi-Factor Authentication (AWS MFA) to have maximum security. Ideally,\n          the solution should be fully managed by AWS.\nWhich technology do you recommend for managing the users' accounts?",
        "skipped": true,
        "choices": [
            "AWS Identity and Access Management (AWS IAM)",
            "Amazon Cognito",
            "Write an AWS Lambda function with Auth0 3rd party integration",
            "Enable the AWS Google Login Service"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon Cognito</strong></p>\n<p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly\n            and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers,\n            such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0. Here Cognito is the\n            best technology choice for managing mobile user accounts.</p>\n<p>Amazon Cognito Features:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q30-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/cognito/details/\">https://aws.amazon.com/cognito/details/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Write an AWS Lambda function with Auth0 3rd party integration</strong> - AWS Lambda lets you run\n            code without provisioning or managing servers. You pay only for the compute time you consume. Using Lambda\n            would require code maintenance for user management functionality, therefore this option is ruled out.</p>\n<p><strong>AWS Identity and Access Management (AWS IAM)</strong> - AWS Identity and Access Management (IAM)\n            enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage\n            AWS users and groups, and use permissions to allow and deny their access to AWS resources. IAM cannot be\n            used to manage mobile user accounts.</p>\n<p><strong>Enable the AWS Google Login Service</strong> - There is no such thing as AWS Google Login service.\n            This option is just added as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/cognito/\">https://aws.amazon.com/cognito/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 3",
        "question": "An Internet-of-Things (IoT) company is planning on distributing a master sensor in people's homes to measure\n          the key metrics from its smart devices. In order to provide adjustment commands for these devices, the company\n          would like to have a streaming system that supports ordered data based on the sensor's key, and also sustains\n          high throughput messages (thousands of messages per second).\nAs a solutions architect, which of the following AWS services would you recommend for this use-case?",
        "skipped": true,
        "choices": [
            "Amazon Simple Queue Service (Amazon SQS)",
            "Amazon Simple Notification Service (Amazon SNS)",
            "AWS Lambda",
            "Amazon Kinesis Data Streams"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon Kinesis Data Streams</strong></p>\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS\n            can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website\n            clickstreams, database event streams, financial transactions, social media feeds, IT logs, and\n            location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without\n            limits via increasing the number of shards within a data stream.</p>\n<p>However, there are certain limits you should keep in mind while using Amazon Kinesis Data Streams:</p>\n<p>A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).</p>\n<p>The maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte\n            (MB).\n            Each shard can support up to 1000 PUT records per second.</p>\n<p>Kinesis is the right answer here, as by providing a partition key in your message, you can guarantee\n            ordered messages for a specific sensor, even if your stream is sharded.</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully\n            managed message queuing service that enables you to decouple and scale microservices, distributed systems,\n            and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and\n            operating message-oriented middleware, and empowers developers to focus on differentiating work. Using\n            Amazon SQS, you can send, store, and receive messages between software components at any volume, without\n            losing messages or requiring other services to be available. Kinesis is better for streaming data since\n            queues aren't meant for real-time streaming of data.</p>\n<p><strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service\n            (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables\n            you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics\n            for high-throughput, push-based, many-to-many messaging. SNS cannot be used for data streaming. Therefore\n            this option is not the best fit for the given use-case.</p>\n<p><strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. It\n            cannot be used for production-grade serverless log analytics. Lambda isn't meant to retain data either.\n            Therefore this option is not the best fit for the given use-case.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 4",
        "question": "Reporters at a news agency upload/download video files (about 500 megabytes each) to/from an Amazon S3 bucket\n          as part of their daily work. As the agency has started offices in remote locations, it has resulted in poor\n          latency for uploading and accessing data to/from the given Amazon S3 bucket. The agency wants to continue\n          using a serverless storage solution such as Amazon S3 but wants to improve the performance.\nAs a solutions architect, which of the following solutions do you propose to address this issue? (Select two)",
        "skipped": true,
        "choices": [
            "Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily\n                    job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances",
            "Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up\n                    uploads as well as downloads for the video files",
            "Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to\n                    Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC\n                    peering connection",
            "Create new Amazon S3 buckets in every region where the agency has a remote office, so that each\n                    office can maintain its storage for the media assets",
            "Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads\n                    as well as downloads for the video files"
        ],
        "correct_answer_indices": [
            1,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Use Amazon CloudFront distribution with origin as the Amazon S3 bucket. This would speed up uploads\n              as well as downloads for the video files</strong></p>\n<p>Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos,\n            applications, and APIs to customers globally with low latency, high transfer speeds, within a\n            developer-friendly environment.\n            When an object from Amazon S3 that is set up with Amazon CloudFront CDN is requested, the request would come\n            through the Edge Location transfer paths only for the first request. Thereafter, it would be served from the\n            nearest edge location to the users until it expires. So in this way, you can speed up uploads as well as\n            downloads for the video files.</p>\n<p>Following is a good reference blog for a deep-dive:</p>\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/\">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a>\n</p>\n<p><strong>Enable Amazon S3 Transfer Acceleration (Amazon S3TA) for the Amazon S3 bucket. This would speed up\n              uploads as well as downloads for the video files</strong></p>\n<p>Amazon S3 Transfer Acceleration (Amazon S3TA) can speed up content transfers to and from Amazon S3 by as\n            much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of\n            Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is\n            routed to Amazon S3 over an optimized network path. So this option is also correct.</p>\n<p>Amazon S3TA:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q21-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/s3/transfer-acceleration/\">https://aws.amazon.com/s3/transfer-acceleration/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Create new Amazon S3 buckets in every region where the agency has a remote office, so that each\n              office can maintain its storage for the media assets</strong> - Creating new Amazon S3 buckets in every\n            region is not an option, since the agency maintains centralized storage. Hence this option is incorrect.</p>\n<p><strong>Move Amazon S3 data into Amazon Elastic File System (Amazon EFS) created in a US region, connect to\n              Amazon EFS file system from Amazon EC2 instances in other AWS regions using an inter-region VPC peering\n              connection</strong></p>\n<p><strong>Spin up Amazon EC2 instances in each region where the agency has a remote office. Create a daily\n              job to transfer Amazon S3 data into Amazon EBS volumes attached to the Amazon EC2 instances</strong></p>\n<p>Both these options using Amazon EC2 instances are not correct for the given use-case, as the agency wants a\n            serverless storage solution.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/s3/transfer-acceleration/\">https://aws.amazon.com/s3/transfer-acceleration/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/\">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 5",
        "question": "A medical devices company uses Amazon S3 buckets to store critical data. Hundreds of buckets are used to keep\n          the data segregated and well organized. Recently, the development team noticed that the lifecycle policies on\n          the Amazon S3 buckets have not been applied optimally, resulting in higher costs.\nAs a Solutions Architect, can you recommend a solution to reduce storage costs on Amazon S3 while keeping the\n          IT team's involvement to a minimum?",
        "skipped": true,
        "choices": [
            "Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data\n                    on-premises",
            "Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage",
            "Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs",
            "Configure Amazon EFS to provide a fast, cost-effective and sharable storage service"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Amazon S3 Intelligent-Tiering storage class to optimize the Amazon S3 storage costs</strong>\n</p>\n<p>The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data\n            to the most cost-effective access tier, without performance impact or operational overhead. It works by\n            storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost\n            tier that is optimized for infrequent access.</p>\n<p>For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the\n            objects in Amazon S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive\n            days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is\n            automatically moved back to the frequent access tier. There are no retrieval fees when using the Amazon S3\n            Intelligent-Tiering storage class, and no additional tiering fees when objects are moved between access\n            tiers. It is the ideal storage class for long-lived data with access patterns that are unknown or\n            unpredictable.</p>\n<p>Amazon S3 Storage Classes can be configured at the object level and a single bucket can contain objects\n            stored in Amazon S3 Standard, Amazon S3 Intelligent-Tiering, Amazon S3 Standard-IA, and Amazon S3 One\n            Zone-IA. You can upload objects directly to Amazon S3 Intelligent-Tiering, or use S3 Lifecycle policies to\n            transfer objects from Amazon S3 Standard and Amazon S3 Standard-IA to Amazon S3 Intelligent-Tiering. You can\n            also archive objects from Amazon S3 Intelligent-Tiering to Amazon S3 Glacier.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure Amazon EFS to provide a fast, cost-effective and sharable storage service</strong> -\n            Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system\n            for use with AWS Cloud services and on-premises resources. Amazon EFS offers sharable service, unlike Amazon\n            Elastic Block Storage (EBS) that cannot be shared by instances. Amazon EFS is costlier than storing data in\n            Amazon S3. Also, Amazon EFS needs an Amazon EC2 instance or an AWS Direct Connect network connection. Hence,\n            this is not the correct option.</p>\n<p><strong>Use Amazon S3 One Zone-Infrequent Access, to reduce the costs on Amazon S3 storage</strong> -\n            Amazon S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed.\n            Unlike other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs),\n            Amazon S3 One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3\n            One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not\n            require the availability and resilience of Amazon S3 Standard or Amazon S3 Standard-IA. Not a right option,\n            since data stored is business-critical and cannot be risked by using Amazon S3 One Zone-IA.</p>\n<p><strong>Use Amazon S3 Outposts storage class to reduce the costs on Amazon S3 storage by storing the data\n              on-premises</strong> - This is a distractor as Amazon S3 on Outposts (S3 Outposts) delivers object storage\n            to your on-premises AWS Outposts environment. It is used in conjunction with AWS Outposts and has no\n            relevance to the current use case.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 6",
        "question": "A company's cloud architect has set up a solution that uses Amazon Route 53 to configure the DNS records for\n          the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a\n          solution where users will be directed to a static error page, configured as a backup, in case of\n          unavailability of the primary website.\nWhich configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
        "skipped": true,
        "choices": [
            "Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the\n                    error page to be displayed. In case of primary failure, the requests get routed to the error page",
            "Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health\n                    check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted\n                    to a static error page, hosted on Amazon S3 bucket",
            "Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health\n                    check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted\n                    to a static error page, hosted on Amazon S3 bucket",
            "Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket\n                    that holds the error page to be displayed"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up Amazon Route 53 active-passive type of failover routing policy. If Amazon Route 53 health\n              check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a\n              static error page, hosted on Amazon S3 bucket</strong></p>\n<p>Use an active-passive failover configuration when you want a primary resource or group of resources to be\n            available the majority of the time and you want a secondary resource or group of resources to be on standby\n            in case all the primary resources become unavailable. When responding to queries, Amazon Route 53 includes\n            only healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only\n            the healthy secondary resources in response to DNS queries.</p>\n<p>Incorrect options:</p>\n<p><strong>Set up Amazon Route 53 active-active type of failover routing policy. If Amazon Route 53 health\n              check determines the Application Load Balancer endpoint as unhealthy, the traffic will be diverted to a\n              static error page, hosted on Amazon S3 bucket</strong> - This option has been added as a distractor as\n            there is no such thing as an active-active failover routing policy in Amazon Route 53. You can configure\n            active-active failover using any routing policy (or combination of routing policies) other than failover\n            routing policy and you configure active-passive failover only using the failover routing policy. In\n            active-active failover configuration, all the records that have the same name, the same type (such as A or\n            AAAA), and the same routing policy (such as weighted or latency) are active unless Amazon Route 53 considers\n            them unhealthy. Amazon Route 53 can respond to a DNS query using any healthy record.</p>\n<p><strong>Use Amazon Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket\n              that holds the error page to be displayed</strong> - If your application is hosted in multiple AWS\n            Regions, you can improve performance for your users by serving their requests from the AWS Region that\n            provides the lowest latency - this is Latency-based routing and is not helpful for the current use case.</p>\n<p><strong>Use Amazon Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the\n              error page to be displayed. In case of primary failure, the requests get routed to the error page</strong>\n            - Weighted routing lets you associate multiple resources with a single domain name (example.com) or\n            subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful\n            for a variety of purposes, including load balancing and testing new versions of the software. This is not\n            useful for the current use case.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 7",
        "question": "The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database\n          requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect -\n          Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective.\n        \nWhich of the following features will help you in disaster recovery of the database? (Select two)",
        "skipped": true,
        "choices": [
            "Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage",
            "Use the database cloning feature of the Amazon RDS Database cluster",
            "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups\n                    across multiple Regions",
            "Use cross-Region Read Replicas",
            "Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in\n                    a single AWS Region"
        ],
        "correct_answer_indices": [
            2,
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Use cross-Region Read Replicas</strong></p>\n<p>In addition to using Read Replicas to reduce the load on your source database instance, you can also use\n            Read Replicas to implement a DR solution for your production DB environment. If the source DB instance\n            fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in\n            a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get\n            back up and running if you experience a regional availability issue.</p>\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups\n              across multiple Regions</strong></p>\n<p>Amazon RDS provides high availability and failover support for database instances using Multi-AZ\n            deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ\n            deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.</p>\n<p>The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance.\n            Amazon RDS will back up your database and transaction logs and store both for a user-specified retention\n            period. If it’s a Multi-AZ configuration, backups occur on standby to reduce the I/O impact on the primary.\n            Amazon RDS supports Cross-Region Automated Backups. Manual snapshots and Read Replicas are also supported\n            across multiple Regions.</p>\n<p>Incorrect options:</p>\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in\n              a single AWS Region</strong> - This is an incorrect statement. Automated backups can be created across AWS\n            Regions.</p>\n<p><strong>Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</strong> -\n            Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable,\n            and consistent I/O performance. This storage type enhances the performance of the RDS database, but this\n            isn't a disaster recovery option.</p>\n<p><strong>Use the database cloning feature of the Amazon RDS Database cluster</strong> - This option has been\n            added as a distractor. Database cloning is only available for Amazon Aurora and not for Amazon RDS.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/rds/features/\">https://aws.amazon.com/rds/features/</a></p>\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/\">https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 8",
        "question": "A DevOps engineer at an IT company was recently added to the admin group of the company's AWS account. The\n          AdministratorAccess managed policy is attached to this group.\nCan you identify the AWS tasks that the DevOps engineer CANNOT perform even though he has full Administrator\n          privileges (Select two)?",
        "skipped": true,
        "choices": [
            "Configure an Amazon S3 bucket to enable AWS Multi-Factor Authentication (AWS MFA) delete",
            "Delete an Amazon S3 bucket from the production environment",
            "Change the password for his own IAM user account",
            "Close the company's AWS account",
            "Delete the IAM user for his manager"
        ],
        "correct_answer_indices": [
            0,
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Configure an Amazon S3 bucket to enable AWS Multi-Factor Authentication (AWS MFA) delete</strong>\n</p>\n<p><strong>Close the company's AWS account</strong></p>\n<p>An IAM user with full administrator access can perform almost all AWS tasks except a few tasks designated\n            only for the root account user. Some of the AWS tasks that only a root account user can do are as follows:\n            change account name or root password or root email address, change AWS support plan, close AWS account,\n            enable AWS Multi-Factor Authentication (AWS MFA) on S3 bucket delete, create Cloudfront key pair, register\n            for GovCloud. Even though the DevOps engineer is part of the admin group, he cannot configure an Amazon S3\n            bucket to enable AWS MFA delete or close the company's AWS account.</p>\n<p>Incorrect Options:</p>\n<p><strong>Delete the IAM user for his manager</strong></p>\n<p><strong>Delete an Amazon S3 bucket from the production environment</strong></p>\n<p><strong>[@@-E</strong></p>\n<p>The DevOps engineer is part of the admin group, so he can delete any IAM user, delete the Amazon S3 bucket,\n            and change the password for his own IAM user account.</p>\n<p>For the complete list of AWS tasks that require AWS account root user credentials, please review this\n            reference link:</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/aws_tasks-that-require-root.html\">https://docs.aws.amazon.com/general/latest/gr/aws_tasks-that-require-root.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 9",
        "question": "A company needs a massive PostgreSQL database and the engineering team would like to retain control over\n          managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team\n          wants to install the database on an Amazon EC2 instance with the optimal storage type on the attached Amazon\n          EBS volume.\nAs a solutions architect, which of the following configurations would you suggest to the engineering team?",
        "skipped": true,
        "choices": [
            "Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type",
            "Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type",
            "Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type",
            "Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type</strong></p>\n<p>Amazon EBS provides the following volume types, which differ in performance characteristics and price so\n            that you can tailor your storage performance and cost to the needs of your applications.</p>\n<p>The volumes types fall into two categories:</p>\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with\n            small I/O size, where the dominant performance attribute is IOPS</p>\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better\n            performance measure than IOPS</p>\n<p>Provision IOPS type supports critical business applications that require sustained IOPS performance, or\n            more than 16,000 IOPS or 250 MiB/s of throughput per volume. Examples are large database workloads, such as:\n            MongoDB\n            Cassandra\n            Microsoft SQL Server\n            MySQL\n            PostgreSQL\n            Oracle</p>\n<p>Therefore, Amazon EC2 with Amazon EBS volume of Provisioned IOPS SSD (io1) type is the right fit for the\n            given use-case.</p>\n<p>Please see this detailed overview of the volume types for Amazon EBS volumes.\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q22-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon EC2 with Amazon EBS volume of General Purpose SSD (gp2) type</strong></p>\n<p><strong>Amazon EC2 with Amazon EBS volume of Throughput Optimized HDD (st1) type</strong></p>\n<p><strong>Amazon EC2 with Amazon EBS volume of cold HDD (sc1) type</strong></p>\n<p>Per the explanation in the detailed overview provided above, these three options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 10",
        "question": "A gaming company is doing pre-launch testing for its new product. The company runs its production database on\n          an Aurora MySQL DB cluster and the performance testing team wants access to multiple test databases that must\n          be re-created from production data. The company has hired you as an AWS Certified Solutions Architect -\n          Associate to deploy a solution to create these test databases quickly with the LEAST required effort.\nWhat would you suggest to address this use case?",
        "skipped": true,
        "choices": [
            "Take a backup of the Aurora MySQL database instance using the mysqldump utility, create multiple\n                    new test database instances and restore each test database from the backup",
            "Set up binlog replication in the Aurora MySQL database instance to create multiple new test\n                    database instances",
            "Use database cloning to create multiple clones of the production database and use each clone as a\n                    test database",
            "Enable database Backtracking on the production database and let the testing team use the production\n                    database"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use database cloning to create multiple clones of the production database and use each clone as a\n              test database</strong></p>\n<p>You can quickly create clones of an Aurora DB by using the database cloning feature. In addition, database\n            cloning uses a copy-on-write protocol, in which data is copied only at the time the data changes, either on\n            the source database or the clone database. Cloning is much faster than a manual snapshot of the DB cluster.\n          </p>\n<p>For the given use case, the most optimal solution is to clone the DB cluster. This would allow the\n            performance testing team to have quick access to the production data in an isolated way. The team can\n            iterate over the various test phases by deleting existing test databases and then cloning the production DB\n            to create new test databases.</p>\n<p>You cannot clone databases across AWS regions. The clone databases must be created in the same region as\n            the source databases. Currently, you are limited to 15 clones based on a copy, including clones based on\n            other clones. After that, only copies can be created. However, each copy can also have up to 15 clones.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q59-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a>\n</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q59-i2.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Enable database Backtracking on the production database and let the testing team use the production\n              database</strong> - Using Backtracking, you can \"rewind\" the DB cluster to any time you specify. One of\n            the major advantages of backtracking is that it can rewind the DB cluster much faster compared to restoring\n            a DB cluster via point-in-time restore (PITR) or via a manual DB cluster snapshot, which can take hours.\n            Backtracking a DB cluster doesn't require a new DB cluster and rewinds the DB cluster in minutes.</p>\n<p>However, as the given use-case is around pre-release testing, it does not make sense to use production DB\n            itself for testing even if backtracking is enabled. The right solution is to use clones of the production DB\n            for testing.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q59-i3.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html</a>\n</p>\n<p><strong>Take a backup of the Aurora MySQL database instance using the mysqldump utility, create multiple\n              new test database instances and restore each test database from the backup</strong> - As the use-case\n            mandates the least effort for database administration, therefore this option is not correct since using the\n            mysqldump utility requires several manual steps to take a backup of a DB and restore into another DB.</p>\n<p><strong>Set up binlog replication in the Aurora MySQL database instance to create multiple new test\n              database instances</strong> - As the use-case mandates the least effort for database administration,\n            therefore this option is not correct since using the binlog replication requires several steps such as\n            creating a snapshot of your replication source, loading the snapshot into your replica target, etc.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.MySQL.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.MySQL.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 11",
        "question": "The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a\n          single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster\n          are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing\n          high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the\n          Aurora DB.\nWhich of the following steps would you combine to address the given scenario? (Select two)",
        "skipped": true,
        "choices": [
            "Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed\n                    during traffic spikes",
            "Create a replica Aurora instance in another Availability Zone to improve the availability as the\n                    replica can serve as a failover target",
            "Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon\n                    Aurora cluster",
            "Handle all read operations for your application by connecting to the reader endpoint of the Amazon\n                    Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora\n                    replica",
            "Create a standby Aurora instance in another Availability Zone to improve the availability as the\n                    standby can serve as a failover target"
        ],
        "correct_answer_indices": [
            1,
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Handle all read operations for your application by connecting to the reader endpoint of the Amazon\n              Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora\n              replica</strong></p>\n<p>When you create a second, third, and so on DB instance in an Aurora-provisioned DB cluster, Aurora\n            automatically sets up replication from the writer DB instance to all the other DB instances. These other DB\n            instances are read-only and are known as Aurora Replicas.</p>\n<p>Aurora Replicas have two main purposes. You can issue queries to them to scale the read operations for your\n            application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can\n            spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora\n            Replicas also help to increase availability. If the writer instance in a cluster becomes unavailable, Aurora\n            automatically promotes one of the reader instances to take its place as the new writer.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q3-i2.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a>\n</p>\n<p><strong>Create a replica Aurora instance in another Availability Zone to improve the availability as the\n              replica can serve as a failover target</strong></p>\n<p>If the primary instance in a DB cluster using single-master replication fails, Aurora automatically fails\n            over to a new primary instance in one of two ways:</p>\n<p>By promoting an existing Aurora Replica to the new primary instance\n            By creating a new primary instance</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q3-i3.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Create a standby Aurora instance in another Availability Zone to improve the availability as the\n              standby can serve as a failover target</strong> - There are no standby instances in Aurora. Aurora\n            performs an automatic failover to a read replica when a problem is detected. So this option is incorrect.\n          </p>\n<p>Read replicas, Multi-AZ deployments, and multi-region deployments:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q3-i4.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a>\n</p>\n<p><strong>Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed\n              during traffic spikes</strong> - Increasing the concurrency of the AWS Lambda function would not resolve\n            the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption\n            for the Aurora instance. This option has been added as a distractor.</p>\n<p><strong>Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon\n              Aurora cluster</strong> - Using Amazon EC2 instances behind an Application Load Balancer would not resolve\n            the issue since the bottleneck is at the database layer, as exhibited by the high CPU and memory consumption\n            for the Aurora instance. This option has been added as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 12",
        "question": "Which of the following is true regarding cross-zone load balancing as seen in Application Load Balancer\n          versus Network Load Balancer?",
        "skipped": true,
        "choices": [
            "By default, cross-zone load balancing is disabled for Application Load Balancer and enabled for\n                    Network Load Balancer",
            "By default, cross-zone load balancing is disabled for both Application Load Balancer and Network\n                    Load Balancer",
            "By default, cross-zone load balancing is enabled for both Application Load Balancer and Network\n                    Load Balancer",
            "By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for\n                    Network Load Balancer"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for\n              Network Load Balancer</strong></p>\n<p>By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network\n            Load Balancer.\n            When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered\n            targets in all the enabled Availability Zones.\n            When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the\n            registered targets in its Availability Zone.</p>\n<p>How cross-zone load balancing works:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q4-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a>\n</p>\n<p>Incorrect Options:</p>\n<p><strong>By default, cross-zone load balancing is disabled for both Application Load Balancer and Network\n              Load Balancer</strong></p>\n<p><strong>By default, cross-zone load balancing is enabled for both Application Load Balancer and Network\n              Load Balancer</strong></p>\n<p><strong>By default, cross-zone load balancing is disabled for Application Load Balancer and enabled for\n              Network Load Balancer</strong></p>\n<p>Per the default cross-zone load balancing settings described earlier in the explanation, these three\n            options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 13",
        "question": "A junior developer is learning to build websites using HTML, CSS, and JavaScript. He has created a static\n          website and then deployed it on Amazon S3. Now he can't seem to figure out the endpoint for his super cool\n          website.\nAs a solutions architect, can you help him figure out the allowed formats for the Amazon S3 website\n          endpoints? (Select two)",
        "skipped": true,
        "choices": [
            "http://s3-website-Region.bucket-name.amazonaws.com",
            "http://s3-website.Region.bucket-name.amazonaws.com",
            "http://bucket-name.Region.s3-website.amazonaws.com",
            "http://bucket-name.s3-website-Region.amazonaws.com",
            "http://bucket-name.s3-website.Region.amazonaws.com"
        ],
        "correct_answer_indices": [
            3,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>http://bucket-name.s3-website.Region.amazonaws.com</strong></p>\n<p><strong>http://bucket-name.s3-website-Region.amazonaws.com</strong></p>\n<p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then\n            upload your website content to the bucket. When you configure a bucket as a static website, you enable\n            static website hosting, set permissions, and add an index document. Depending on your website requirements,\n            you can also configure other options, including redirects, web traffic logging, and custom error documents.\n          </p>\n<p>When you configure your bucket as a static website, the website is available at the AWS Region-specific\n            website endpoint of the bucket.</p>\n<p>Depending on your Region, your Amazon S3 website endpoints follow one of these two formats.</p>\n<p>s3-website dash (-) Region ‐ http://bucket-name.s3-website.Region.amazonaws.com</p>\n<p>s3-website dot (.) Region ‐ http://bucket-name.s3-website-Region.amazonaws.com</p>\n<p>These URLs return the default index document that you configure for the website.</p>\n<p>Incorrect options:</p>\n<p><strong>http://s3-website-Region.bucket-name.amazonaws.com</strong></p>\n<p><strong>http://s3-website.Region.bucket-name.amazonaws.com</strong></p>\n<p><strong>http://bucket-name.Region.s3-website.amazonaws.com</strong></p>\n<p>These three options do not meet the specifications for the Amazon S3 website endpoints format, so these are\n            incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 14",
        "question": "A financial services company has to retain the activity logs for each of their customers to meet compliance\n          guidelines. Depending on the business line, the company wants to retain the logs for 5-10 years in highly\n          available and durable storage on AWS. The overall data size is expected to be in Petabytes. In case of an\n          audit, the data would need to be accessible within a timeframe of up to 48 hours.\nWhich AWS storage option is the MOST cost-effective for the given compliance requirements?",
        "skipped": true,
        "choices": [
            "Amazon S3 Glacier Deep Archive",
            "Third party tape storage",
            "Amazon S3 Glacier",
            "Amazon S3 Standard storage"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon S3 Glacier Deep Archive</strong></p>\n<p>Amazon S3 Glacier and Amazon S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3\n            cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999%\n            durability, and provide comprehensive security and compliance capabilities that can help meet even the most\n            stringent regulatory requirements.</p>\n<p>Amazon S3 Glacier Deep Archive is a new Amazon S3 storage class that provides secure and durable object\n            storage for long-term retention of data that is accessed once or twice in a year. From just $0.00099 per\n            GB-month (less than one-tenth of one cent, or about $1 per TB-month), Amazon S3 Glacier Deep Archive offers\n            the lowest cost storage in the cloud, at prices significantly lower than storing and maintaining data in\n            on-premises magnetic tape libraries or archiving data off-site.</p>\n<p>Amazon S3 Glacier Deep Archive is up to 75% less expensive than Amazon S3 Glacier and provides retrieval\n            within 12 hours using the Standard retrieval speed. You may also reduce retrieval costs by selecting Bulk\n            retrieval, which will return data within 48 hours.</p>\n<p>Therefore, Amazon S3 Glacier Deep Archive is the correct choice.</p>\n<p>Amazon S3 Glacier vs Amazon S3 Glacier Deep Archive:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q24-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon S3 Glacier</strong> - As mentioned earlier, Amazon S3 Glacier Deep Archive is up to 75% less\n            expensive than Amazon S3 Glacier and provides retrieval within 12 hours. So using Amazon S3 Glacier is not\n            the correct choice.</p>\n<p><strong>Third party tape storage</strong></p>\n<p><strong>Amazon S3 Standard storage</strong></p>\n<p>Given the relaxed retrieval times, Amazon S3 standard storage would be much costlier than the Amazon S3\n            Glacier Deep Archive, so Amazon S3 standard storage is not the correct option. Using Third-party tape\n            storage is ruled out as the company wants to use an AWS storage service. Therefore, both of these options\n            are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 15",
        "question": "The data engineering team at an e-commerce company has set up a workflow to ingest the clickstream data into\n          the raw zone of the Amazon S3 data lake. The team wants to run some SQL based data sanity checks on the raw\n          zone of the data lake.\nWhat AWS services would you recommend for this use-case such that the solution is cost-effective and easy to\n          maintain?",
        "skipped": true,
        "choices": [
            "Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and\n                    use SparkSQL to run the SQL based sanity checks",
            "Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based\n                    sanity checks",
            "Use Amazon Athena to run SQL based analytics against Amazon S3 data",
            "Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity\n                    checks"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Amazon Athena to run SQL based analytics against Amazon S3 data</strong></p>\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3\n            using standard SQL. Amazon Athena is serverless, so there is no infrastructure to set up or manage, and\n            customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis,\n            and run interactive queries.</p>\n<p>Amazon Athena Benefits:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q12-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Load the incremental raw zone data into Amazon Redshift on an hourly basis and run the SQL based\n              sanity checks</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse\n            product designed for large scale data set storage and analysis.\n            As the development team would have to maintain and monitor the Amazon Redshift cluster size and would\n            require significant development time to set up the processes to consume the data periodically, so this\n            option is ruled out.</p>\n<p><strong>Load the incremental raw zone data into an Amazon EMR based Spark Cluster on an hourly basis and\n              use SparkSQL to run the SQL based sanity checks</strong> - Amazon EMR is the industry-leading cloud big\n            data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive,\n            Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to\n            distribute your data and processing across a resizable cluster of Amazon EC2 instances.\n            Using an Amazon EMR cluster would imply managing the underlying infrastructure so it’s ruled out because the\n            correct solution for the given use-case should require the least amount of development effort and ongoing\n            maintenance.</p>\n<p><strong>Load the incremental raw zone data into Amazon RDS on an hourly basis and run the SQL based sanity\n              checks</strong> - Loading the incremental data into Amazon RDS implies data migration jobs will have to be\n            written via a AWS Lambda function or an Amazon EC2 based process. This goes against the requirement that the\n            solution should involve the least amount of development effort and ongoing maintenance. Hence this option is\n            not correct.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 16",
        "question": "The engineering team at a weather tracking company wants to enhance the performance of its relational\n          database and is looking for a caching solution that supports geospatial data.\nAs a solutions architect, which of the following solutions will you suggest?",
        "skipped": true,
        "choices": [
            "Use Amazon ElastiCache for Memcached",
            "Use Amazon ElastiCache for Redis",
            "Use AWS Global Accelerator",
            "Use Amazon DynamoDB Accelerator (DAX)"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Amazon ElastiCache for Redis</strong></p>\n<p>Amazon ElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory\n            data store or cache environment in the cloud. Redis, which stands for Remote Dictionary Server, is a fast,\n            open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis\n            now delivers sub-millisecond response times enabling millions of requests per second for real-time\n            applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for\n            caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing,\n            chat/messaging, media streaming, and pub/sub apps.</p>\n<p>All Redis data resides in the server’s main memory, in contrast to databases such as PostgreSQL, Cassandra,\n            MongoDB and others that store most data on disk or on SSDs. In comparison to traditional disk based\n            databases where most operations require a roundtrip to disk, in-memory data stores such as Redis don’t\n            suffer the same penalty. They can therefore support an order of magnitude more operations and faster\n            response times. The result is – blazing fast performance with average read or write operations taking less\n            than a millisecond and support for millions of operations per second.</p>\n<p>Redis has purpose-built commands for working with real-time geospatial data at scale. You can perform\n            operations like finding the distance between two elements (for example people or places) and finding all\n            elements within a given distance of a point.</p>\n<p>Incorrect options:</p>\n<p><strong>Use Amazon ElastiCache for Memcached</strong> - Both Redis and MemCached are in-memory, open-source\n            data stores. Memcached, a high-performance distributed memory cache service, is designed for simplicity\n            while Redis offers a rich set of features that make it effective for a wide range of use cases. Memcached\n            does not offer support for geospatial data.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q65-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a>\n</p>\n<p><strong>Use Amazon DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully\n            managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases.\n          </p>\n<p><strong>Use AWS Global Accelerator</strong> - AWS Global Accelerator is a networking service that helps you\n            improve the availability and performance of the applications that you offer to your global users. This\n            option has been added as a distractor, it has nothing to do with database caching.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 17",
        "question": "A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part\n          of this digital transformation, the media company wants to archive about 5 petabytes of data in its\n          on-premises data center to durable long term storage.\nAs a solutions architect, what is your recommendation to migrate this data in the MOST cost-optimal way?",
        "skipped": true,
        "choices": [
            "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to\n                    transfer the data into Amazon S3 Glacier",
            "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the\n                    AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into\n                    Amazon S3 Glacier",
            "Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the\n                    AWS Snowball Edge data into Amazon S3 Glacier",
            "Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this\n                    connection to transfer the data into Amazon S3 Glacier"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the\n              AWS Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into Amazon S3\n              Glacier</strong></p>\n<p>AWS Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer\n            dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1\n            TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and\n            pre-processing use cases.\n            The data stored on AWS Snowball Edge device can be copied into Amazon S3 bucket and later transitioned into\n            Amazon S3 Glacier via a lifecycle policy. You can't directly copy data from AWS Snowball Edge devices into\n            Amazon S3 Glacier.</p>\n<p>Incorrect options:</p>\n<p><strong>Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the\n              AWS Snowball Edge data into Amazon S3 Glacier</strong> - As mentioned earlier, you can't directly copy\n            data from AWS Snowball Edge devices into Amazon S3 Glacier. Hence, this option is incorrect.</p>\n<p><strong>Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to\n              transfer the data into Amazon S3 Glacier</strong> - AWS Direct Connect lets you establish a dedicated\n            network connection between your network and one of the AWS Direct Connect locations. Using industry-standard\n            802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. Direct Connect\n            involves significant monetary investment and takes more than a month to set up, therefore it's not the\n            correct fit for this use-case where just a one-time data transfer has to be done.</p>\n<p><strong>Setup AWS Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this\n              connection to transfer the data into Amazon S3 Glacier</strong> - AWS Site-to-Site VPN enables you to\n            securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon\n            VPC). VPN Connections are a good solution if you have an immediate need, and have low to modest bandwidth\n            requirements. Because of the high data volume for the given use-case, Site-to-Site VPN is not the correct\n            choice.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 18",
        "question": "A company has noticed that its application performance has deteriorated after a new Auto Scaling group was\n          deployed a few days back. Upon investigation, the team found out that the Launch Configuration selected for\n          the Auto Scaling group is using the incorrect instance type that is not optimized to handle the application\n          workflow.\nAs a solutions architect, what would you recommend to provide a long term resolution for this issue?",
        "skipped": true,
        "choices": [
            "Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group\n                    to use this new launch configuration. Delete the old launch configuration as it is no longer needed",
            "Modify the launch configuration to use the correct instance type and continue to use the existing\n                    Auto Scaling group",
            "No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct\n                    instance type",
            "No need to modify the launch configuration. Just modify the Auto Scaling group to use more number\n                    of existing instance types. More instances may offset the loss of performance"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create a new launch configuration to use the correct instance type. Modify the Auto Scaling group\n              to use this new launch configuration. Delete the old launch configuration as it is no longer\n              needed</strong></p>\n<p>A launch configuration is an instance configuration template that an Auto Scaling group uses to launch\n            Amazon EC2 instances. When you create a launch configuration, you specify information for the instances.\n            Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security\n            groups, and a block device mapping.</p>\n<p>It is not possible to modify a launch configuration once it is created. The correct option is to create a\n            new launch configuration to use the correct instance type. Then modify the Auto Scaling group to use this\n            new launch configuration. Lastly to clean-up, just delete the old launch configuration as it is no longer\n            needed.</p>\n<p>Incorrect options:</p>\n<p><strong>Modify the launch configuration to use the correct instance type and continue to use the existing\n              Auto Scaling group</strong> - As mentioned earlier, it is not possible to modify a launch configuration\n            once it is created. Hence, this option is incorrect.</p>\n<p><strong>No need to modify the launch configuration. Just modify the Auto Scaling group to use the correct\n              instance type</strong> - You cannot use an Auto Scaling group to directly modify the instance type of the\n            underlying instances. Hence, this option is incorrect.</p>\n<p><strong>No need to modify the launch configuration. Just modify the Auto Scaling group to use more number\n              of existing instance types. More instances may offset the loss of performance</strong> - Using the Auto\n            Scaling group to increase the number of instances to cover up for the performance loss is not recommended as\n            it does not address the root cause of the problem. The Machine Learning workflow requires a certain instance\n            type that is optimized to handle Machine Learning computations. Hence, this option is incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 19",
        "question": "An Internet-of-Things (IoT) company is looking for a database solution on AWS Cloud that has Auto Scaling\n          capabilities and is highly available. The database should be able to handle any changes in data attributes\n          over time, in case the company updates the data feed from its IoT devices. The database must provide the\n          capability to output a continuous stream with details of any changes to the underlying data.\nAs a Solutions Architect, which database will you recommend?",
        "skipped": true,
        "choices": [
            "Amazon Relational Database Service (Amazon RDS)",
            "Amazon DynamoDB",
            "Amazon Redshift",
            "Amazon Aurora"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon DynamoDB</strong></p>\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at\n            any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup\n            and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10\n            trillion requests per day and can support peaks of more than 20 million requests per second. DynamoDB is\n            serverless with no servers to provision, patch, or manage and no software to install, maintain, or operate.\n          </p>\n<p>A Amazon DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When\n            you enable a stream on a table, Amazon DynamoDB captures information about every modification to data items\n            in the table.</p>\n<p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream\n            record with the primary key attributes of the items that were modified. A stream record contains information\n            about a data modification to a single item in a DynamoDB table. You can configure the stream so that the\n            stream records capture additional information, such as the \"before\" and \"after\" images of modified items.\n          </p>\n<p>Amazon DynamoDB is horizontally scalable, has a DynamoDB streams capability and is multi-AZ by default. On\n            top of it, we can adjust the RCU and WCU automatically using Auto Scaling. This is the right choice for\n            current requirements.</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon Relational Database Service (Amazon RDS)</strong> - Amazon Relational Database Service\n            (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides\n            cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware\n            provisioning, database setup, patching and backups. Schema changes on relational databases are not straight\n            forward and are hard to maintain if the schema requirements change often.</p>\n<p><strong>Amazon Aurora</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database\n            built for the cloud, that combines the performance and availability of traditional enterprise databases with\n            the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed,\n            fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not\n            an in-memory database. Schema changes on relational databases are not straight forward and are hard to\n            maintain if the schema requirements change often.</p>\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud based data\n            warehouse product designed for large scale data set storage and analysis. It is a powerful warehousing\n            service from Amazon. The current requirement, however, is not looking for a warehousing solution and hence\n            Redshift is not an option here.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 20",
        "question": "A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an\n          unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually\n          restarted via the AWS management console.\nWhich of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution\n          until a permanent fix is delivered by the development team?",
        "skipped": true,
        "choices": [
            "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an\n                    Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the\n                    instance",
            "Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an\n                    Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification\n                    Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function\n                    can use Amazon EC2 API to reboot the instance",
            "Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every\n                    5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2\n                    API to reboot the instance",
            "Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every\n                    5 minutes"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an\n              Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the\n              instance</strong></p>\n<p>Using Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or\n            recover your Amazon EC2 instances. You can use the stop or terminate actions to help you save money when you\n            no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot\n            those instances or recover them onto new hardware if a system impairment occurs.</p>\n<p>You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically reboots\n            the instance. The reboot alarm action is recommended for Instance Health Check failures (as opposed to the\n            recover alarm action, which is suited for System Health Check failures).</p>\n<p>Incorrect options:</p>\n<p><strong>Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an\n              Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification\n              Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use\n              Amazon EC2 API to reboot the instance</strong></p>\n<p><strong>Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every\n              5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to\n              reboot the instance</strong></p>\n<p><strong>Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every\n              5 minutes</strong></p>\n<p>Using Amazon EventBridge event or Amazon CloudWatch alarm to trigger an AWS lambda function, directly or\n            indirectly, is wasteful of resources. You should just use the EC2 Reboot CloudWatch Alarm Action to reboot\n            the instance. So all the options that trigger the AWS lambda function are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 21",
        "question": "A company hires experienced specialists to analyze the customer service calls attended by its call center\n          representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to\n          analyze customer service calls for sentiment analysis via ad-hoc SQL queries.\nAs a Solutions Architect, which of the following solutions would you recommend?",
        "skipped": true,
        "choices": [
            "Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to run analysis on these\n                    text files to understand the underlying patterns. Visualize and display them onto user Dashboards\n                    for human analysis",
            "Use Amazon Transcribe to convert audio files to text and Amazon Athena to understand the underlying\n                    customer sentiments",
            "Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to\n                    convert the audio files into text and run customer sentiment analysis",
            "Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text.\n                    Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used\n                    to visualize and display the output"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Amazon Transcribe to convert audio files to text and Amazon Athena to understand the underlying\n              customer sentiments</strong></p>\n<p>Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy to convert audio to\n            text. One key feature of the service is called speaker identification, which you can use to label each\n            individual speaker when transcribing multi-speaker audio files. You can specify Amazon Transcribe to\n            identify 2–10 speakers in the audio clip.</p>\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using\n            standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the\n            queries that you run. To leverage Athena, you can simply point to your data in Amazon S3, define the schema,\n            and start querying using standard SQL. Most results are delivered within seconds.</p>\n<p>Analyzing multi-speaker audio files using Amazon Transcribe and Amazon Athena:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q58-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena\">https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to\n              convert the audio files into text and run customer sentiment analysis</strong> - Amazon Kinesis can be\n            used to stream real-time data for further analysis and storage. Kinesis Data Streams cannot read audio\n            files. You will still need to use AWS Transcribe for ASR services.</p>\n<p><strong>Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text.\n              Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to\n              visualize and display the output</strong> - Amazon Kinesis Data Streams cannot read audio files. Amazon\n            Alexa cannot be used as an Automatic Speech Recognition (ASR) service, though Alexa internally uses ASR for\n            its working.</p>\n<p><strong>Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to run analysis on these\n              text files to understand the underlying patterns. Visualize and display them onto user Dashboards for\n              human analysis</strong> - Amazon Quicksight is for the visual representation of data through Dashboards,\n            graphs and various other modes. It has a rich feature set that helps analyze data and the complex\n            relationships that exist between different data features. However, it is not an SQL query based analysis\n            tool like Amazon Athena.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena\">https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena</a>\n</p>\n<p><a href=\"https://aws.amazon.com/athena\">https://aws.amazon.com/athena</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 22",
        "question": "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores\n          patient health records on Amazon Simple Storage Service (Amazon S3). The engineering team needs to implement\n          an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access.\n        \nAs a solutions architect, which of the following solutions would you recommend?",
        "skipped": true,
        "choices": [
            "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control\n                    List to enforce compliance controls",
            "Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy\n                    to enforce compliance controls",
            "Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access\n                    Control List to enforce compliance controls",
            "Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle\n                    policy to enforce compliance controls"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Amazon S3 Glacier vault to store the sensitive archived data and then use a vault lock policy\n              to enforce compliance controls</strong></p>\n<p>Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data\n            archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide\n            comprehensive security and compliance capabilities that can help meet even the most stringent regulatory\n            requirements.</p>\n<p>An Amazon S3 Glacier vault is a container for storing archives. When you create a vault, you specify a\n            vault name and the AWS Region in which you want to create the vault. Amazon S3 Glacier Vault Lock allows you\n            to easily deploy and enforce compliance controls for individual Amazon S3 Glacier vaults with a vault lock\n            policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the\n            policy from future edits. Therefore, this is the correct option.</p>\n<p>Incorrect options:</p>\n<p><strong>Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 lifecycle\n              policy to enforce compliance controls</strong> - You can use lifecycle policy to define actions you want\n            Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to\n            another storage class, archive them, or delete them after a specified period. It cannot be used to enforce\n            compliance controls. Therefore, this option is incorrect.</p>\n<p><strong>Use Amazon S3 Glacier vault to store the sensitive archived data and then use an Amazon S3 Access\n              Control List to enforce compliance controls</strong>- Amazon S3 access control lists (ACLs) enable you to\n            manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this\n            option is incorrect.</p>\n<p><strong>Use Amazon S3 Glacier to store the sensitive archived data and then use an Amazon S3 Access Control\n              List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage\n            access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is\n            incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 23",
        "question": "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary\n          algorithm running on Amazon EC2 instances. The ideal storage would facilitate high-performance IOPS when doing\n          file processing in a temporary storage space before uploading the results back into Amazon S3.\nAs a solutions architect, which of the following AWS storage options would you recommend as the MOST\n          performant as well as cost-optimal?",
        "skipped": true,
        "choices": [
            "Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage option",
            "Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option",
            "Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option",
            "Use Amazon EC2 instances with Instance Store as the storage option"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Amazon EC2 instances with Instance Store as the storage option</strong></p>\n<p>An instance store provides temporary block-level storage for your instance. This storage is located on\n            disks that are physically attached to the host computer. Instance store is ideal for the temporary storage\n            of information that changes frequently, such as buffers, caches, scratch data, and other temporary content,\n            or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.\n            Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance.\n            This is a good option when you need storage with very low latency, but you don't need the data to persist\n            when the instance terminates or you can take advantage of fault-tolerant architectures.</p>\n<p>As Instance Store delivers high random I/O performance, it can act as a temporary storage space, and these\n            volumes are included as part of the instance's usage cost, therefore this is the correct option.</p>\n<p>Amazon EC2 Instance Store:\n            <img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_storage.png\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Use Amazon EC2 instances with Amazon EBS General Purpose SSD (gp2) as the storage option</strong> -\n            General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads.\n            These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended\n            periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB\n            and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes\n            to deliver its provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.\n            Amazon EBS gp2 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in\n            addition to that of the Amazon EC2 instance), therefore this option is not correct.</p>\n<p><strong>Use Amazon EC2 instances with Amazon EBS Provisioned IOPS SSD (io1) as the storage option</strong>\n            - Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly\n            database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a\n            bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate\n            when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.\n            Amazon EBS io1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in\n            addition to that of the Amazon EC2 instance), therefore this option is not correct.</p>\n<p><strong>Use Amazon EC2 instances with Amazon EBS Throughput Optimized HDD (st1) as the storage\n              option</strong> - Throughput Optimized HDD (st1) are low-cost HDD volumes designed for frequently\n            accessed, throughput-intensive workloads such as Big data and Data warehouses. Amazon EBS st1 is persistent\n            storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the\n            Amazon EC2 instance), therefore this option is not correct.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 24",
        "question": "A leading video streaming provider is migrating to AWS Cloud infrastructure for delivering its content to\n          users across the world. The company wants to make sure that the solution supports at least a million requests\n          per second for its Amazon EC2 server farm.\nAs a solutions architect, which type of Elastic Load Balancing would you recommend as part of the solution\n          stack?",
        "skipped": true,
        "choices": [
            "Application Load Balancer",
            "Network Load Balancer",
            "Infrastructure Load Balancer",
            "Classic Load Balancer"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Network Load Balancer</strong></p>\n<p>Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that\n            involve scaling to millions of requests per second. Network Load Balancer operates at the connection level\n            (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within\n            Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.</p>\n<p>Incorrect options:</p>\n<p><strong>Application Load Balancer</strong> - Application Load Balancer operates at the request level (layer\n            7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the\n            content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load\n            Balancer provides advanced request routing targeted at delivery of modern application architectures,\n            including microservices and container-based applications.\n            Application Load Balancer is not a good fit for the low latency and high throughput scenario mentioned in\n            the given use-case.</p>\n<p><strong>Classic Load Balancer</strong> - Classic Load Balancer provides basic load balancing across\n            multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load\n            Balancer is intended for applications that were built within the EC2-Classic network. Classic Load Balancer\n            is not a good fit for the low latency and high throughput scenario mentioned in the given use-case.</p>\n<p><strong>Infrastructure Load Balancer</strong> - There is no such thing as Infrastructure Load Balancer and\n            this option just acts as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 25",
        "question": "A Customer relationship management (CRM) application is facing user experience issues with users reporting\n          frequent sign-in requests from the application. The application is currently hosted on multiple Amazon EC2\n          instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy\n          servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based\n          session management solution.\nAs a solutions architect, which of the following solutions would you recommend?",
        "skipped": true,
        "choices": [
            "Use Amazon DynamoDB for distributed in-memory cache based session management",
            "Use Amazon Elasticache for distributed in-memory cache based session management",
            "Use Amazon RDS for distributed in-memory cache based session management",
            "Use Application Load Balancer sticky sessions"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Amazon Elasticache for distributed in-memory cache based session management</strong></p>\n<p>Amazon ElastiCache can be used as a distributed in-memory cache for session management. Amazon ElastiCache\n            allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the\n            cloud. Session stores can be set up using both Memcached or Redis for ElastiCache.</p>\n<p>Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use\n            cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming,\n            queues, real-time analytics, and session store.</p>\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be\n            used as a cache or a data store. Session stores are easy to create with Amazon ElastiCache for Memcached.\n          </p>\n<p>How Amazon ElastiCache Works:\n            <img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_how-it-works.ec509f8b878f549b7fb8a49669bf2547878303f6.png\"/>\n            via - <a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Use Amazon RDS for distributed in-memory cache based session management</strong> - Amazon\n            Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database\n            in the cloud. It cannot be used as a distributed in-memory cache for session management, hence this option\n            is incorrect.</p>\n<p><strong>Use Amazon DynamoDB for distributed in-memory cache based session management</strong> - Amazon\n            DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any\n            scale. Amazon DynamoDB is a NoSQL database and is not the right fit for a distributed in-memory cache-based\n            session management solution.</p>\n<p><strong>Use Application Load Balancer sticky sessions</strong> - Although sticky sessions enable each user\n            to interact with one server and one server only, however, in case of an unhealthy server, all the session\n            data is gone as well. Therefore Amazon Elasticache powered distributed in-memory cache-based session\n            management is a better solution.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/\">https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 26",
        "question": "A mobile chat application uses Amazon DynamoDB as its database service to provide low latency chat updates. A\n          new developer has joined the team and is reviewing the configuration settings for Amazon DynamoDB which have\n          been tweaked for certain technical requirements. AWS CloudTrail service has been enabled on all the resources\n          used for the project. Yet, Amazon DynamoDB encryption details are nowhere to be found.\nWhich of the following options can explain the root cause for the given issue?",
        "skipped": true,
        "choices": [
            "By default, all Amazon DynamoDB tables are encrypted using AWS owned keys, which do not write to\n                    AWS CloudTrail logs",
            "By default, all Amazon DynamoDB tables are encrypted under AWS managed Keys, which do not write to\n                    AWS CloudTrail logs",
            "By default, all Amazon DynamoDB tables are encrypted under Customer managed keys, which do not\n                    write to AWS CloudTrail logs",
            "By default, all Amazon DynamoDB tables are encrypted using Data keys, which do not write to AWS\n                    CloudTrail logs"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>By default, all Amazon DynamoDB tables are encrypted using AWS owned keys, which do not write to\n              AWS CloudTrail logs</strong></p>\n<p>AWS owned keys are not stored in your AWS account. They are part of a collection of KMS keys that AWS owns\n            and manages for use in multiple AWS accounts. AWS services can use AWS owned keys to protect your data. AWS\n            owned keys used by DynamoDB are rotated every year (approximately 365 days).</p>\n<p>You cannot view, manage, or use AWS owned keys, or audit their use. However, you do not need to do any work\n            or change any programs to protect the keys that encrypt your data. You are not charged a monthly fee or a\n            usage fee for use of AWS owned keys, and they do not count against AWS KMS quotas for your account.</p>\n<p>All DynamoDB tables are encrypted. There is no option to enable or disable encryption for new or existing\n            tables. By default, all tables are encrypted under an AWS owned key in the DynamoDB service account.\n            However, you can select an option to encrypt some or all of your tables under a customer managed key or the\n            AWS managed key for DynamoDB in your account.</p>\n<p>Incorrect options:</p>\n<p><strong>By default, all Amazon DynamoDB tables are encrypted under AWS managed Keys, which do not write to\n              AWS CloudTrail logs</strong></p>\n<p><strong>By default, all Amazon DynamoDB tables are encrypted under Customer managed keys, which do not\n              write to AWS CloudTrail logs</strong></p>\n<p><strong>By default, all Amazon DynamoDB tables are encrypted using Data keys, which do not write to AWS\n              CloudTrail logs</strong></p>\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 27",
        "question": "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the\n          internet using the Internet Gateway.\nWhich conditions should be met for internet connectivity to be established? (Select two)",
        "skipped": true,
        "choices": [
            "The subnet has been configured to be public and has no access to the internet",
            "The instance's subnet is associated with multiple route tables with conflicting configurations",
            "The route table in the instance’s subnet should have a route to an Internet Gateway",
            "The instance's subnet is not associated with any route table",
            "The network access control list (network ACL) associated with the subnet must have rules to allow\n                    inbound and outbound traffic"
        ],
        "correct_answer_indices": [
            2,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>The network access control list (network ACL) associated with the subnet must have rules to allow\n              inbound and outbound traffic</strong></p>\n<p>The network access control list (network ACL) that is associated with the subnet must have rules to allow\n            inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a\n            necessary condition for Internet Gateway connectivity.</p>\n<p><strong>The route table in the instance’s subnet should have a route to an Internet Gateway</strong></p>\n<p>A route table contains a set of rules, called routes, that are used to determine where network traffic from\n            your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to\n            the Internet Gateway.</p>\n<p>Incorrect options:</p>\n<p><strong>The instance's subnet is not associated with any route table</strong> - This is an incorrect\n            statement. A subnet is implicitly associated with the main route table if it is not explicitly associated\n            with a particular route table. So, a subnet is always associated with some route table.</p>\n<p><strong>The instance's subnet is associated with multiple route tables with conflicting\n              configurations</strong> - This is an incorrect statement. A subnet can only be associated with one route\n            table at a time.</p>\n<p><strong>The subnet has been configured to be public and has no access to the internet</strong> - This is an\n            incorrect statement. Public subnets have access to the internet via Internet Gateway.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 28",
        "question": "An application hosted on Amazon EC2 contains sensitive personal information about all its customers and needs\n          to be protected from all types of cyber-attacks. The company is considering using the AWS Web Application\n          Firewall (AWS WAF) to handle this requirement.\nCan you identify the correct solution leveraging the capabilities of AWS WAF?",
        "skipped": true,
        "choices": [
            "AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway.\n                    One of these two services can then be configured with Amazon EC2 to build the needed secure\n                    architecture",
            "Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2\n                    instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF\n                    cannot be directly configured on ALB. This configuration not only provides necessary safety but is\n                    scalable too",
            "Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF\n                    on Amazon CloudFront to provide the necessary safety measures",
            "AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the\n                    underlying application data"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create Amazon CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF\n              on Amazon CloudFront to provide the necessary safety measures</strong></p>\n<p>When you use AWS WAF with Amazon CloudFront, you can protect your applications running on any HTTP\n            webserver, whether it's a webserver that's running in Amazon Elastic Compute Cloud (Amazon EC2) or a web\n            server that you manage privately. You can also configure Amazon CloudFront to require HTTPS between\n            CloudFront and your own webserver, as well as between viewers and Amazon CloudFront.</p>\n<p>AWS WAF is tightly integrated with Amazon CloudFront and the Application Load Balancer (ALB), services that\n            AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on\n            Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your\n            end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped\n            before they reach your web servers. When you use AWS WAF on Application Load Balancer, your rules run in the\n            region and can be used to protect internet-facing as well as internal load balancers.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure an Application Load Balancer (ALB) to balance the workload for all the Amazon EC2\n              instances. Configure Amazon CloudFront to distribute from an Application Load Balancer since AWS WAF\n              cannot be directly configured on ALB. This configuration not only provides necessary safety but is\n              scalable too</strong> - This statement is wrong. You can configure AWS WAF on Application Load Balancers\n            (ALB).</p>\n<p><strong>AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the\n              underlying application data</strong> - AWS WAF can be deployed on Amazon CloudFront, the Application Load\n            Balancer (ALB), and Amazon API Gateway. It cannot be configured directly on an Amazon EC2 instance.</p>\n<p><strong>AWS WAF can be directly configured only on an Application Load Balancer or an Amazon API Gateway.\n              One of these two services can then be configured with Amazon EC2 to build the needed secure\n              architecture</strong> - This statement is only partially correct. AWS WAF can also be deployed on Amazon\n            CloudFront service.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/waf/faqs/\">https://aws.amazon.com/waf/faqs/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 29",
        "question": "Your company is evolving towards a microservice approach for their website. The company plans to expose the\n          website from the same load balancer, linked to different target groups with different URLs, that are similar\n          to these - checkout.mycorp.com, www.mycorp.com, mycorp.com/profile, and mycorp.com/search.\nAs a Solutions Architect, which Load Balancer type do you recommend to achieve this routing feature with\n          MINIMUM configuration and development effort?",
        "skipped": true,
        "choices": [
            "Create a Classic Load Balancer",
            "Create a Network Load Balancer",
            "Create an Application Load Balancer",
            "Create an NGINX based load balancer on an Amazon EC2 instance to have advanced routing capabilities"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create an Application Load Balancer</strong></p>\n<p>Application Load Balancer can automatically distribute incoming application traffic across multiple\n            targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the\n            varying load of your application traffic in a single Availability Zone or across multiple Availability\n            Zones.</p>\n<p>If your application is composed of several individual services, an Application Load Balancer can route a\n            request to a service based on the content of the request.</p>\n<p>Here are the different types -</p>\n<p>Host-based Routing:\n            You can route a client request based on the Host field of the HTTP header allowing you to route to multiple\n            domains from the same load balancer. You can use host conditions to define rules that route requests based\n            on the hostname in the host header (also known as host-based routing). This enables you to support multiple\n            domains using a single load balancer. Example hostnames:\n            example.com\n            test.example.com\n            *.example.com\n            The rule *.example.com matches test.example.com but doesn't match example.com.</p>\n<p>Path-based Routing:\n            You can route a client request based on the URL path of the HTTP header. You can use path conditions to\n            define rules that route requests based on the URL in the request (also known as path-based routing). Example\n            path patterns:\n            /img/*\n            /img/<em>/pics\n              The path pattern is used to route requests but does not alter them. For example, if a rule has a path\n              pattern of /img/</em>, the rule would forward a request for /img/picture.jpg to the specified target group\n            as a request for /img/picture.jpg.\n            The path pattern is applied only to the path of the URL, not to its query parameters.</p>\n<p>HTTP header-based routing:\n            You can route a client request based on the value of any standard or custom HTTP header.</p>\n<p>HTTP method-based routing:\n            You can route a client request based on any standard or custom HTTP method.</p>\n<p>Query string parameter-based routing:\n            You can route a client request based on query string or query parameters.</p>\n<p>Source IP address CIDR-based routing:\n            You can route a client request based on source IP address CIDR from where the request originates.</p>\n<p>Path based routing and host based routing are only available for the Application Load Balancer (ALB).\n            Therefore this is the correct option for the given use-case.</p>\n<p>Incorrect options:</p>\n<p><strong>Create an NGINX based load balancer on an Amazon EC2 instance to have advanced routing\n              capabilities</strong> - Although it is technically possible to set up NGINX based load balancer, however,\n            this option involves a lot of configuration effort, so this option is ruled out for the given use-case. So,\n            deploying an NGINX load balancer on Amazon EC2 would work but would suffer management and scaling issues.\n          </p>\n<p><strong>Create a Network Load Balancer</strong> - Network Load Balancer is best suited for use-cases\n            involving low latency and high throughput workloads that involve scaling to millions of requests per second.\n            Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon\n            EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP\n            protocol data.</p>\n<p><strong>Create a Classic Load Balancer</strong> - Classic Load Balancer provides basic load balancing\n            across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic\n            Load Balancer is intended for applications that were built within the EC2-Classic network.</p>\n<p>As mentioned in the description above, these two options are incorrect for the given use-case.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/\">https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 30",
        "question": "A health-care company manages its web application on Amazon EC2 instances running behind Auto Scaling group\n          (ASG). The company provides ambulances for critical patients and needs the application to be reliable. The\n          workload of the company can be managed on 2 Amazon EC2 instances and can peak up to 6 instances when traffic\n          increases.\nAs a Solutions Architect, which of the following configurations would you select as the best fit for these\n          requirements?",
        "skipped": true,
        "choices": [
            "The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance\n                    each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be\n                    set to 6",
            "The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum\n                    capacity set to 6 in a single Availability Zone",
            "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances\n                    each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be\n                    set to 6",
            "The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances\n                    each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to 6"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances\n              each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to\n              6</strong></p>\n<p>You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity.\n            The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is\n            optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.</p>\n<p>Amazon EC2 Auto Scaling enables you to take advantage of the safety and reliability of geographic\n            redundancy by spanning Auto Scaling groups across multiple Availability Zones within a Region. When one\n            Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected\n            Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling\n            automatically redistributes the application instances evenly across all of the designated Availability\n            Zones. Since the application is extremely critical and needs to have a reliable architecture to support it,\n            the Amazon EC2 instances should be maintained in at least two Availability Zones (AZs) for uninterrupted\n            service.</p>\n<p>Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are\n            enabled for your Auto Scaling group. This is why the minimum capacity should be 4 instances and not 2. Auto\n            Scaling group will launch 2 instances each in both the AZs and this redundancy is needed to keep the service\n            available always.</p>\n<p>Incorrect options:</p>\n<p><strong>The Auto Scaling group should be configured with the minimum capacity set to 2, with 1 instance\n              each in two different Availability Zones. The maximum capacity of the Auto Scaling group should be set to\n              6</strong></p>\n<p><strong>The Auto Scaling group should be configured with the minimum capacity set to 2 and the maximum\n              capacity set to 6 in a single Availability Zone</strong></p>\n<p>The explanation above gives the correct rationale for minimum capacity as well as the instance distribution\n            across AZs, so both these options are incorrect.</p>\n<p><strong>The Auto Scaling group should be configured with the minimum capacity set to 4, with 2 instances\n              each in two different AWS Regions. The maximum capacity of the Auto Scaling group should be set to\n              6</strong> - An Auto Scaling group can contain Amazon EC2 instances in one or more Availability Zones\n            within the same region. However, Auto Scaling groups cannot span multiple Regions.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 31",
        "question": "A company wants to ensure high availability for its Amazon RDS database. The development team wants to opt\n          for Multi-AZ deployment and they would like to understand what happens when the primary instance of the\n          Multi-AZ configuration goes down.\nAs a Solutions Architect, which of the following will you identify as the outcome of the scenario?",
        "skipped": true,
        "choices": [
            "The application will be down until the primary database has recovered itself",
            "The URL to access the database will change to the standby database",
            "The CNAME record will be updated to point to the standby database",
            "An email will be sent to the System Administrator asking for manual intervention"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>The CNAME record will be updated to point to the standby database</strong></p>\n<p>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments.\n            Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for\n            MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology. SQL Server DB\n            instances use SQL Server Database Mirroring (DBM) or Always On Availability Groups (AGs).</p>\n<p>In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica\n            in a different Availability Zone. The primary DB instance is synchronously replicated across Availability\n            Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes\n            during system backups. Running a DB instance with high availability can enhance availability during planned\n            system maintenance, and help protect your databases against DB instance failure and Availability Zone\n            disruption.</p>\n<p>Failover is automatically handled by Amazon RDS so that you can resume database operations as quickly as\n            possible without administrative intervention. When failing over, Amazon RDS simply flips the canonical name\n            record (CNAME) for your DB instance to point at the standby, which is in turn promoted to become the new\n            primary. Multi-AZ means the URL is the same, the failover is automated, and the CNAME will automatically be\n            updated to point to the standby database.</p>\n<p>Incorrect options:</p>\n<p><strong>The URL to access the database will change to the standby database</strong> - As discussed above,\n            URL remains the same.</p>\n<p><strong>An email will be sent to the System Administrator asking for manual intervention</strong> - This\n            option is incorrect and it has been added as a distractor.</p>\n<p><strong>The application will be down until the primary database has recovered itself</strong> - This option\n            is incorrect and it has been added as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 32",
        "question": "An online gaming company wants to block access to its application from specific countries; however, the\n          company wants to allow its remote development team (from one of the blocked countries) to have access to the\n          application. The application is deployed on Amazon EC2 instances running under an Application Load Balancer\n          with AWS Web Application Firewall (AWS WAF).\nAs a solutions architect, which of the following solutions can be combined to address the given use-case?\n          (Select two)",
        "skipped": true,
        "choices": [
            "Use AWS WAF IP set statement that specifies the IP addresses that you want to allow through",
            "Use Application Load Balancer IP set statement that specifies the IP addresses that you want to\n                    allow through",
            "Use Application Load Balancer geo match statement listing the countries that you want to block",
            "Create a deny rule for the blocked countries in the network access control list (network ACL)\n                    associated with each of the Amazon EC2 instances",
            "Use AWS WAF geo match statement listing the countries that you want to block"
        ],
        "correct_answer_indices": [
            0,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Use AWS WAF geo match statement listing the countries that you want to block</strong></p>\n<p><strong>Use AWS WAF IP set statement that specifies the IP addresses that you want to allow\n              through</strong></p>\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web\n            exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives\n            you control over how traffic reaches your applications by enabling you to create security rules that block\n            common attack patterns and rules that filter out specific traffic patterns you define.</p>\n<p>You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer\n            that fronts your web servers or origin servers running on Amazon EC2, or Amazon API Gateway for your APIs.\n          </p>\n<p>AWS WAF - How it Works?:\n            <img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\"/>\n            via - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a>\n</p>\n<p>To block specific countries, you can create a AWS WAF geo match statement listing the countries that you\n            want to block, and to allow traffic from IPs of the remote development team, you can create a WAF IP set\n            statement that specifies the IP addresses that you want to allow through. You can combine the two rules as\n            shown below:</p>\n<p>Incorrect options:</p>\n<p><strong>Create a deny rule for the blocked countries in the network access control list (network ACL)\n              associated with each of the Amazon EC2 instances</strong> - A network access control list (network ACL) is\n            an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one\n            or more subnets. A network access control list (network ACL) does not have the capability to block traffic\n            based on geographic match conditions.</p>\n<p><strong>Use Application Load Balancer geo match statement listing the countries that you want to\n              block</strong></p>\n<p><strong>Use Application Load Balancer IP set statement that specifies the IP addresses that you want to\n              allow through</strong></p>\n<p>An Application Load Balancer operates at the request level (layer 7), routing traffic to targets – Amazon\n            EC2 instances, containers, IP addresses, and AWS Lambda functions based on the content of the request. Ideal\n            for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request\n            routing targeted at delivery of modern application architectures, including microservices and\n            container-based applications.</p>\n<p>An Application Load Balancer cannot block or allow traffic based on geographic match conditions or IP based\n            conditions. Both these options have been added as distractors.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/\">https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 33",
        "question": "An application with global users across AWS Regions had suffered an issue when the Elastic Load Balancing\n          (ELB) in a Region malfunctioned thereby taking down the traffic with it. The manual intervention cost the\n          company significant time and resulted in major revenue loss.\nWhat should a solutions architect recommend to reduce internet latency and add automatic failover across AWS\n          Regions?",
        "skipped": true,
        "choices": [
            "Set up an Amazon Route 53 geoproximity routing policy to route traffic",
            "Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations",
            "Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the\n                    nearest edge location to the user",
            "Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is\n                    deployed"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up AWS Global Accelerator and add endpoints to cater to users in different geographic\n              locations</strong></p>\n<p>As your application architecture grows, so does the complexity, with longer user-facing IP lists and more\n            nuanced traffic routing logic. AWS Global Accelerator solves this by providing you with two static IPs that\n            are anycast from our globally distributed edge locations, giving you a single entry point to your\n            application, regardless of how many AWS Regions it’s deployed in. This allows you to add or remove origins,\n            Availability Zones or Regions without reducing your application availability. Your traffic routing is\n            managed manually, or in console with endpoint traffic dials and weights. If your application endpoint has a\n            failure or availability issue, AWS Global Accelerator will automatically redirect your new connections to a\n            healthy endpoint within seconds.</p>\n<p>By using AWS Global Accelerator, you can:</p>\n<ol>\n<li>\n<p>Associate the static IP addresses provided by AWS Global Accelerator to regional AWS resources or\n                endpoints, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IP\n                addresses. The IP addresses are anycast from AWS edge locations so they provide onboarding to the AWS\n                global network close to your users.</p>\n</li>\n<li>\n<p>Easily move endpoints between Availability Zones or AWS Regions without needing to update your DNS\n                configuration or change client-facing applications.</p>\n</li>\n<li>\n<p>Dial traffic up or down for a specific AWS Region by configuring a traffic dial percentage for your\n                endpoint groups. This is especially useful for testing performance and releasing updates.</p>\n</li>\n<li>\n<p>Control the proportion of traffic directed to each endpoint within an endpoint group by assigning\n                weights across the endpoints.</p>\n</li>\n</ol>\n<p>AWS Global Accelerator for Multi-Region applications:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q55-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is\n              deployed</strong> - AWS Direct Connect can reduce latency to great extent. Direct Connect is used to\n            connect on-premises systems to AWS Cloud for extremely low latency use cases. It cannot be used to serve\n            users directly.</p>\n<p><strong>Create Amazon S3 buckets in different AWS Regions and configure Amazon CloudFront to pick the\n              nearest edge location to the user</strong> - If most of the content is static, we can configure Amazon\n            CloudFront to improve performance. In the current scenario, the architecture has ELBs, Amazon EC2 instances\n            too that need to be covered in the automatic failover plan.</p>\n<p><strong>Set up an Amazon Route 53 geoproximity routing policy to route traffic</strong> - Geoproximity\n            routing lets Amazon Route 53 route traffic to your resources based on the geographic location of your users\n            and your resources. Unlike AWS Global Accelerator, managing and routing to different instances, ELBs and\n            other AWS resources will become an operational overhead as the resource count reaches into the hundreds.\n            With inbuilt features like Static anycast IP addresses, fault tolerance using network zones, Global\n            performance-based routing, TCP Termination at the Edge - AWS Global Accelerator is the right choice for\n            multi-region, low latency use cases.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/global-accelerator/features/\">https://aws.amazon.com/global-accelerator/features/</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 34",
        "question": "A streaming solutions company is building a video streaming product by using an Application Load Balancer\n          (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a\n          peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances\n          whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement\n          instance.\nWhat could explain this anomaly?",
        "skipped": true,
        "choices": [
            "Both the Auto Scaling group and Application Load Balancer are using ALB based health check",
            "The Auto Scaling group is using ALB based health check and the Application Load Balancer is using\n                    Amazon EC2 based health check",
            "The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is\n                    using ALB based health check",
            "Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is\n              using ALB based health check</strong></p>\n<p>An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping\n            for automatic scaling and management.</p>\n<p>Auto Scaling Group Overview:\n            <img src=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/as-basic-diagram.png\"/>\n            via - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a>\n</p>\n<p>Application Load Balancer automatically distributes incoming application traffic across multiple targets,\n            such as Amazon EC2 instances, containers, and AWS Lambda functions. It can handle the varying load of your\n            application traffic in a single Availability Zone or across multiple Availability Zones.</p>\n<p>If the Auto Scaling group (ASG) is using EC2 as the health check type and the Application Load Balancer\n            (ALB) is using its in-built health check, there may be a situation where the ALB health check fails because\n            the health check pings fail to receive a response from the instance. At the same time, ASG health check can\n            come back as successful because it is based on EC2 based health check.\n            Therefore, in this scenario, the ALB will remove the instance from its inventory, however, the Auto Scaling\n            Group will fail to provide the replacement instance. This can lead to the scaling issues mentioned in the\n            problem statement.</p>\n<p>Incorrect options:</p>\n<p><strong>The Auto Scaling group is using ALB based health check and the Application Load Balancer is using\n              Amazon EC2 based health check</strong> - Application Load Balancer cannot use EC2 based health checks, so\n            this option is incorrect.</p>\n<p><strong>Both the Auto Scaling group and Application Load Balancer are using ALB based health check</strong>\n            - It is recommended to use ALB based health checks for both Auto Scaling group and Application Load\n            Balancer. If both the Auto Scaling group and Application Load Balancer use ALB based health checks, then you\n            will be able to avoid the scenario mentioned in the question.</p>\n<p><strong>Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health\n              check</strong> - Application Load Balancer cannot use EC2 based health checks, so this option is\n            incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 35",
        "question": "A retail company wants to establish encrypted network connectivity between its on-premises data center and\n          AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should\n          also support encryption in transit.\nAs a solutions architect, which of the following solutions would you suggest to the company?",
        "skipped": true,
        "choices": [
            "Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data\n                    center and AWS Cloud",
            "Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data\n                    center and AWS Cloud",
            "Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data\n                    center and AWS Cloud",
            "Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center\n                    and AWS Cloud"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use AWS Site-to-Site VPN to establish encrypted network connectivity between the on-premises data\n              center and AWS Cloud</strong></p>\n<p>AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your\n            Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network\n            to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish\n            encrypted network connectivity between your on-premises network and Amazon VPC over the Internet. IPsec is a\n            protocol suite for securing IP communications by authenticating and encrypting each IP packet in a data\n            stream.</p>\n<p>Incorrect options:</p>\n<p><strong>Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data\n              center and AWS Cloud</strong> - AWS Direct Connect lets you establish a dedicated network connection\n            between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this\n            dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not\n            encrypt your traffic that is in transit. To encrypt the data in transit that traverses AWS Direct Connect,\n            you must use the transit encryption options for that service. As AWS Direct Connect does not support\n            encrypted network connectivity between an on-premises data center and AWS Cloud, therefore this option is\n            incorrect.</p>\n<p><strong>Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center\n              and AWS Cloud</strong> - AWS DataSync makes it simple and fast to move large amounts of data online\n            between on-premises storage and AWS. AWS DataSync eliminates or automatically handles many of these tasks,\n            including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network\n            utilization. As AWS Data Sync cannot be used to establish network connectivity between an on-premises data\n            center and AWS Cloud, therefore this option is incorrect.</p>\n<p><strong>Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data\n              center and AWS Cloud</strong> - AWS Secrets Manager helps you protect secrets needed to access your\n            applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve\n            database credentials, API keys, and other secrets throughout their lifecycle. As AWS Secrets Manager cannot\n            be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this\n            option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 36",
        "question": "A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon\n          S3. The system will also read these log files in parallel on a near real-time basis. The engineering team\n          wants to address any data discrepancies that might arise when the trading system overwrites an existing log\n          file and then tries to read that specific log file.\nWhich of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?",
        "skipped": true,
        "choices": [
            "A process replaces an existing object and immediately tries to read it. Until the change is fully\n                    propagated, Amazon S3 does not return any data",
            "A process replaces an existing object and immediately tries to read it. Amazon S3 always returns\n                    the latest version of the object",
            "A process replaces an existing object and immediately tries to read it. Until the change is fully\n                    propagated, Amazon S3 might return the previous data",
            "A process replaces an existing object and immediately tries to read it. Until the change is fully\n                    propagated, Amazon S3 might return the new data"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns\n              the latest version of the object</strong></p>\n<p>Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or\n            availability, without sacrificing regional isolation for applications, and at no additional cost.</p>\n<p>After a successful write of a new object or an overwrite of an existing object, any subsequent read request\n            immediately receives the latest version of the object. Amazon S3 also provides strong consistency for list\n            operations, so after a write, you can immediately perform a listing of the objects in a bucket with any\n            changes reflected.</p>\n<p>Strong read-after-write consistency helps when you need to immediately read an object after a write. For\n            example, strong read-after-write consistency when you often read and list immediately after writing objects.\n          </p>\n<p>To summarize, all Amazon S3 GET, PUT, and LIST operations, as well as operations that change object tags,\n            ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST\n            will be an accurate reflection of what’s in the bucket.</p>\n<p>Incorrect options:</p>\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully\n              propagated, Amazon S3 might return the previous data</strong></p>\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully\n              propagated, Amazon S3 does not return any data</strong></p>\n<p><strong>A process replaces an existing object and immediately tries to read it. Until the change is fully\n              propagated, Amazon S3 might return the new data</strong></p>\n<p>These three options contradict the earlier details provided in the explanation.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a>\n</p>\n<p><a href=\"https://aws.amazon.com/s3/faqs/\">https://aws.amazon.com/s3/faqs/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 37",
        "question": "A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field\n          devices of an agricultural sciences company. Multiple consumer applications are using the incoming data\n          streams and the engineers have noticed a performance lag for the data delivery speed between producers and\n          consumers of the data streams.\nAs a solutions architect, which of the following would you recommend for improving the performance for the\n          given use-case?",
        "skipped": true,
        "choices": [
            "Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose",
            "Use Enhanced Fanout feature of Amazon Kinesis Data Streams",
            "Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues",
            "Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Enhanced Fanout feature of Amazon Kinesis Data Streams</strong></p>\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS\n            can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website\n            clickstreams, database event streams, financial transactions, social media feeds, IT logs, and\n            location-tracking events.</p>\n<p>By default, the 2MB/second/shard output is shared between all of the applications consuming data from the\n            stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in\n            parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive\n            their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the\n            number of shards in a stream.</p>\n<p>Amazon Kinesis Data Streams Fanout:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q38-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose</strong> - Amazon Kinesis\n            Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics\n            tools. It is a fully managed service that automatically scales to match the throughput of your data and\n            requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before\n            loading it, minimizing the amount of storage used at the destination and increasing security. Amazon Kinesis\n            Data Firehose can only write to Amazon S3, Amazon Redshift, Amazon Elasticsearch or Splunk. You can't have\n            applications consuming data streams from Amazon Kinesis Data Firehose, that's the job of Amazon Kinesis Data\n            Streams. Therefore this option is not correct.</p>\n<p><strong>Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues</strong></p>\n<p><strong>Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues</strong></p>\n<p>Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to\n            decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two\n            types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once\n            delivery. Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the\n            exact order that they are sent. As multiple applications are consuming the same stream concurrently, both\n            Amazon SQS Standard and Amazon SQS FIFO are not the right fit for the given use-case.</p>\n<p>Exam Alert:</p>\n<p>Please understand the differences between the capabilities of Amazon Kinesis Data Streams vs Amazon SQS, as\n            you may be asked scenario-based questions on this topic in the exam.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q38-i2.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 38",
        "question": "An e-commerce company uses a two-tier architecture with application servers in the public subnet and an\n          Amazon RDS MySQL DB in a private subnet. The development team can use a bastion host in the public subnet to\n          access the MySQL database and run queries from the bastion host. However, end-users are reporting application\n          errors. Upon inspecting application logs, the team notices several \"could not connect to server: connection\n          timed out\" error messages.\nWhich of the following options represent the root cause for this issue?",
        "skipped": true,
        "choices": [
            "The database user credentials (username and password) configured for the application do not have\n                    the required privilege for the given database",
            "The security group configuration for the application servers does not have the correct rules to\n                    allow inbound connections from the database instance",
            "The database user credentials (username and password) configured for the application are incorrect",
            "The security group configuration for the database instance does not have the correct rules to allow\n                    inbound connections from the application servers"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>The security group configuration for the database instance does not have the correct rules to allow\n              inbound connections from the application servers</strong></p>\n<p>You should use security groups to control the inbound and outbound traffic for your database instance. For\n            your application servers, create a security group with inbound rules that use the IP addresses of the client\n            application as the source. This security group allows your client application to connect to your application\n            servers. Then create a second security group for your database instance and create a new rule by specifying\n            the security group that you created earlier as the source for this database-specific security group.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q29-i2.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>The security group configuration for the application servers does not have the correct rules to\n              allow inbound connections from the database instance</strong> - As mentioned in the explanation above, the\n            application servers don't need inbound connections from the database instance, rather the database instance\n            needs the correct inbound rule with application servers' security group as the source.</p>\n<p><strong>The database user credentials (username and password) configured for the application are\n              incorrect</strong></p>\n<p><strong>The database user credentials (username and password) configured for the application do not have\n              the required privilege for the given database</strong></p>\n<p>These two options have been added as a distractor since the error mentions a \"connection timeout\" issue\n            rather than an \"access denied\" error.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 39",
        "question": "A retail company maintains an AWS Direct Connect connection to AWS and has recently migrated its data\n          warehouse to AWS. The data analysts at the company query the data warehouse using a visualization tool. The\n          average size of a query returned by the data warehouse is 60 megabytes and the query responses returned by the\n          data warehouse are not cached in the visualization tool. Each webpage returned by the visualization tool is\n          approximately 600 kilobytes.\nWhich of the following options offers the LOWEST data transfer egress cost for the company?",
        "skipped": true,
        "choices": [
            "Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct\n                    Connect connection at a location in the same AWS region",
            "Deploy the visualization tool in the same AWS region as the data warehouse. Access the\n                    visualization tool over the internet at a location in the same region",
            "Deploy the visualization tool in the same AWS region as the data warehouse. Access the\n                    visualization tool over a Direct Connect connection at a location in the same region",
            "Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location\n                    in the same AWS region"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Deploy the visualization tool in the same AWS region as the data warehouse. Access the\n              visualization tool over a Direct Connect connection at a location in the same region</strong></p>\n<p>AWS Direct Connect is a networking service that provides an alternative to using the internet to connect to\n            AWS. Using AWS Direct Connect, data that would have previously been transported over the internet is\n            delivered through a private network connection between your on-premises data center and AWS.</p>\n<p>For the given use case, the main pricing parameter while using the AWS Direct Connect connection is the\n            Data Transfer Out (DTO) from AWS to the on-premises data center. DTO refers to the cumulative network\n            traffic that is sent through AWS Direct Connect to destinations outside of AWS. This is charged per gigabyte\n            (GB), and unlike capacity measurements, DTO refers to the amount of data transferred, not the speed.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q2-i2.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/directconnect/pricing/\">https://aws.amazon.com/directconnect/pricing/</a></p>\n<p>Each query response is 60 megabytes in size and each webpage for the visualization tool is 600 kilobytes in\n            size. If you deploy the visualization tool in the same AWS region as the data warehouse, then you only need\n            to pay for the 600 kilobytes of DTO charges for the webpage. Therefore this option is correct.</p>\n<p>However, if you deploy the visualization tool on-premises, then you need to pay for the 60 MB of DTO\n            charges for the query response from the data warehouse to the visualization tool.</p>\n<p>Incorrect options:</p>\n<p><strong>Deploy the visualization tool in the same AWS region as the data warehouse. Access the\n              visualization tool over the internet at a location in the same region</strong></p>\n<p><strong>Deploy the visualization tool on-premises. Query the data warehouse over the internet at a location\n              in the same AWS region</strong></p>\n<p>Data transfer pricing over AWS Direct Connect is lower than data transfer pricing over the internet, so\n            both of these options are incorrect.</p>\n<p><strong>Deploy the visualization tool on-premises. Query the data warehouse directly over an AWS Direct\n              Connect connection at a location in the same AWS region</strong> - As mentioned in the explanation above,\n            if you deploy the visualization tool on-premises, then you need to pay for the 60 megabytes of DTO charges\n            for the query response from the data warehouse to the visualization tool. So this option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/directconnect/pricing/\">https://aws.amazon.com/directconnect/pricing/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/getting-started/hands-on/connect-data-center-to-aws/services-costs/\">https://aws.amazon.com/getting-started/hands-on/connect-data-center-to-aws/services-costs/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/directconnect/faqs/\">https://aws.amazon.com/directconnect/faqs/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 40",
        "question": "You have built an application that is deployed with Elastic Load Balancing and an Auto Scaling Group. As a\n          Solutions Architect, you have configured aggressive Amazon CloudWatch alarms, making your Auto Scaling Group\n          (ASG) scale in and out very quickly, renewing your fleet of Amazon EC2 instances on a daily basis. A\n          production bug appeared two days ago, but the team is unable to SSH into the instance to debug the issue,\n          because the instance has already been terminated by the Auto Scaling Group. The log files are saved on the\n          Amazon EC2 instance.\nHow will you resolve the issue and make sure it doesn't happen again?",
        "skipped": true,
        "choices": [
            "Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon S3",
            "Disable the Termination from the Auto Scaling Group any time a user reports an issue",
            "Make a snapshot of the Amazon EC2 instance just before it gets terminated",
            "Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon\n                    CloudWatch"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Install an Amazon CloudWatch Logs agents on the Amazon EC2 instances to send logs to Amazon\n              CloudWatch</strong></p>\n<p>You can use the Amazon CloudWatch Logs agent installer on an existing Amazon EC2 instance to install and\n            configure the Amazon CloudWatch Logs agent. After installation is complete, logs automatically flow from the\n            instance to the log stream you create while installing the agent. The agent confirms that it has started and\n            it stays running until you disable it.</p>\n<p>Here, the natural and by far the easiest solution would be to use the Amazon CloudWatch Logs agents on the\n            Amazon EC2 instances to automatically send log files into Amazon CloudWatch, so we can analyze them in the\n            future easily should any problem arise.</p>\n<p>To control whether an Auto Scaling group can terminate a particular instance when scaling in, use instance\n            scale-in protection. You can enable the instance scale-in protection setting on an Auto Scaling group or on\n            an individual Auto Scaling instance. When the Auto Scaling group launches an instance, it inherits the\n            instance scale-in protection setting of the Auto Scaling group. You can change the instance scale-in\n            protection setting for an Auto Scaling group or an Auto Scaling instance at any time.</p>\n<p>Incorrect options:</p>\n<p><strong>Disable the Termination from the Auto Scaling Group any time a user reports an issue</strong> -\n            Disabling the Termination from the Auto Scaling Group would prevent our Auto Scaling Group from being\n            Elastic and impact our costs. Therefore this option is incorrect.</p>\n<p><strong>Make a snapshot of the Amazon EC2 instance just before it gets terminated</strong> - Making a\n            snapshot of the Amazon EC2 instance before it gets terminated could work but it's tedious, not elastic and\n            very expensive, since our interest is just the log files. Therefore this option is not the best fit for the\n            given use-case.</p>\n<p>You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots.\n            Snapshots are incremental backups, which means that only the blocks on the device that have changed after\n            your most recent snapshot are saved. This minimizes the time required to create the snapshot and saves on\n            storage costs by not duplicating data.</p>\n<p><strong>Use AWS Lambda to regularly SSH into the Amazon EC2 instances and copy the log files to Amazon\n              S3</strong> - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for\n            production-grade serverless log analytics. Using AWS Lambda would be extremely hard to use for this task.\n            Therefore this option is not the best fit for the given use-case.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 41",
        "question": "An IT company has built a custom data warehousing solution for a retail organization by using Amazon\n          Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older\n          than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However\n          the analysts want to retain the ability to cross-reference this historical data along with the daily reports.\n        \nThe company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions\n          architect, which option would you recommend to facilitate this use-case?",
        "skipped": true,
        "choices": [
            "Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon\n                    Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon\n                    Redshift",
            "Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc\n                    queries are run for the historic data, it can be removed from Amazon Redshift",
            "Setup access to the historical data via Amazon Athena. The analytics team can run historical data\n                    queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports\n                    need to be cross-referenced, the analytics team need to export these in flat files and then do\n                    further analysis",
            "Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying\n                    historical data in Amazon S3. The analytics team can then query this historical data to\n                    cross-reference with the daily reports from Redshift"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying\n              historical data in Amazon S3. The analytics team can then query this historical data to cross-reference\n              with the daily reports from Redshift</strong></p>\n<p>Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large\n            scale data set storage and analysis.</p>\n<p>Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data\n            from files in Amazon S3 without having to load the data into Amazon Redshift tables.</p>\n<p>Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster.\n            Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to\n            the Redshift Spectrum layer. Thus, Amazon Redshift Spectrum queries use much less of your cluster's\n            processing capacity than other queries.</p>\n<p>Redshift Spectrum Overview:\n            <img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/18/redshift_spectrum-1.gif\"/>\n            via - <a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Setup access to the historical data via Amazon Athena. The analytics team can run historical data\n              queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to\n              be cross-referenced, the analytics team need to export these in flat files and then do further\n              analysis</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data\n            directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or\n            manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc\n            analysis, and run interactive queries.\n            Providing access to historical data via Athena would mean that historical data reconciliation would become\n            difficult as the daily report would still be produced via Redshift. Such a setup is cumbersome to maintain\n            on a day to day basis. Hence the option to use Athena is ruled out.</p>\n<p><strong>Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon\n              Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon\n              Redshift</strong></p>\n<p><strong>Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc\n              queries are run for the historic data, it can be removed from Amazon Redshift</strong></p>\n<p>Loading historical data into Amazon Redshift via COPY command or AWS Glue ETL job would cost heavy for a\n            one-time ad-hoc process. The same result can be achieved more cost-efficiently by using Amazon Redshift\n            Spectrum. Therefore both these options to load historical data into Redshift are also incorrect for the\n            given use-case.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview\nhttps://aws.amazon.com/blogs/big-data/\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview\n              https://aws.amazon.com/blogs/big-data/</a></p>\n<p><a href=\"#\">amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 42",
        "question": "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from\n          their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for\n          ongoing updates from the on-premises applications.\nAs a solutions architect, which of the following would you select as the MOST performant solution for the\n          given use-case?",
        "skipped": true,
        "choices": [
            "Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use\n                    Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises applications",
            "Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for\n                    ongoing updates",
            "Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then\n                    use AWS DataSync for ongoing updates from the on-premises applications",
            "Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access\n                    to the migrated data for ongoing updates from the on-premises applications"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access\n              to the migrated data for ongoing updates from the on-premises applications</strong></p>\n<p>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large\n            amounts of data to and from AWS storage services over the internet or AWS Direct Connect.</p>\n<p>AWS DataSync fully automates and accelerates moving large active datasets to AWS, up to 10 times faster\n            than command-line tools. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File\n            Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage\n            services, as well as detailed monitoring of the transfer.\n            DataSync uses a purpose-built network protocol and scale-out architecture to transfer data. A single AWS\n            DataSync agent is capable of saturating a 10 Gbps network link.</p>\n<p>AWS DataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms,\n            network optimizations, built-in task scheduling, monitoring via the AWS DataSync API and Console, and Amazon\n            CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. AWS\n            DataSync performs data integrity verification both during the transfer and at the end of the transfer.</p>\n<p>How AWS DataSync Works:\n            <img src=\"https://d1.awsstatic.com/cloud-storage/Storage/aws-datasync-how-it-works-diagram-s3-efs-fsx.c26c66393dc4e433369ee9947f39e9c54cd338bb.png\"/>\n            via - <a href=\"https://aws.amazon.com/datasync/\">https://aws.amazon.com/datasync/</a>\n</p>\n<p>AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually\n            unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File\n            Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching\n            data locally for low-latency access. File gateway offers SMB or NFS-based access to data in Amazon S3 with\n            local caching.</p>\n<p>The combination of AWS DataSync and File Gateway is the correct solution. AWS DataSync enables you to\n            automate and accelerate online data transfers to AWS storage services. File Gateway then provides your\n            on-premises applications with low latency access to the migrated data.</p>\n<p>Incorrect options:</p>\n<p><strong>Use AWS DataSync to migrate existing data to Amazon S3 as well as access the Amazon S3 data for\n              ongoing updates</strong> - AWS DataSync is used to easily transfer data to and from AWS with up to 10x\n            faster speeds. It is used to transfer data and cannot be used to facilitate ongoing updates to the migrated\n            files from the on-premises applications.</p>\n<p><strong>Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use\n              Amazon S3 Transfer Acceleration (Amazon S3TA) for ongoing updates from the on-premises\n              applications</strong> - File Gateway can be used to move on-premises data to AWS Cloud, but it not an\n            optimal solution for high volumes. Migration services such as AWS DataSync are best suited for this purpose.\n            Amazon S3 Transfer Acceleration cannot facilitate ongoing updates to the migrated files from the on-premises\n            applications.</p>\n<p><strong>Use Amazon S3 Transfer Acceleration (Amazon S3TA) to migrate existing data to Amazon S3 and then\n              use AWS DataSync for ongoing updates from the on-premises applications</strong> - If your application is\n            already integrated with the Amazon S3 API, and you want higher throughput for transferring large files to\n            Amazon S3, Amazon S3 Transfer Acceleration can be used. However AWS DataSync cannot be used to facilitate\n            ongoing updates to the migrated files from the on-premises applications.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/datasync/features/\">https://aws.amazon.com/datasync/features/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 43",
        "question": "A company wants to publish an event into an Amazon Simple Queue Service (Amazon SQS) queue whenever a new\n          object is uploaded on Amazon S3.\nWhich of the following statements are true regarding this functionality?",
        "skipped": true,
        "choices": [
            "Neither Standard Amazon SQS queue nor FIFO SQS queue are allowed as an Amazon S3 event notification\n                    destination",
            "Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas\n                    FIFO SQS queue is not allowed",
            "Only FIFO Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas\n                    Standard SQS queue is not allowed",
            "Both Standard Amazon SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification\n                    destination"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas\n              FIFO SQS queue is not allowed</strong></p>\n<p>The Amazon S3 notification feature enables you to receive notifications when certain events happen in your\n            bucket. To enable notifications, you must first add a notification configuration that identifies the events\n            you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.</p>\n<p>Amazon S3 supports the following destinations where it can publish events:</p>\n<p>Amazon Simple Notification Service (Amazon SNS) topic</p>\n<p>Amazon Simple Queue Service (Amazon SQS) queue</p>\n<p>AWS Lambda</p>\n<p>Currently, the Standard Amazon SQS queue is only allowed as an Amazon S3 event notification destination,\n            whereas the FIFO SQS queue is not allowed.</p>\n<p>Incorrect options:</p>\n<p><strong>Both Standard Amazon SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification\n              destination</strong></p>\n<p><strong>Neither Standard Amazon SQS queue nor FIFO SQS queue are allowed as an Amazon S3 event notification\n              destination</strong></p>\n<p><strong>Only FIFO Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas\n              Standard SQS queue is not allowed</strong></p>\n<p>These three options contradict the details provided in the explanation above. To summarize, the Standard\n            Amazon SQS queue is only allowed as an Amazon S3 event notification destination, whereas the FIFO SQS queue\n            is not allowed. Hence these three options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 44",
        "question": "A multi-national company is looking at optimizing their AWS resources across various countries and regions.\n          They want to understand the best practices on cost optimization, performance, and security for their system\n          architecture spanning across multiple business units.\nWhich AWS service is the best fit for their requirements?",
        "skipped": true,
        "choices": [
            "AWS Systems Manager",
            "AWS Management Console",
            "AWS Trusted Advisor",
            "AWS Config"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>AWS Trusted Advisor</strong></p>\n<p>AWS Trusted Advisor is an online tool that draws upon best practices learned from AWS’s aggregated\n            operational history of serving hundreds of thousands of AWS customers. AWS Trusted Advisor inspects your AWS\n            environment and makes recommendations for saving money, improving system performance, or closing security\n            gaps. It scans your AWS infrastructure and compares it to AWS Best practices in five categories (Cost\n            Optimization, Performance, Security, Fault Tolerance, Service limits) and then provides recommendations.</p>\n<p>How AWS Trusted Advisor Works:\n            <img src=\"https://d1.awsstatic.com/product-marketing/AWS%20Support/AWS-trusted-advisor.5b9909d5f29f680eeb12ccff536e8d88d8701304.png\"/>\n            via - <a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>AWS Config</strong> - AWS Config is a service that enables you to assess, audit, and evaluate the\n            configurations of your AWS resources. With Config, you can review changes in configurations and\n            relationships between AWS resources, dive into detailed resource configuration histories, and determine your\n            overall compliance against the configurations specified in your internal guidelines. You can use Config to\n            answer questions such as - “What did my AWS resource look like at xyz point in time?”. It does not offer any\n            feedback about architectural best practices.</p>\n<p><strong>AWS Management Console</strong> - The AWS Management Console is a web application that comprises\n            and refers to a broad collection of service consoles for managing Amazon Web Services. You log into your AWS\n            account using the AWS Management console. It does not offer any feedback about architectural best practices.\n          </p>\n<p><strong>AWS Systems Manager</strong> - AWS Systems Manager is an AWS service that you can use to view and\n            control your infrastructure on AWS. Using the Systems Manager console, you can view operational data from\n            multiple AWS services and automate operational tasks across your AWS resources. With Systems Manager, you\n            can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application,\n            view operational data for monitoring and troubleshooting, and take action on your groups of resources. It\n            does not offer any feedback about architectural best practices.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 45",
        "question": "An e-commerce company uses Amazon Simple Queue Service (Amazon SQS) queues to decouple their application\n          architecture. The engineering team has observed message processing failures for some customer orders.\nAs a solutions architect, which of the following solutions would you recommend for handling such message\n          failures?",
        "skipped": true,
        "choices": [
            "Use short polling to handle message processing failures",
            "Use a temporary queue to handle message processing failures",
            "Use long polling to handle message processing failures",
            "Use a dead-letter queue to handle message processing failures"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use a dead-letter queue to handle message processing failures</strong></p>\n<p>Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be\n            processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging\n            system because they let you isolate problematic messages to determine why their processing doesn't succeed.\n            Sometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments\n            on a story but it remains unprocessed because the original story itself is deleted by the author while the\n            comments were being posted. In such a case, the dead-letter queue can be used to handle message processing\n            failures.</p>\n<p>How do dead-letter queues work?:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q44-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Use a temporary queue to handle message processing failures</strong> - The most common use case for\n            temporary queues is the request-response messaging pattern (for example, processing a login request), where\n            a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS\n            queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary\n            queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing\n            failures.</p>\n<p><strong>Use short polling to handle message processing failures</strong></p>\n<p><strong>Use long polling to handle message processing failures</strong></p>\n<p>Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use\n            short polling. With short polling, Amazon SQS sends the response right away, even if the query found no\n            messages. With long polling, Amazon SQS sends a response after it collects at least one available message,\n            up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if\n            the polling wait time expires.\n            Neither short polling nor long polling can be used to handle message processing failures.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 46",
        "question": "You have just terminated an instance in the us-west-1a Availability Zone (AZ). The attached\n          Amazon EBS volume is now available for attachment to other instances. An intern launches a new Linux Amazon\n          EC2 instance in the us-west-1b Availability Zone (AZ) and is attempting to attach the Amazon EBS\n          volume. The intern informs you that it is not possible and needs your help.\nWhich of the following explanations would you provide to them?",
        "skipped": true,
        "choices": [
            "Amazon EBS volumes are Availability Zone (AZ) locked",
            "Amazon EBS volumes are region locked",
            "The Amazon EBS volume is encrypted",
            "The required IAM permissions are missing"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon EBS volumes are Availability Zone (AZ) locked</strong></p>\n<p>An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After\n            you attach a volume to an instance, you can use it as you would use a physical hard drive. Amazon EBS\n            volumes are flexible. For current-generation volumes attached to current-generation instance types, you can\n            dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production\n            volumes.</p>\n<p>When you create an Amazon EBS volume, it is automatically replicated within its Availability Zone to\n            prevent data loss due to the failure of any single hardware component. You can attach an Amazon EBS volume\n            to an Amazon EC2 instance in the same Availability Zone (AZ).</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon EBS volumes are region locked</strong> - It's confined to an Availability Zone (AZ) and not\n            by region.</p>\n<p><strong>The required IAM permissions are missing</strong> - This is a possibility as well but if\n            permissions are not an issue then you are still confined to an availability zone (AZ).</p>\n<p><strong>The Amazon EBS volume is encrypted</strong> - This doesn't affect the ability to attach an Amazon\n            EBS volume.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 47",
        "question": "A DevOps engineer at an organization is debugging issues related to an Amazon EC2 instance. The engineer has\n          SSH'ed into the instance and he needs to retrieve the instance public IP from within a shell script running on\n          the instance command line.\nCan you identify the correct URL path to get the instance public IP?",
        "skipped": true,
        "choices": [
            "http://254.169.254.169/latest/meta-data/public-ipv4",
            "http://169.254.169.254/latest/meta-data/public-ipv4",
            "http://169.254.169.254/latest/user-data/public-ipv4",
            "http://254.169.254.169/latest/user-data/public-ipv4"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>http://169.254.169.254/latest/meta-data/public-ipv4</strong></p>\n<p>Instance metadata is the data about your instance that you can use to configure or manage the running\n            instance.</p>\n<p>Instance user data is the data that you specified in the form of a configuration script while launching\n            your instance.</p>\n<p>The following URL paths can be used to get the instance meta data and user data from within the instance:\n            http://169.254.169.254/latest/meta-data/</p>\n<p>http://169.254.169.254/latest/user-data/</p>\n<p>Further, you can get the instance public IP via the URL -\n            http://169.254.169.254/latest/meta-data/public-ipv4</p>\n<p>Incorrect options:</p>\n<p><strong>http://169.254.169.254/latest/user-data/public-ipv4</strong></p>\n<p><strong>http://254.169.254.169/latest/meta-data/public-ipv4</strong></p>\n<p><strong>http://254.169.254.169/latest/user-data/public-ipv4</strong></p>\n<p>These three options do not meet the specification for the URL path to get the instance public IP, so these\n            are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 48",
        "question": "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company\n          needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files\n          which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for\n          storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data.\nAs a Solutions Architect, which set of services will you recommend to meet these requirements?",
        "skipped": true,
        "choices": [
            "Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent,\n                    durable storage, and Amazon S3 Glacier Deep Archive for archival storage",
            "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for\n                    archival storage",
            "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon\n                    S3 Glacier for archival storage",
            "Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data\n                    access and Amazon S3 Glacier Deep Archive for archival storage"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon\n              S3 Glacier for archival storage</strong></p>\n<p>An instance store provides temporary block-level storage for your instance. This storage is located on\n            disks that are physically attached to the host computer. Instance store is ideal for the temporary storage\n            of information that changes frequently, such as buffers, caches, scratch data, and other temporary content,\n            or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p>\n<p>You can specify instance store volumes for an instance only when you launch it. You can't detach an\n            instance store volume from one instance and attach it to a different instance.</p>\n<p>Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance.\n            This is a good option when you need storage with very low latency, but you don't need the data to persist\n            when the instance terminates or you can take advantage of fault-tolerant architectures.</p>\n<p>Amazon S3 Standard offers high durability, availability, and performance object storage for frequently\n            accessed data. Because it delivers low latency and high throughput, Amazon S3 Standard is appropriate for a\n            wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and\n            gaming applications, and big data analytics.</p>\n<p>Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably\n            store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep\n            costs low yet suitable for varying needs, Amazon S3 Glacier provides three retrieval options that range from\n            a few minutes to hours. You can upload objects directly to Amazon S3 Glacier, or use S3 Lifecycle policies\n            to transfer data between any of the Amazon S3 Storage Classes for active data (S3 Standard, S3\n            Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA) and S3 Glacier.</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent,\n              durable storage, and Amazon S3 Glacier Deep Archive for archival storage</strong> - Amazon EC2 instance\n            store volumes provide the best I/O performance for low latency requirement, as in the current use case. The\n            Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to\n            the most cost-effective access tier, without performance impact or operational overhead.</p>\n<p>Amazon S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention\n            and digital preservation for data that may be accessed once or twice a year. It is designed for customers —\n            particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public\n            Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements.</p>\n<p><strong>Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for\n              archival storage</strong> - Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes\n            for use with EC2 instances. Amazon EBS volumes are particularly well-suited for use as the primary storage\n            for file systems, databases, or for any applications that require fine granular updates and access to raw,\n            unformatted, block-level storage. For high I/O performance, instance store volumes are a better option.</p>\n<p><strong>Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data\n              access and Amazon S3 Glacier Deep Archive for archival storage</strong> - AWS Storage Gateway is a hybrid\n            cloud storage service that gives you on-premises access to virtually unlimited cloud storage. AWS Storage\n            Gateway will be the right answer if the customer wanted to retain the on-premises data storage and just move\n            the applications to AWS Cloud. In the absence of such requirements, instance store is a better option for\n            high performance and Amazon S3 for durable storage.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 49",
        "question": "A startup has created a cost-effective backup solution in another AWS Region. The application is running in\n          warm standby mode and has Application Load Balancer (ALB) to support it from the front. The current failover\n          process is manual and requires updating the DNS alias record to point to the secondary Application Load\n          Balancer in another Region in case of failure of the primary Application Load Balancer.\nAs a Solutions Architect, what will you recommend to automate the failover process?",
        "skipped": true,
        "choices": [
            "Enable an Amazon Route 53 health check",
            "Enable an Amazon EC2 instance health check",
            "Configure AWS Trusted Advisor to check on unhealthy instances",
            "Enable an ALB health check"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Enable an Amazon Route 53 health check</strong></p>\n<p>Determining the health of an ELB endpoint is more complex than health checking a single IP address. For\n            example, what if your application is running fine on Amazon EC2, but the load balancer itself isn't\n            reachable? Or if your load balancer and your Amazon EC2 instances are working correctly, but a bug in your\n            code causes your application to crash? Or how about if the Amazon EC2 instances in one Availability Zone of\n            a multi-AZ ELB are experiencing problems?</p>\n<p>Amazon Route 53 DNS Failover handles all of these failure scenarios by integrating with ELB behind the\n            scenes. Once enabled, Route 53 automatically configures and manages health checks for individual ELB nodes.\n            Amazon Route 53 also takes advantage of the Amazon EC2 instance health checking that ELB performs\n            (information on configuring your ELB health checks is available here). By combining the results of health\n            checks of your Amazon EC2 instances and your ELBs, Amazon Route 53 DNS Failover can evaluate the health of\n            the load balancer and the health of the application running on the Amazon EC2 instances behind it. In other\n            words, if any part of the stack goes down, Amazon Route 53 detects the failure and routes traffic away from\n            the failed endpoint.</p>\n<p>Using Amazon Route 53 DNS Failover, you can run your primary application simultaneously in multiple AWS\n            regions around the world and failover across regions. Your end-users will be routed to the closest (by\n            latency), healthy region for your application. Amazon Route 53 automatically removes from service any region\n            where your application is unavailable - it will pull an endpoint out of service if there is region-wide\n            connectivity or operational issue, if your application goes down in that region, or if your ELB or Amazon\n            EC2 instances go down in that region.</p>\n<p>Incorrect options:</p>\n<p><strong>Enable an ALB health check</strong> - ELB health check verifies that a specified TCP port on an\n            instance is accepting connections or a specified page has returned an error code of 200. It is not useful\n            for the given failover scenario.</p>\n<p><strong>Enable an Amazon EC2 instance health check</strong> - Instance status checks monitor the software\n            and network configuration of your instance. It is not intelligent enough to understand if the application on\n            the instance is working correctly. Hence, this is not the right choice for the given use-case.</p>\n<p><strong>Configure AWS Trusted Advisor to check on unhealthy instances</strong> - AWS Trusted Advisor\n            examines the health check configuration for Auto Scaling groups. If Elastic Load Balancing is being used for\n            an Auto Scaling group, the recommended configuration is to enable an Elastic Load Balancing health check.\n            AWS Trusted Advisor recommends certain configuration changes by comparing your system configurations to AWS\n            Best practices. It cannot handle a failover the way Amazon Route 53 does.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-route-53-elb-integration-dns-failover/\">https://aws.amazon.com/blogs/aws/amazon-route-53-elb-integration-dns-failover/</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 50",
        "question": "Your firm has implemented a multi-tiered networking structure within the VPC - with two public and two\n          private subnets. The public subnets are used to deploy the Application Load Balancers, while the two private\n          subnets are used to deploy the application on Amazon EC2 instances. The development team wants the Amazon EC2\n          instances to have access to the internet. The solution has to be fully managed by AWS and needs to work over\n          IPv4.\nWhat will you recommend?",
        "skipped": true,
        "choices": [
            "NAT Gateways deployed in your public subnet",
            "NAT Instances deployed in your public subnet",
            "Egress-Only Internet Gateways deployed in your private subnet",
            "Internet Gateways deployed in your private subnet"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>NAT Gateways deployed in your public subnet</strong></p>\n<p>You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect\n            to the internet or other AWS services, but prevent the internet from initiating a connection with those\n            instances. A NAT gateway has the following characteristics and limitations:</p>\n<ol>\n<li>A NAT gateway supports 5 Gbps of bandwidth and automatically scales up to 45 Gbps.</li>\n<li>You can associate exactly one Elastic IP address with a NAT gateway.</li>\n<li>A NAT gateway supports the following protocols: TCP, UDP, and ICMP.</li>\n<li>You cannot associate a security group with a NAT gateway.</li>\n<li>You can use a network access control list (network ACL) to control the traffic to and from the subnet in\n              which the NAT gateway is located.</li>\n<li>A NAT gateway can support up to 55,000 simultaneous connections to each unique destination.</li>\n</ol>\n<p>Therefore you must use a NAT Gateway in your public subnet in order to provide internet access to your\n            instances in your private subnets. You are charged for creating and using a NAT gateway in your account. NAT\n            gateway hourly usage and data processing rates apply.</p>\n<p>Comparison of NAT instances and NAT gateways:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q31-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>NAT Instances deployed in your public subnet</strong> - You can use a network address translation\n            (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound\n            IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic\n            initiated by someone on the Internet. Amazon provides Amazon Linux AMIs that are configured to run as NAT\n            instances. These AMIs include the string amzn-ami-vpc-nat in their names, so you can search for them in the\n            Amazon EC2 console. NAT Instances would work but won't scale and you would have to manage them (as they're\n            nothing but Amazon EC2 instances).</p>\n<p><strong>Internet Gateways deployed in your private subnet</strong> - An internet gateway is a horizontally\n            scaled, redundant, and highly available VPC component that allows communication between instances in your\n            VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network\n            traffic. Internet Gateways must be deployed in a public subnet, hence not an option here.</p>\n<p><strong>Egress-Only Internet Gateways deployed in your private subnet</strong> - An Egress-Only Internet\n            Gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound\n            communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from\n            initiating an IPv6 connection with your instances. Egress-Only Internet Gateways are for IPv6, not IPv4.\n            Therefore, this option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 51",
        "question": "A developer in your team has set up a classic 3 tier architecture composed of an Application Load Balancer,\n          an Auto Scaling group managing a fleet of Amazon EC2 instances, and an Amazon Aurora database. As a Solutions\n          Architect, you would like to adhere to the security pillar of the well-architected framework.\nHow do you configure the security group of the Aurora database to only allow traffic coming from the Amazon\n          EC2 instances?",
        "skipped": true,
        "choices": [
            "Add a rule authorizing the Elastic Load Balancing security group",
            "Add a rule authorizing the Amazon Aurora security group",
            "Add a rule authorizing the Auto Scaling group subnets CIDR",
            "Add a rule authorizing the Amazon EC2 security group"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Add a rule authorizing the Amazon EC2 security group</strong></p>\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you\n            launch an instance, you can specify one or more security groups; otherwise, we use the default security\n            group. You can add rules to each security group that allow traffic to or from its associated instances. You\n            can modify the rules for a security group at any time; the new rules are automatically applied to all\n            instances that are associated with the security group. When we decide whether to allow traffic to reach an\n            instance, we evaluate all the rules from all the security groups that are associated with the instance.</p>\n<p>The following are the characteristics of security group rules:</p>\n<p>By default, security groups allow all outbound traffic.</p>\n<p>Security group rules are always permissive; you can't create rules that deny access.</p>\n<p>Security groups are stateful.</p>\n<p>For the given scenario, the Amazon EC2 instances that are part of the Auto Scaling Group are the ones\n            accessing the database layer. The correct response is to add a rule to the security group attached to Aurora\n            authorizing the Amazon EC2 instance's security group.</p>\n<p>Incorrect options:</p>\n<p><strong>Add a rule authorizing the Amazon Aurora security group</strong> - Adding a rule, authorizing the\n            Aurora security group, is just a distractor. Since it has no bearing on traffic allowed from the Amazon EC2\n            instances.</p>\n<p><strong>Add a rule authorizing the Auto Scaling group subnets CIDR</strong> - Authorizing the entire CIDR\n            of the ASG's subnets is overkill and would allow non-Auto Scaling Group instances, access Aurora if they\n            were part of the same CIDR.</p>\n<p><strong>Add a rule authorizing the Elastic Load Balancing security group</strong> - Adding a rule\n            authorizing the ELB security group would dilute the security for the Aurora databases because only the\n            Amazon EC2 instances that are part of the Auto Scaling Group are the ones accessing the database layer.\n            Therefore, it is not the correct option.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 52",
        "question": "A solutions architect has been tasked to design a low-latency solution for a static, single-page application,\n          accessed by users through a custom domain name. The solution must be serverless, provide in-transit data\n          encryption and needs to be cost-effective.\nWhich AWS services can be combined to build the simplest possible solution for the company's requirement?",
        "skipped": true,
        "choices": [
            "Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low\n                    latency access",
            "Host the application on AWS Fargate and front it with Elastic Load Balancing for an improved\n                    performance",
            "Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application",
            "Host the application on Amazon EC2 instance with instance store volume for high performance and low\n                    latency access to users"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use Amazon S3 to host the static website and Amazon CloudFront to distribute the content for low\n              latency access</strong></p>\n<p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then\n            upload your website content to the bucket. When you configure a bucket as a static website, you must enable\n            website hosting, set permissions, and create and add an index document. Depending on your website\n            requirements, you can also configure redirects, web traffic logging, and a custom error document.</p>\n<p>After you configure your bucket as a static website, you can access the bucket through the AWS\n            Region-specific Amazon S3 website endpoints for your bucket. Website endpoints are different from the\n            endpoints where you send REST API requests. Amazon S3 doesn't support HTTPS access for website endpoints. If\n            you want to use HTTPS, you can use CloudFront to serve a static website hosted on Amazon S3.</p>\n<p>You can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website\n            files (such as HTML, images, and video) available from data centers around the world (called edge\n            locations). When a visitor requests a file from your website, Amazon CloudFront automatically redirects the\n            request to a copy of the file at the nearest edge location. This results in faster download times than if\n            the visitor had requested the content from a data center that is located farther away.</p>\n<p>Amazon CloudFront caches content at edge locations for a period of time that you specify. If a visitor\n            requests content that has been cached for longer than the expiration date, Amazon CloudFront checks the\n            origin server to see if a newer version of the content is available. If a newer version is available, Amazon\n            CloudFront copies the new version to the edge location. Changes that you make to the original content are\n            replicated to edge locations as visitors request the content.</p>\n<p>Incorrect options:</p>\n<p><strong>Host the application on Amazon EC2 instance with instance store volume for high performance and low\n              latency access to users</strong> - Since the use case speaks about a serverless solution, Amazon EC2\n            cannot be the answer, since Amazon EC2 is not serverless.</p>\n<p><strong>Host the application on AWS Fargate and front it with Elastic Load Balancing for an improved\n              performance</strong> - AWS Fargate is a serverless compute engine for containers that works with both\n            Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Elastic Load Balancing\n            can spread the incoming requests across a fleet of Amazon EC2 instances. This added complexity is not needed\n            since we are looking at a static single-page webpage.</p>\n<p><strong>Configure Amazon S3 to store the static data and use AWS Fargate for hosting the\n              application</strong> - AWS Fargate is overkill for hosting a static single-page webpage.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 53",
        "question": "A company wants to store business-critical data on Amazon Elastic Block Store (Amazon EBS) volumes which\n          provide persistent storage independent of Amazon EC2 instances. During a test run, the development team found\n          that on terminating an Amazon EC2 instance, the attached Amazon EBS volume was also lost, which was contrary\n          to their assumptions.\nAs a solutions architect, could you explain this issue?",
        "skipped": true,
        "choices": [
            "The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of volume",
            "On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always terminated",
            "The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of\n                    the instance, the default behavior is to also terminate the attached root volume",
            "The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss\n                    of volume"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>The Amazon EBS volume was configured as the root volume of Amazon EC2 instance. On termination of\n              the instance, the default behavior is to also terminate the attached root volume</strong></p>\n<p>Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use\n            with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any\n            scale.</p>\n<p>When you launch an instance, the root device volume contains the image used to boot the instance. You can\n            choose between AMIs backed by Amazon EC2 instance store and AMIs backed by Amazon EBS.</p>\n<p>By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates. You\n            can change the default behavior to ensure that the volume persists after the instance terminates. Non-root\n            EBS volumes remain available even after you terminate an instance to which the volumes were attached.\n            Therefore, this option is correct.</p>\n<p>Incorrect options:</p>\n<p><strong>The Amazon EBS volumes were not backed up on Amazon S3 storage, resulting in the loss of\n              volume</strong></p>\n<p><strong>The Amazon EBS volumes were not backed up on Amazon EFS file system storage, resulting in the loss\n              of volume</strong></p>\n<p>Amazon EBS volumes do not need to back up the data on Amazon S3 or Amazon EFS filesystem. Both these\n            options are added as distractors.</p>\n<p><strong>On termination of an Amazon EC2 instance, all the attached Amazon EBS volumes are always\n              terminated</strong> - As mentioned earlier, non-root Amazon EBS volumes remain available even after you\n            terminate an instance to which the volumes were attached. Hence this option is incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 54",
        "question": "A development team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The\n          application layer is in a Docker container that provides both static and dynamic content through an\n          Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage.\n          The development team has looked into the network usage and found that 90% of it is due to distributing static\n          content of the application.\nAs a Solutions Architect, what do you recommend to improve the application's network usage and decrease\n          costs?",
        "skipped": true,
        "choices": [
            "Distribute the static content through Amazon S3",
            "Distribute the static content through Amazon EFS",
            "Distribute the dynamic content through Amazon EFS",
            "Distribute the dynamic content through Amazon S3"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Distribute the static content through Amazon S3</strong></p>\n<p>You can use Amazon S3 to host a static website. On a static website, individual web pages include static\n            content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure\n            an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you\n            configure a bucket as a static website, you must enable website hosting, set permissions, and create and add\n            an index document. Depending on your website requirements, you can also configure redirects, web traffic\n            logging, and a custom error document.</p>\n<p>Distributing the static content through Amazon S3 allows us to offload most of the network usage to Amazon\n            S3 and free up our applications running on Amazon ECS.</p>\n<p>Incorrect options:</p>\n<p><strong>Distribute the dynamic content through Amazon S3</strong> - By contrast, a dynamic website relies\n            on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET. Amazon S3 does not\n            support server-side scripting, but AWS has other resources for hosting dynamic websites.</p>\n<p><strong>Distribute the static content through Amazon EFS</strong></p>\n<p><strong>Distribute the dynamic content through Amazon EFS</strong></p>\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system\n            for use with AWS Cloud services and on-premises resources. Using Amazon EFS for static or dynamic content\n            will not change anything as static content on EFS would still have to be distributed by the Amazon ECS\n            instances.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 55",
        "question": "The infrastructure team at a company maintains 5 different VPCs (let's call these VPCs A, B, C, D, E) for\n          resource isolation. Due to the changed organizational structure, the team wants to interconnect all VPCs\n          together. To facilitate this, the team has set up VPC peering connection between VPC A and all other VPCs in a\n          hub and spoke model with VPC A at the center. However, the team has still failed to establish connectivity\n          between all VPCs.\nAs a solutions architect, which of the following would you recommend as the MOST resource-efficient and\n          scalable solution?",
        "skipped": true,
        "choices": [
            "Establish VPC peering connections between all VPCs",
            "Use AWS transit gateway to interconnect the VPCs",
            "Use an internet gateway to interconnect the VPCs",
            "Use a VPC endpoint to interconnect the VPCs"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use AWS transit gateway to interconnect the VPCs</strong></p>\n<p>An AWS transit gateway is a network transit hub that you can use to interconnect your virtual private\n            clouds (VPC) and on-premises networks.</p>\n<p>AWS Transit Gateway Overview:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q14-i2.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html\">https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html</a>\n</p>\n<p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic\n            between them using private IPv4 addresses or IPv6 addresses. Transitive Peering does not work for VPC\n            peering connections. So, if you have a VPC peering connection between VPC A and VPC B (pcx-aaaabbbb), and\n            between VPC A and VPC C (pcx-aaaacccc). Then, there is no VPC peering connection between VPC B and VPC C.\n            Instead of using VPC peering, you can use an AWS Transit Gateway that acts as a network transit hub, to\n            interconnect your VPCs or connect your VPCs with on-premises networks. Therefore this is the correct option.\n          </p>\n<p>VPC Peering Connections Overview:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q14-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html\">https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Use an internet gateway to interconnect the VPCs</strong> - An internet gateway is a horizontally\n            scaled, redundant, and highly available VPC component that allows communication between instances in your\n            VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network\n            traffic. You cannot use an internet gateway to interconnect your VPCs and on-premises networks, hence this\n            option is incorrect.</p>\n<p><strong>Use a VPC endpoint to interconnect the VPCs</strong> - A VPC endpoint enables you to privately\n            connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without\n            requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. You cannot use\n            a VPC endpoint to interconnect your VPCs and on-premises networks, hence this option is incorrect.</p>\n<p><strong>Establish VPC peering connections between all VPCs</strong> - Establishing VPC peering between all\n            VPCs is an inelegant and clumsy way to establish connectivity between all VPCs. Instead, you should use a\n            Transit Gateway that acts as a network transit hub to interconnect your VPCs and on-premises networks.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html\">https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 56",
        "question": "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development\n          process. Most of the daily workflows would be centered around running batch jobs on Amazon EC2 instances with\n          storage on Amazon Elastic Block Store (Amazon EBS) volumes. The CTO is concerned about meeting HIPAA\n          compliance norms for sensitive data stored on Amazon EBS.\nWhich of the following options outline the correct capabilities of an encrypted Amazon EBS volume? (Select\n          three)",
        "skipped": true,
        "choices": [
            "Data at rest inside the volume is NOT encrypted",
            "Any snapshot created from the volume is NOT encrypted",
            "Data at rest inside the volume is encrypted",
            "Data moving between the volume and the instance is NOT encrypted",
            "Data moving between the volume and the instance is encrypted",
            "Any snapshot created from the volume is encrypted"
        ],
        "correct_answer_indices": [
            2,
            4,
            5
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Data at rest inside the volume is encrypted</strong></p>\n<p><strong>Any snapshot created from the volume is encrypted</strong></p>\n<p><strong>Data moving between the volume and the instance is encrypted</strong></p>\n<p>Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with Amazon EC2\n            instances. When you create an encrypted Amazon EBS volume and attach it to a supported instance type, data\n            stored at rest on the volume, data moving between the volume and the instance, snapshots created from the\n            volume and volumes created from those snapshots are all encrypted. It uses AWS Key Management Service (AWS\n            KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. Encryption operations occur\n            on the servers that host Amazon EC2 instances, ensuring the security of both data-at-rest and\n            data-in-transit between an instance and its attached Amazon EBS storage.</p>\n<p>Therefore, the incorrect options are:</p>\n<p><strong>Data moving between the volume and the instance is NOT encrypted</strong></p>\n<p><strong>Any snapshot created from the volume is NOT encrypted</strong></p>\n<p><strong>Data at rest inside the volume is NOT encrypted</strong></p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 57",
        "question": "The engineering team at an online fashion retailer uses AWS Cloud to manage its technology infrastructure.\n          The Amazon EC2 server fleet is behind an Application Load Balancer and the fleet strength is managed by an\n          Auto Scaling group. Based on the historical data, the team is anticipating a huge traffic spike during the\n          upcoming Thanksgiving sale.\nAs an AWS solutions architect, what feature of the Auto Scaling group would you leverage so that the\n          potential surge in traffic can be preemptively addressed?",
        "skipped": true,
        "choices": [
            "Auto Scaling group lifecycle hook",
            "Auto Scaling group scheduled action",
            "Auto Scaling group target tracking scaling policy",
            "Auto Scaling group step scaling policy"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Auto Scaling group scheduled action</strong></p>\n<p>The engineering team can create a scheduled action for the Auto Scaling group to pre-emptively provision\n            additional instances for the sale duration. This makes sure that adequate instances are ready before the\n            sale goes live.\n            The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create\n            a scheduled scaling action, you specify the start time when the scaling action should take effect, and the\n            new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto\n            Scaling updates the group with the values for minimum, maximum, and desired size that are specified by the\n            scaling action.</p>\n<p>Incorrect options:</p>\n<p><strong>Auto Scaling group target tracking scaling policy</strong> - With target tracking scaling policies,\n            you choose a scaling metric and set a target value. Application Auto Scaling creates and manages the Amazon\n            CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric\n            and the target value.</p>\n<p><strong>Auto Scaling group step scaling policy</strong> - With step scaling, you choose scaling metrics and\n            threshold values for the Amazon CloudWatch alarms that trigger the scaling process as well as define how\n            your scalable target should be scaled when a threshold is in breach for a specified number of evaluation\n            periods.</p>\n<p>Both the target tracking as well as step scaling policies entail a lag wherein the instances will be\n            provisioned only when the underlying Amazon CloudWatch alarms go off. Therefore these two options are not\n            pre-emptive in nature and ruled out for the given use-case.</p>\n<p><strong>Auto Scaling group lifecycle hook</strong> - Auto Scaling group lifecycle hooks enable you to\n            perform custom actions as the Auto Scaling group launches or terminates instances. For example, you could\n            install or configure software on newly launched instances, or download log files from an instance before it\n            terminates. Lifecycle hooks cannot be used to pre-emptively provision additional instances for a specific\n            period such as the sale duration.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 58",
        "question": "As a Solutions Architect, you have been hired to work with the engineering team at a company to create a REST\n          API using the serverless architecture.\nWhich of the following solutions will you recommend to move the company to the serverless architecture?",
        "skipped": true,
        "choices": [
            "Amazon Route 53 with Amazon EC2 as backend",
            "AWS Fargate with AWS Lambda at the front",
            "Public-facing Application Load Balancer with Amazon Elastic Container Service (Amazon ECS) on\n                    Amazon EC2",
            "Amazon API Gateway exposing AWS Lambda Functionality"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon API Gateway exposing AWS Lambda Functionality</strong></p>\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish,\n            maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access\n            data, business logic, or functionality from your backend services.</p>\n<p>How Amazon API Gateway Works:\n            <img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\"/>\n            via - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a>\n</p>\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time\n            you consume.</p>\n<p>How AWS Lambda function works:\n            <img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\"/>\n            via - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a>\n</p>\n<p>Amazon API Gateway can expose AWS Lambda functionality through RESTful APIs. Both are serverless options\n            offered by AWS and hence the right choice for this scenario, considering all the functionality they offer.\n          </p>\n<p>Incorrect options:</p>\n<p><strong>AWS Fargate with AWS Lambda at the front</strong> - AWS Lambda cannot directly handle RESTful API\n            requests. You can invoke an AWS Lambda function over HTTPS by defining a custom RESTful API using Amazon API\n            Gateway. So, AWS Fargate with AWS Lambda as the front-facing service is a wrong combination, though both\n            Fargate and Lambda are serverless.</p>\n<p><strong>Public-facing Application Load Balancer with Amazon Elastic Container Service (Amazon ECS) on\n              Amazon EC2</strong> - Amazon ECS on Amazon EC2 does not come under serverless and hence cannot be\n            considered for this use case.</p>\n<p><strong>Amazon Route 53 with Amazon EC2 as backend</strong> - Amazon EC2 is not a serverless service and\n            hence cannot be considered for this use case.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/serverless/\">https://aws.amazon.com/serverless/</a></p>\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 59",
        "question": "A financial services company runs its flagship web application on AWS. The application serves thousands of\n          users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands\n          of financial transactions with multiple internal applications. The solution should also remove sensitive\n          details from the transactions before storing the cleansed transactions in a document database for low-latency\n          retrieval.\nAs an AWS Certified Solutions Architect Associate, which of the following would you recommend?",
        "skipped": true,
        "choices": [
            "Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS\n                    Lambda function to remove sensitive data from the raw transactions in the flat file and then store\n                    the cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions\n                    data with the internal applications",
            "Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration\n                    to remove sensitive data from every transaction and then store the cleansed transactions in Amazon\n                    DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data\n                    Firehose",
            "Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration\n                    to remove sensitive data from every transaction and then store the cleansed transactions in Amazon\n                    DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data\n                    Stream",
            "Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update\n                    the transaction by removing sensitive data whenever any new raw transaction is written. Leverage\n                    Amazon DynamoDB Streams to share the transactions data with the internal applications"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration\n              to remove sensitive data from every transaction and then store the cleansed transactions in Amazon\n              DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data\n              Stream</strong></p>\n<p>You can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data\n            for specialized needs.\n            Amazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to\n            stream your data at the level of your data throughput. You don't have to worry about provisioning,\n            deployment, or ongoing maintenance of hardware, software, or other services for your data streams.</p>\n<p>How Amazon Kinesis Data Streams Work:\n            <img src=\"https://d1.awsstatic.com/Digital%20Marketing/House/1up/products/kinesis/Product-Page-Diagram_Amazon-Kinesis-Data-Streams.e04132af59c6aa1e9372cabf44a17749f4a81b16.png\"/>\n            via - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a>\n</p>\n<p>Amazon Kinesis Data Streams Key Concepts:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q35-i1.jpg\"/>\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q35-i2.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a>\n</p>\n<p>For the given use case, you can stream the raw financial transactions into Amazon Kinesis Data Streams,\n            which in turn, are processed by the AWS Lambda function that is set up as one of the consumers of the data\n            stream. The Lambda would remove sensitive data from every transaction and then store the cleansed\n            transactions in Amazon DynamoDB. The internal applications can be configured as the other consumers of the\n            data stream and ingest the raw transactions</p>\n<p>Incorrect options:</p>\n<p><strong>Batch process the raw transactions data into Amazon S3 flat files. Use S3 events to trigger an AWS\n              Lambda function to remove sensitive data from the raw transactions in the flat file and then store the\n              cleansed transactions in Amazon DynamoDB. Leverage DynamoDB Streams to share the transactions data with\n              the internal applications</strong>- The use case requires a near-real-time solution for cleansing,\n            processing and storing the transactions, so using a batch process would be incorrect.</p>\n<p><strong>Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration\n              to remove sensitive data from every transaction and then store the cleansed transactions in Amazon\n              DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data\n              Firehose</strong> - Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that\n            reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics\n            services.</p>\n<p><img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\"/>\n            via - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n<p>You cannot set up multiple consumers for Amazon Kinesis Data Firehose delivery streams as it can dump data\n            in a single data repository at a time, so this option is incorrect.</p>\n<p><strong>Persist the raw transactions into Amazon DynamoDB. Configure a rule in Amazon DynamoDB to update\n              the transaction by removing sensitive data whenever any new raw transaction is written. Leverage Amazon\n              DynamoDB Streams to share the transactions data with the internal applications</strong> - There is no such\n            rule within Amazon DynamoDB that can auto-update every time a new item is written in a DynamoDB table. You\n            would need to use a Amazon DynamoDB trigger to invoke an external service like a Lambda function on every\n            new write, which can then cleanse and update the item. In addition, this process introduces inefficiency in\n            the workflow as the same item is written and then updated for cleansing purposes. Therefore this option is\n            incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 60",
        "question": "A silicon valley based startup helps its users legally sign highly confidential contracts. To meet the\n          compliance guidelines, the startup must ensure that the signed contracts are encrypted using the AES-256\n          algorithm via an encryption key that is generated as well as managed internally. The startup is now migrating\n          to AWS Cloud and would like the data to be encrypted on AWS. The startup wants to continue using their\n          existing encryption key generation as well as key management mechanism.\nWhat do you recommend?",
        "skipped": true,
        "choices": [
            "SSE-C",
            "Client-Side Encryption",
            "SSE-KMS",
            "SSE-S3"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>SSE-C</strong></p>\n<p>With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon\n            S3 manages the encryption, as it writes to disks, and decryption when you access your objects. With SSE-C,\n            the startup can still generate and manage the encryption key but let AWS do the encryption. Therefore, this\n            is the correct option.</p>\n<p>Incorrect options:</p>\n<p><strong>SSE-KMS</strong> - AWS Key Management Service (AWS KMS) is a service that combines secure, highly\n            available hardware and software to provide a key management system scaled for the cloud. When you use\n            server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already\n            created. But, you never get to know the actual key here.</p>\n<p><strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each\n            object is encrypted with a unique key. However, this option does not provide the ability to audit trail the\n            usage of the encryption keys.</p>\n<p><strong>Client-Side Encryption</strong> - Client-side encryption is the act of encrypting data before\n            sending it to Amazon S3. To enable client-side encryption, you have the following options: Use a AWS KMS key\n            stored in AWS Key Management Service (AWS KMS), Use a master key you store within your application. Since\n            the customer wants to use AWS provided facility, this is not an option.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 61",
        "question": "A global media company uses a fleet of Amazon EC2 instances (behind an Application Load Balancer) to power\n          its video streaming application. To improve the performance of the application, the engineering team has also\n          created an Amazon CloudFront distribution with the Application Load Balancer as the custom origin. The\n          security team at the company has noticed a spike in the number and types of SQL injection and cross-site\n          scripting attack vectors on the application.\nAs a solutions architect, which of the following solutions would you recommend as the MOST effective in\n          countering these malicious attacks?",
        "skipped": true,
        "choices": [
            "Use AWS Security Hub with Amazon CloudFront distribution",
            "Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution",
            "Use Amazon Route 53 with Amazon CloudFront distribution",
            "Use AWS Firewall Manager with CloudFront distribution"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use AWS Web Application Firewall (AWS WAF) with Amazon CloudFront distribution</strong></p>\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web\n            exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives\n            you control over how traffic reaches your applications by enabling you to create security rules that block\n            common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific\n            traffic patterns you define.</p>\n<p>How AWS WAF Works:\n            <img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\"/>\n            via - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a>\n</p>\n<p>A web access control list (web ACL) gives you fine-grained control over the web requests that your Amazon\n            CloudFront distribution, Amazon API Gateway API, or Application Load Balancer responds to.</p>\n<p>When you create a web ACL, you can specify one or more Amazon CloudFront distributions that you want AWS\n            WAF to inspect. AWS WAF starts to allow, block, or count web requests for those distributions based on the\n            conditions that you identify in the web ACL. Therefore, combining AWS WAF with Amazon CloudFront can prevent\n            SQL injection and cross-site scripting attacks. So this is the correct option.</p>\n<p>Incorrect options:</p>\n<p><strong>Use Amazon Route 53 with Amazon CloudFront distribution</strong> - Amazon Route 53 is a highly\n            available and scalable cloud Domain Name System (DNS) web service. You cannot use Route 53 to prevent SQL\n            injection and cross-site scripting attacks. So this option is incorrect.</p>\n<p><strong>Use AWS Security Hub with Amazon CloudFront distribution</strong> - AWS Security Hub gives you a\n            comprehensive view of your high-priority security alerts and security posture across your AWS accounts. With\n            Security Hub, you have a single place that aggregates, organizes, and prioritizes your security alerts, or\n            findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, Amazon Macie, AWS Identity\n            and Access Management (IAM) Access Analyzer, and AWS Firewall Manager, as well as from AWS Partner\n            solutions. You cannot use Security Hub to prevent SQL injection and cross-site scripting attacks. So this\n            option is incorrect.</p>\n<p><strong>Use AWS Firewall Manager with CloudFront distribution</strong> - AWS Firewall Manager is a security\n            management service that allows you to centrally configure and manage firewall rules across your accounts and\n            applications in AWS Organization. You cannot use AWS Firewall Manager to prevent SQL injection and\n            cross-site scripting attacks. So this option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/waf/features/\">https://aws.amazon.com/waf/features/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 62",
        "question": "A cyber security company is running a mission critical application using a single Spread placement group of\n          Amazon EC2 instances. The company needs 15 Amazon EC2 instances for optimal performance.\nHow many Availability Zones (AZs) will the company need to deploy these Amazon EC2 instances per the given\n          use-case?",
        "skipped": true,
        "choices": [
            "14",
            "3",
            "7",
            "15"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>3</strong></p>\n<p>When you launch a new Amazon EC2 instance, the EC2 service attempts to place the instance in such a way\n            that all of your instances are spread out across underlying hardware to minimize correlated failures. You\n            can use placement groups to influence the placement of a group of interdependent instances to meet the needs\n            of your workload. Depending on the type of workload, you can create a placement group using one of the\n            following placement strategies:</p>\n<p>Cluster placement group</p>\n<p>Partition placement group</p>\n<p>Spread placement group.</p>\n<p>A Spread placement group is a group of instances that are each placed on distinct racks, with each rack\n            having its own network and power source.</p>\n<p>Spread placement groups are recommended for applications that have a small number of critical instances\n            that should be kept separate from each other. Launching instances in a spread placement group reduces the\n            risk of simultaneous failures that might occur when instances share the same racks.</p>\n<p>A spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of\n            seven running instances per Availability Zone per group. Therefore, to deploy 15 Amazon EC2 instances in a\n            single Spread placement group, the company needs to use 3 Availability Zones.</p>\n<p>Spread placement group overview:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q11-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>7</strong></p>\n<p><strong>14</strong></p>\n<p><strong>15</strong></p>\n<p>These three options contradict the details provided in the explanation above, so these options are\n            incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 63",
        "question": "A financial services company is moving its IT infrastructure to AWS Cloud and wants to enforce adequate data\n          protection mechanisms on Amazon Simple Storage Service (Amazon S3) to meet compliance guidelines. The\n          engineering team has hired you as a solutions architect to build a solution for this requirement.\nCan you help the team identify the INCORRECT option from the choices below?",
        "skipped": true,
        "choices": [
            "Amazon S3 can encrypt object metadata by using Server-Side Encryption",
            "Amazon S3 can encrypt data in transit using HTTPS (TLS)",
            "Amazon S3 can protect data at rest using Server-Side Encryption",
            "Amazon S3 can protect data at rest using Client-Side Encryption"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon S3 can encrypt object metadata by using Server-Side Encryption</strong></p>\n<p>Amazon S3 is a simple key-value store designed to store as many objects as you want. You store these\n            objects in one or more buckets, and each object can be up to 5 TB in size.</p>\n<p>An object consists of the following:</p>\n<p>Key – The name that you assign to an object. You use the object key to retrieve the object.</p>\n<p>Version ID – Within a bucket, a key and version ID uniquely identify an object.</p>\n<p>Value – The content that you are storing.</p>\n<p>Metadata – A set of name-value pairs with which you can store information regarding the object.</p>\n<p>Subresources – Amazon S3 uses the subresource mechanism to store object-specific additional information.\n          </p>\n<p>Access Control Information – You can control access to the objects you store in Amazon S3.</p>\n<p>Metadata, which can be included with the object, is not encrypted while being stored on Amazon S3.\n            Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata.</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon S3 can protect data at rest using Server-Side Encryption</strong> - This is possible and AWS\n            provides three different ways of doing this - Server-side encryption with Amazon S3‐managed keys (SSE-S3),\n            Server-side encryption with customer master keys stored in AWS Key Management Service (SSE-KMS), Server-side\n            encryption with customer-provided keys (SSE-C).</p>\n<p><strong>Amazon S3 can protect data at rest using Client-Side Encryption</strong> - This is a possible\n            scenario too. You can encrypt data on the client-side and upload the encrypted data to Amazon S3. In this\n            case, the client manages the encryption process, the encryption keys, and related tools.</p>\n<p><strong>Amazon S3 can encrypt data in transit using HTTPS (TLS)</strong> - This is also possible and you\n            can use HTTPS (TLS) to help prevent potential attackers from eavesdropping on or manipulating network\n            traffic using person-in-the-middle or similar attacks.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#server-side\">https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#server-side</a>\n</p>\n<p><a href=\"https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf?did=wp_card&amp;trk=wp_card\">https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf?did=wp_card&amp;trk=wp_card</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 64",
        "question": "A pharma company is working on developing a vaccine for the COVID-19 virus. The researchers at the company\n          want to process the reference healthcare data in a highly available as well as HIPAA compliant in-memory\n          database that supports caching results of SQL queries.\nAs a solutions architect, which of the following AWS services would you recommend for this task?",
        "skipped": true,
        "choices": [
            "Amazon DynamoDB Accelerator (DAX)",
            "Amazon DocumentDB",
            "Amazon DynamoDB",
            "Amazon ElastiCache for Redis/Memcached"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon ElastiCache for Redis/Memcached</strong></p>\n<p>Amazon ElastiCache Overview:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q16-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a>\n</p>\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency\n            to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time\n            transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards,\n            geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache\n            for Redis supports replication, high availability, and cluster sharding right out of the box.</p>\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be\n            used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an\n            in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or\n            NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached.</p>\n<p>Both Amazon ElastiCache for Redis and Amazon ElastiCache for Memcached are HIPAA Eligible. Therefore, this\n            is the correct option.</p>\n<p>Exam Alert:</p>\n<p>Please review this comparison sheet for Redis vs Memcached features:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q16-i2.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a>\n</p>\n<p>Incorrect Options:</p>\n<p><strong>Amazon DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB is a key-value and document database\n            that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region,\n            multi-master, durable database with built-in security, backup and restore, and in-memory caching for\n            internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from\n            fast in-memory performance for demanding applications. DAX does not support SQL query caching.</p>\n<p><strong>Amazon DynamoDB</strong> - Amazon DynamoDB is a key-value and document database that delivers\n            single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable\n            database with built-in security, backup and restore, and in-memory caching (via DAX) for internet-scale\n            applications. Amazon DynamoDB is not an in-memory database, so this option is incorrect.</p>\n<p><strong>Amazon DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully\n            managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB\n            makes it easy to store, query, and index JSON data. Amazon DocumentDB is not an in-memory database, so this\n            option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-elasticache-for-redis-is-now-hipaa-eligible-to-help-you-power-secure-healthcare-applications-with-sub-millisecond-latency/\">https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-elasticache-for-redis-is-now-hipaa-eligible-to-help-you-power-secure-healthcare-applications-with-sub-millisecond-latency/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-elasticache-memcached-hipaa-eligible/\">https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-elasticache-memcached-hipaa-eligible/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds/\">https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 65",
        "question": "The content division at a digital media agency has an application that generates a large number of files on\n          Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years\n          before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but\n          are rarely accessed after the first 30 days. The files contain critical business data that is not easy to\n          reproduce, therefore, immediate accessibility is always required.\nWhich solution is the MOST cost-effective for the given use case?",
        "skipped": true,
        "choices": [
            "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3\n                    Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5\n                    years after object creation",
            "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One\n                    Zone-IA 30 days after object creation. Delete the files 5 years after object creation",
            "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3\n                    Standard-IA 30 days after object creation. Delete the files 5 years after object creation",
            "Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3\n                    Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object\n                    creation"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3\n              Standard-IA 30 days after object creation. Delete the files 5 years after object creation</strong></p>\n<p>Amazon S3 Standard-IA class is for data that is accessed less frequently but requires rapid access when\n            needed. Amazon S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard,\n            with a low per gigabyte storage price and per GB retrieval charge.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q51-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a>\n</p>\n<p>For the given use case, you can set up an Amazon S3 lifecycle configuration and create a transition action\n            to move objects from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. You can set\n            up an expiration action to delete the object 5 years after object creation.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q51-i2.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a>\n</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt5-q51-i3.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3\n              Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object\n              creation</strong> - Amazon S3 Glacier Flexible Retrieval storage class has the best case retrieval time of\n            the order of minutes, so this option is incorrect for the given requirement.</p>\n<p><strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3\n              Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years\n              after object creation</strong> - The files can simply be deleted 5 years after object creation instead of\n            archiving the files to Amazon S3 Glacier Deep Archive. There is no need to incur the cost of archival.</p>\n<p><strong>Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One\n              Zone-IA 30 days after object creation. Delete the files 5 years after object creation</strong> - Unlike\n            other Amazon S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), Amazon S3\n            One Zone-IA stores data in a single AZ and costs 20% less than Amazon S3 Standard-IA. Amazon S3 One Zone-IA\n            is a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. The\n            given scenario clearly states that the business-critical data is not easy to reproduce, so this option is\n            incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/glacier/\">https://aws.amazon.com/s3/storage-classes/glacier/</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a>\n</p>\n</div>\n</div>"
    }
]