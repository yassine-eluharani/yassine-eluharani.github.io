[
    {
        "question_number": "Question 1",
        "question": "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data\n          center. The team is evaluating Amazon Relational Database Service (Amazon RDS) as the database tier for its\n          flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on\n          Amazon RDS Multi-AZ capabilities.\nWhich of the following would you identify as correct for Amazon RDS Multi-AZ? (Select two)",
        "skipped": true,
        "choices": [
            "For automated backups, I/O activity is suspended on your primary database since backups are not\n                    taken from standby database",
            "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for\n                    any reason",
            "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests",
            "Updates to your database Instance are asynchronously replicated across the Availability Zone to the\n                    standby in order to keep both in sync",
            "Amazon RDS applies operating system updates by performing maintenance on the standby, then\n                    promoting the standby to primary and finally performing maintenance on the old primary, which\n                    becomes the new standby"
        ],
        "correct_answer_indices": [
            1,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Amazon RDS applies operating system updates by performing maintenance on the standby, then\n              promoting the standby to primary and finally performing maintenance on the old primary, which becomes the\n              new standby</strong></p>\n<p>Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because\n            Amazon RDS applies operating system updates by following these steps:</p>\n<p>Perform maintenance on the standby.</p>\n<p>Promote the standby to primary.</p>\n<p>Perform maintenance on the old primary, which becomes the new standby.</p>\n<p>When you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades\n            both the primary and secondary DB instances at the same time. In this case, the database engine for the\n            entire Multi-AZ deployment is shut down during the upgrade.</p>\n<p><strong>Amazon RDS automatically initiates a failover to the standby, in case primary database fails for\n              any reason</strong></p>\n<p>You also benefit from enhanced database availability when running your DB instance as a Multi-AZ\n            deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is\n            limited to the time automatic failover takes to complete.</p>\n<p>Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover\n            is automatic and requires no administration. In an Amazon RDS context, this means you are not required to\n            monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone\n            failure or DB instance failure.</p>\n<p>Incorrect options:</p>\n<p><strong>For automated backups, I/O activity is suspended on your primary database since backups are not\n              taken from standby database</strong> - The availability benefits of Multi-AZ also extend to planned\n            maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during\n            your preferred backup window, since backups are taken from the standby.</p>\n<p><strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read\n              requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to\n            provide enhanced database availability and durability, rather than read scaling benefits. As such, the\n            feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary\n            and the standby are constantly in sync, but precludes using the standby for read or write operations.</p>\n<p><strong>Updates to your database Instance are asynchronously replicated across the Availability Zone to the\n              standby in order to keep both in sync</strong> - When you create your DB instance to run as a Multi-AZ\n            deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different\n            Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to\n            the standby in order to keep both in sync and protect your latest database updates against DB instance\n            failure.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 2",
        "question": "The engineering team at a social media company has noticed that while some of the images stored in Amazon S3\n          are frequently accessed, others sit idle for a considerable span of time.\nAs a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
        "skipped": true,
        "choices": [
            "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket\n                    storing the images. The application is triggered daily via Amazon CloudWatch and it changes the\n                    storage class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed\n                    objects are migrated to Amazon S3 Standard class",
            "Store the images using the Amazon S3 Standard-IA storage class",
            "Store the images using the Amazon S3 Intelligent-Tiering storage class",
            "Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket\n                    storing the images. The application is triggered daily via Amazon CloudWatch and it changes the\n                    storage class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed\n                    objects are migrated to Amazon S3 Standard class"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Store the images using the Amazon S3 Intelligent-Tiering storage class</strong></p>\n<p>The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data\n            to the most cost-effective access tier, without performance impact or operational overhead. It works by\n            storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost\n            tier that is optimized for infrequent access.</p>\n<p>For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the\n            objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to\n            the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically\n            moved back to the frequent access tier. Therefore using the Amazon S3 Intelligent-Tiering storage class is\n            the correct solution for the given problem statement.</p>\n<p>Amazon S3 Storage Classes Overview:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q9-i1.jpg\"/>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Store the images using the Amazon S3 Standard-IA storage class</strong></p>\n<p>Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed.\n            Amazon S3 Standard-IA offers high durability, high throughput, and low latency of Amazon S3 Standard, with a\n            low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes\n            Amazon S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\n            The minimum storage duration charge is 30 days. As some of the objects are frequently accessed, the per GB\n            retrieval fee for Amazon S3 Standard-IA can cause the costs to shoot up, hence this option is incorrect.</p>\n<p><strong>Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket\n              storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage\n              class of infrequently accessed objects to Amazon S3 One Zone-IA and the frequently accessed objects are\n              migrated to Amazon S3 Standard class</strong></p>\n<p><strong>Create a data monitoring application on an Amazon EC2 instance in the same region as the bucket\n              storing the images. The application is triggered daily via Amazon CloudWatch and it changes the storage\n              class of infrequently accessed objects to Amazon S3 Standard-IA and the frequently accessed objects are\n              migrated to Amazon S3 Standard class</strong></p>\n<p>Creating a data monitoring application on an Amazon EC2 instance for managing the desired Amazon S3 storage\n            class entails significant development cost as well as infrastructure maintenance effort. The Amazon S3\n            Intelligent-Tiering storage class does the job in a cost-effective way. Therefore both these options are\n            incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 3",
        "question": "A healthcare company runs a fleet of Amazon EC2 instances in two private subnets (named PR1 and PR2) across\n          two Availability Zones (AZs) named A1 and A2. The Amazon EC2 instances need access to the internet for\n          operating system patch management and third-party software maintenance. To facilitate this, the engineering\n          team at the company wants to set up two Network Address Translation gateways (NAT gateways) in a highly\n          available configuration.\nWhich of the following options would you suggest?",
        "skipped": true,
        "choices": [
            "Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public\n                    subnet PU1 in any of the Availability Zones A1 or A2",
            "Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in\n                    Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone A2",
            "Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in\n                    Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone A2",
            "Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of\n                    the Availability Zones A1 or A2"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up a total of two NAT gateways. NAT gateway N1 should be set up in public subnet PU1 in\n              Availability Zone A1. NAT gateway N2 should be set up in public subnet PU2 in Availability Zone\n              A2</strong></p>\n<p>A NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances\n            in a private subnet can connect to services outside your VPC but external services cannot initiate a\n            connection with those instances.</p>\n<p>For the given use case, the Amazon EC2 instances in the private subnets can connect to the internet through\n            public NAT gateways in their respective Availability Zones (AZ). You should create public NAT gateway in the\n            public subnet of each AZ and must associate an elastic IP address with the NAT gateway at creation. Then,\n            you can route traffic from the NAT gateway to the internet gateway for the VPC.</p>\n<p>If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT\n            gateway’s Availability Zone is down, resources in the other Availability Zones lose internet access. To\n            create a highly available or an Availability Zone independent architecture, create a NAT gateway in each\n            Availability Zone and configure your routing to ensure that resources use the NAT gateway in the same\n            Availability Zone.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q13-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Set up a total of two NAT gateways. NAT gateway N1 should be set up in private subnet PR1 in\n              Availability Zone A1. NAT gateway N2 should be set up in private subnet PR2 in Availability Zone\n              A2</strong> - For the Amazon EC2 instances in the private subnet, you can facilitate outbound internet\n            connectivity in a highly available configuration by creating a public NAT gateway in the public subnet of\n            each AZ. You cannot create NAT gateways in the private subnet for the given use case.</p>\n<p><strong>Set up a total of two NAT gateways. Both NAT gateways N1 and N2 should be set up in a single public\n              subnet PU1 in any of the Availability Zones A1 or A2</strong> - For the Amazon EC2 instances in the\n            private subnet, you can facilitate outbound internet connectivity in a highly available configuration by\n            creating a public NAT gateway in the public subnet of each AZ. You cannot create both NAT gateways in a\n            single public subnet, as this configuration would not be highly available.</p>\n<p><strong>Set up a total of one NAT gateway. NAT gateway N1 should be set up in public subnet PU1 in any of\n              the Availability Zones A1 or A2</strong> - For the Amazon EC2 instances in the private subnet, you can\n            facilitate outbound internet connectivity in a highly available configuration by creating a public NAT\n            gateway in the public subnet of each AZ. You cannot create a single NAT gateway, as this configuration would\n            not be highly available.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 4",
        "question": "A Big Data consulting company runs large distributed and replicated workloads on the on-premises data center.\n          The company now wants to move these workloads to Amazon EC2 instances by using the placement groups feature\n          and it wants to minimize correlated hardware failures.\nWhich of the following represents the correct placement group configuration for the given requirement?",
        "skipped": true,
        "choices": [
            "Spread placement groups",
            "Partition placement groups",
            "Cluster placement groups",
            "Multi-AZ placement groups"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Partition placement groups</strong></p>\n<p>Partition placement groups help reduce the likelihood of correlated hardware failures for your application.\n            When using partition placement groups, Amazon EC2 divides each group into logical segments called\n            partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each\n            rack has its own network and power source. No two partitions within a placement group share the same racks,\n            allowing you to isolate the impact of a hardware failure within your application.</p>\n<p>The following image is a simple visual representation of a partition placement group in a single\n            Availability Zone. It shows instances that are placed into a partition placement group with three\n            partitions—Partition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The\n            instances in a partition do not share racks with the instances in the other partitions, allowing you to\n            contain the impact of a single hardware failure to only the associated partition.</p>\n<p>Partition placement groups can be used to deploy large distributed and replicated workloads, such as HDFS,\n            HBase, and Cassandra, across distinct racks. When you launch instances into a partition placement group,\n            Amazon EC2 tries to distribute the instances evenly across the number of partitions that you specify. You\n            can also launch instances into a specific partition to have more control over where the instances are\n            placed.</p>\n<p>A partition placement group can have partitions in multiple Availability Zones in the same Region. A\n            partition placement group can have a maximum of seven partitions per Availability Zone. The number of\n            instances that can be launched into a partition placement group is limited only by the limits of your\n            account.</p>\n<p>Partition placement groups:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Cluster placement groups</strong> - A cluster placement group is a logical grouping of instances\n            within a single Availability Zone. A cluster placement group can span peered VPCs in the same Region.\n            Instances in the same cluster placement group enjoy a higher per-flow throughput limit for TCP/IP traffic\n            and are placed in the same high-bisection bandwidth segment of the network. Cluster placement groups are\n            recommended for applications that benefit from low network latency, high network throughput, or both. They\n            are also recommended when the majority of the network traffic is between the instances in the group. As the\n            instances are packed close together inside an Availability Zone, this option is not correct for the given\n            use case.</p>\n<p>Cluster placement groups:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i2.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition</a>\n</p>\n<p><strong>Spread placement groups</strong> - A spread placement group is a group of instances that are each\n            placed on distinct racks, with each rack having its own network and power source. Spread placement groups\n            are recommended for applications that have a small number of critical instances that should be kept separate\n            from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures\n            that might occur when instances share the same racks. Spread placement groups provide access to distinct\n            racks, and are therefore suitable for mixing instance types or launching instances over time. As the\n            use-case talks about running large distributed and replicated workloads, so it needs more instances,\n            therefore this option is not the right fit for the given use-case.</p>\n<p>A spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of\n            seven running instances per Availability Zone per group.</p>\n<p>The following image shows seven instances in a single Availability Zone that are placed into a spread\n            placement group. The seven instances are placed on seven different racks.</p>\n<p>Spread placement groups:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i3.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition</a>\n</p>\n<p><strong>Multi-AZ placement groups</strong> - This is a made-up option, given as a distractor. You should\n            note that the Partition and Spread placement groups can span across multiple Availability Zones in the same\n            Region.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 5",
        "question": "A software engineering intern at a company is documenting the features offered by Amazon EC2 Spot instances\n          and Spot fleets.\nCan you help the intern by selecting the correct options that identify the key characteristics of these two\n          types of Spot entities? (Select two)",
        "skipped": true,
        "choices": [
            "Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets\n                    are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
            "A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are\n                    launched to meet your target capacity",
            "Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot\n                    instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification",
            "Spot fleets allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid\n                    being interrupted",
            "A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target\n                    capacity"
        ],
        "correct_answer_indices": [
            1,
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices. Spot\n              instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</strong>\n</p>\n<p>Spot instances are spare Amazon EC2 capacity that can save you up 90% off of On-Demand prices that Amazon\n            Web Services can interrupt with a 2-minute notification. Because Spot Instances enable you to request unused\n            EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a\n            cost-effective choice if you can be flexible about when your applications run and if your applications can\n            be interrupted.</p>\n<p><strong>A Spot fleet can consist of a set of Spot Instances and optionally On-Demand Instances that are\n              launched to meet your target capacity</strong></p>\n<p>A Spot fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances. The Spot\n            fleet attempts to launch the number of Spot Instances and On-Demand Instances to meet the target capacity\n            that you specified in the Spot fleet request. A Spot fleet allows you to automatically request and manage\n            multiple Spot instances that provide the lowest price per unit of capacity for your cluster or application,\n            like a batch processing job, a Hadoop workflow, or an HPC grid computing job.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q2-i1.jpg\"/>\n            via - <a href=\"https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html\">https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>A Spot fleet can only consist of a set of Spot Instances that are launched to meet your target\n              capacity</strong></p>\n<p><strong>Spot fleets are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot fleets\n              are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</strong></p>\n<p><strong>[@@-F]</strong></p>\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://www.amazonaws.cn/en/ec2/spot-instances/faqs/\">https://www.amazonaws.cn/en/ec2/spot-instances/faqs/</a>\n</p>\n<p><a href=\"https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html\">https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 6",
        "question": "A social media application lets users upload photos and perform image editing operations. The application\n          offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be\n          processed before those submitted by lite users. Photos are uploaded to Amazon S3 and the job information is\n          sent to Amazon SQS.\nAs a solutions architect, which of the following solutions would you recommend?",
        "skipped": true,
        "choices": [
            "Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up\n                    Amazon EC2 instances to prioritize visibility settings so pro photos are processed first",
            "Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to\n                    prioritize polling for the pro queue over the lite queue",
            "Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use\n                    short polling and the pro queue to use long polling",
            "Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short\n                    polling and the pro queue to use long polling"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create two Amazon SQS standard queues: one for pro and one for lite. Set up Amazon EC2 instances to\n              prioritize polling for the pro queue over the lite queue</strong></p>\n<p>AWS recommends using separate queues to provide prioritization of work. Therefore, for the given use case,\n            you need to create an Amazon SQS standard queue for processing pro users' photos and another Amazon SQS\n            standard queue for processing lite users' photos. Then you can configure Amazon EC2 instances to prioritize\n            polling for the pro queue over the lite queue.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q32-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create two Amazon SQS standard queues: one for pro and one for lite. Set the lite queue to use\n              short polling and the pro queue to use long polling</strong></p>\n<p><strong>Create two Amazon SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short\n              polling and the pro queue to use long polling</strong></p>\n<p>Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short\n            polling returns immediately, even if the message queue being polled is empty, long-polling doesn’t return a\n            response until a message arrives in the message queue, or the long poll times out. Since long polling or\n            short polling cannot impact the priority of processing for the two queues, so both these options are\n            incorrect.</p>\n<p><strong>Create one Amazon SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up\n              Amazon EC2 instances to prioritize visibility settings so pro photos are processed first</strong> - To\n            prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of\n            time during which Amazon SQS prevents other consumers from receiving and processing the message. The default\n            visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. Setting\n            visibility timeout to zero can result in the same pro photo being processed by more than one consumer. This\n            does not help in prioritizing the processing of pro photos over the lite photos.</p>\n<p><img src=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/images/sqs-visibility-timeout-diagram.png\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a>\n</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 7",
        "question": "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't\n          been created to work in distributed mode. Still, you want to make sure your setup can automatically recover\n          from the failure of an Availability Zone (AZ).\nWhich of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
        "skipped": true,
        "choices": [
            "Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it",
            "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1, desired=1",
            "Assign an Amazon EC2 Instance Role to perform the necessary API calls",
            "Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2, desired=2",
            "Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling\n                    Group",
            "Create a Spot Fleet request"
        ],
        "correct_answer_indices": [
            0,
            1,
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=1,\n              desired=1</strong></p>\n<p>Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available\n            to handle the load for your application. You create collections of EC2 instances, called Auto Scaling\n            groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto\n            Scaling ensures that your group never goes below this size.</p>\n<p>So we have an Auto Scaling Group with desired=1, across two AZ, so that if an instance goes down, it is\n            automatically recreated in another AZ. So this option is correct.</p>\n<p><strong>Create an elastic IP address (EIP) and use the Amazon EC2 user-data script to attach it</strong>\n</p>\n<p>Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets –\n            Amazon EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request.\n            Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced\n            request routing targeted at delivery of modern application architectures, including microservices and\n            container-based applications.</p>\n<p>An Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address\n            is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or\n            software by rapidly remapping the address to another instance in your account.</p>\n<p>Now, between the ALB and the Elastic IP. If we use an ALB, things will still work, but we will have to pay\n            for the provisioned ALB which sends traffic to only one Amazon EC2 instance. Instead, to minimize costs, we\n            must use an Elastic IP.</p>\n<p><strong>Assign an Amazon EC2 Instance Role to perform the necessary API calls</strong></p>\n<p>For that Elastic IP to be attached to our Amazon EC2 instance, we must use an EC2 user data script, and our\n            Amazon EC2 instance must have the correct IAM permissions to perform the API call, so we need an Amazon EC2\n            instance role.</p>\n<p>Incorrect options:</p>\n<p><strong>Create a Spot Fleet request</strong> - A Spot Instance is an unused Amazon EC2 instance that is\n            available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2\n            instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot\n            Instance is called a Spot price.</p>\n<p>The Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the\n            target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching\n            replacement instances after Spot Instances in the fleet are terminated.</p>\n<p>Spot Fleets requests would not fit our purpose as we are looking at a critical application. Spot instances\n            can be terminated. So this option is incorrect.</p>\n<p><strong>Create an auto-scaling group that spans across 2 Availability Zones, which min=1, max=2,\n              desired=2</strong> - An Auto Scaling Group with desired=2 would create two instances, and this won't work\n            for us as our monolith application is not made to work with two instances as per the given use-case.</p>\n<p><strong>Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling\n              Group</strong> - If we use an Application Load Balancer (ALB), things will still work, but we will have to\n            pay for the provisioned ALB which sends traffic to only one Amazon EC2 instance. So this option is not\n            correct.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 8",
        "question": "A systems administration team has a requirement to run certain custom scripts only once during the launch of\n          the Amazon Elastic Compute Cloud (Amazon EC2) instances that host their application.\nWhich of the following represents the best way of configuring a solution for this requirement with minimal\n          effort?",
        "skipped": true,
        "choices": [
            "Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data\n                    scripts, are run only during the boot process",
            "Run the custom scripts as user data scripts on the Amazon EC2 instances",
            "Use AWS CLI to run the user data scripts only once while launching the instance",
            "Run the custom scripts as instance metadata scripts on the Amazon EC2 instances"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Run the custom scripts as user data scripts on the Amazon EC2 instances</strong></p>\n<p>When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that\n            can be used to perform common automated configuration tasks and even run scripts after the instance starts.\n            You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives.</p>\n<p>By default, user data scripts and cloud-init directives run only during the boot cycle when you first\n            launch an instance. Hence, no extra configuration is needed, apart from including the custom scripts in user\n            data scripts.</p>\n<p>Incorrect options:</p>\n<p><strong>Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data\n              scripts, are run only during the boot process</strong> - You can update your configuration to ensure that\n            your user data scripts and cloud-init directives run every time you restart your instance. By default, the\n            scripts are run, only once during the boot process while first launching the instance.</p>\n<p><strong>Run the custom scripts as instance metadata scripts on the Amazon EC2 instances</strong>- Instance\n            metadata is data about your instance that you can use to configure or manage the running instance. Metadata\n            cannot be used to run custom scripts.</p>\n<p><strong>Use AWS CLI to run the user data scripts only once while launching the instance</strong> - This\n            statement is incorrect and used only as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 9",
        "question": "A development team wants to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?\nWhich of the following options represents the correct solution?",
        "skipped": true,
        "choices": [
            "Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header\n                    set to true",
            "Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to\n                    private",
            "Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set",
            "Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption\n                    header set"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption\n              header set</strong></p>\n<p>Amazon S3 encrypts your data at the object level as it writes to disks in AWS data centers, and decrypts it\n            for you when you access it. You can encrypt objects by using client-side encryption or server-side\n            encryption. Client-side encryption occurs when an object is encrypted before you upload it to Amazon S3, and\n            the keys are not managed by AWS. With server-side encryption, Amazon manages the keys in one of three ways:\n          </p>\n<ol>\n<li>Server-side encryption with customer-provided encryption keys (SSE-C).</li>\n<li>SSE-S3.</li>\n<li>SSE-KMS.</li>\n</ol>\n<p>Server-side encryption is about data encryption at rest—that is, Amazon S3 encrypts your data at the object\n            level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as\n            you authenticate your request and you have access permissions, there is no difference in the way you access\n            encrypted or unencrypted objects.</p>\n<p>To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to\n            the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS.</p>\n<p>In order to enforce object encryption, create an Amazon S3 bucket policy that denies any S3 Put request\n            that does not include the x-amz-server-side-encryption header. There are two possible values for the\n            x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells\n            Amazon S3 to use AWS KMS–managed keys.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to\n              private</strong> - The x-amz-acl header is used to specify an ACL in the PutObject request. Access\n            permissions are defined using this header.</p>\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header\n              set to true</strong> - By default, Amazon S3 allows both HTTP and HTTPS requests. aws:SecureTransport key\n            is used to check if the request is sent through HTTP or HTTPS. When this key is true, it means that the\n            request is sent through HTTPS.</p>\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header\n              set</strong> - As discussed above, the s3:x-amz-acl header is used to set permissions on the specified S3\n            bucket and has nothing to do with encryption.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\">https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 10",
        "question": "As a Solutions Architect, you would like to completely secure the communications between your Amazon\n          CloudFront distribution and your Amazon S3 bucket which contains the static files for your website. Users\n          should only be able to access the Amazon S3 bucket through Amazon CloudFront and not directly.\nWhat do you recommend?",
        "skipped": true,
        "choices": [
            "Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront\n                    distribution",
            "Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy",
            "Make the Amazon S3 bucket public",
            "Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront\n                    security group"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create an origin access identity (OAI) and update the Amazon S3 Bucket Policy</strong></p>\n<p>To restrict access to content that you serve from Amazon S3 buckets, you need to follow the following\n            steps:</p>\n<ol>\n<li>Create a special Amazon CloudFront user called an origin access identity (OAI) and associate it with\n              your distribution.</li>\n<li>Configure your Amazon S3 bucket permissions so that Amazon CloudFront can use the OAI to access the\n              files in your bucket and serve them to your users. Make sure that users can’t use a direct URL to the\n              Amazon S3 bucket to access a file there.</li>\n</ol>\n<p>After you take these steps, users can only access your files through Amazon CloudFront, not directly from\n            the Amazon S3 bucket.</p>\n<p>In general, if you’re using an Amazon S3 bucket as the origin for a Amazon CloudFront distribution, you can\n            either allow everyone to have access to the files there, or you can restrict access. If you restrict access\n            by using, for example, Amazon CloudFront signed URLs or signed cookies, you also won’t want people to be\n            able to view files by simply using the direct Amazon S3 URL for the file. Instead, you want them to only\n            access the files by using the Amazon CloudFront URL, so your content remains protected.</p>\n<p>Incorrect options:</p>\n<p><strong>Update the Amazon S3 bucket security groups to only allow traffic from the Amazon CloudFront\n              security group</strong> - Amazon S3 buckets don't have security groups, hence this is an incorrect option.\n          </p>\n<p><strong>Make the Amazon S3 bucket public</strong> - If the Amazon S3 bucket is made public, it can be\n            accessed by anyone directly. This is not the requirement.</p>\n<p><strong>Create a bucket policy to only authorize the IAM role attached to the Amazon CloudFront\n              distribution</strong> - You cannot attach IAM roles to the Amazon CloudFront distribution. Here you need\n            to use an OAI.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 11",
        "question": "A company helps its customers legally sign highly confidential contracts. To meet the strong industry\n          requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary\n          algorithm. The company is now migrating to AWS Cloud using Amazon Simple Storage Service (Amazon S3) and would\n          like you, the solution architect, to advise them on the encryption scheme to adopt.\nWhat do you recommend?",
        "skipped": true,
        "choices": [
            "Server-side encryption with customer-provided keys (SSE-C)",
            "Client Side Encryption",
            "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
            "Server-side encryption with AWS KMS keys (SSE-KMS)"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Client Side Encryption</strong></p>\n<p>Client-side encryption is the act of encrypting your data locally to help ensure its security in transit\n            and at rest. To encrypt your objects before you send them to Amazon S3, use the Amazon S3 Encryption Client.\n            When your objects are encrypted in this manner, your objects aren't exposed to any third party, including\n            AWS. Amazon S3 receives your objects already encrypted; Amazon S3 does not play a role in encrypting or\n            decrypting your objects. You can use both the Amazon S3 Encryption Client and server-side encryption to\n            encrypt your data. When you send encrypted objects to Amazon S3, Amazon S3 doesn't recognize the objects as\n            being encrypted, it only detects typical objects.</p>\n<p>Incorrect options:</p>\n<p><strong>Server-side encryption with AWS KMS keys (SSE-KMS)</strong> - AWS Key Management Service (AWS KMS)\n            is a service that combines secure, highly available hardware and software to provide a key management system\n            scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a\n            customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when\n            your CMK was used and by whom.</p>\n<p><strong>Server-side encryption with Amazon S3 managed keys (SSE-S3)</strong> - When you use Server-Side\n            Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key.</p>\n<p><strong>Server-side encryption with customer-provided keys (SSE-C)</strong> - With Server-Side Encryption\n            with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as\n            it writes to disks, and decryption when you access your objects.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 12",
        "question": "The engineering team at a company wants to create a daily big data analysis job leveraging Spark for\n          analyzing online/offline sales and customer loyalty data to create customized reports on a client-by-client\n          basis. The big data analysis job needs to read the data from Amazon S3 and output it back to Amazon S3.\nWhich technology do you recommend to run the Big Data analysis job? (Select two)",
        "skipped": true,
        "choices": [
            "Amazon Athena",
            "AWS Batch",
            "Amazon EMR",
            "Amazon Redshift",
            "AWS Glue"
        ],
        "correct_answer_indices": [
            2,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Amazon EMR</strong></p>\n<p>Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open\n            source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With\n            EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions\n            and over 3x faster than standard Apache Spark. EMR is used for launching Hadoop / Spark clusters. For\n            short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For\n            long-running workloads, you can create highly available clusters that automatically scale to meet demand.\n            Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable\n            cluster of Amazon EC2 instances.</p>\n<p><strong>AWS Glue</strong></p>\n<p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to\n            prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.\n            AWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS\n            Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or\n            on-premises JDBC data stores as a target.</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data\n            warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse\n            is a collection of computing resources called nodes, which are organized into a group called a cluster. Each\n            cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster\n            consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives\n            queries from client applications, parses the queries, and develops query execution plans. The leader node\n            then coordinates the parallel execution of these plans with the compute nodes and aggregates the\n            intermediate results from these nodes. It then finally returns the results to the client applications.</p>\n<p><strong>Amazon Athena</strong> - Amazon Athena is an interactive query service that makes it easy to\n            analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure\n            to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs,\n            perform ad-hoc analysis, and run interactive queries.</p>\n<p><strong>AWS Batch</strong> - AWS Batch can be used to plan, schedule, and execute your batch computing\n            workloads on Amazon EC2 Instances. AWS Batch dynamically provisions the optimal quantity and type of compute\n            resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource\n            requirements of the batch jobs submitted.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/emr/\">https://aws.amazon.com/emr/</a></p>\n<p><a href=\"https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 13",
        "question": "A data analytics company is running a proprietary database on an Amazon EC2 instance using Amazon EBS\n          volumes. The database is heavily input/output (I/O) bound. As a solutions architect, which of the following\n          RAID configurations would you recommend improving the I/O performance?",
        "skipped": true,
        "choices": [
            "Amazon EBS does not support the standard RAID configurations",
            "Both RAID 0 and RAID 1 provide equally good I/O performance",
            "Use RAID 0 when I/O performance is more important than fault tolerance",
            "Use RAID 1 when I/O performance is more important than fault tolerance"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use RAID 0 when I/O performance is more important than fault tolerance</strong></p>\n<p>With Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional\n            bare metal server, as long as that particular RAID configuration is supported by the operating system for\n            your instance. This is because all RAID is accomplished at the software level.</p>\n<p>RAID configuration options for I/O performance v/s fault tolerance:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q37-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Use RAID 1 when I/O performance is more important than fault tolerance</strong> - This is incorrect\n            because you should use RAID 1 when fault tolerance is more important than I/O performance.</p>\n<p><strong>Both RAID 0 and RAID 1 provide equally good I/O performance</strong> - This is incorrect because\n            RAID 0 provides better I/O performance.</p>\n<p><strong>Amazon EBS does not support the standard RAID configurations</strong> - This is incorrect because\n            Amazon EBS supports the standard RAID configurations.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 14",
        "question": "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the\n          data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts\n          of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance\n          performance.\nWhich combination of steps should the solutions architect take? (Select two)",
        "skipped": true,
        "choices": [
            "Set up Amazon Kinesis Data Streams to ingest the data",
            "Set up AWS Lambda with AWS Step Functions to process the data",
            "Set up AWS Database Migration Service (AWS DMS) to ingest the data",
            "Provision Amazon EC2 instances in an Auto Scaling group to process the data",
            "Set up AWS Fargate with Amazon ECS to process the data"
        ],
        "correct_answer_indices": [
            0,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Set up Amazon Kinesis Data Streams to ingest the data</strong></p>\n<p><strong>Set up AWS Fargate with Amazon ECS to process the data</strong></p>\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS\n            can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website\n            clickstreams, database event streams, financial transactions, social media feeds, IT logs, and\n            location-tracking events. The data collected is available in milliseconds to enable real-time analytics use\n            cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n<p>AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container\n            Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on\n            building your applications. Fargate removes the need to provision and manage servers, lets you specify and\n            pay for resources per application, and improves security through application isolation by design.</p>\n<p>For the given use case, we can use Kinesis Data Streams as the ingestion layer and the containerized ECS\n            application on AWS Fargate as the processing layer. Both these components are serverless and can scale to\n            offer the desired performance.</p>\n<p>Incorrect options:</p>\n<p><strong>Set up AWS Database Migration Service (AWS DMS) to ingest the data</strong> - AWS Database\n            Migration Service helps you migrate databases to AWS quickly and securely. DMS cannot be used for real-time\n            data ingestion. Hence, this option is incorrect.</p>\n<p><strong>Set up AWS Lambda with AWS Step Functions to process the data</strong> - The maximum timeout value\n            for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the\n            execution of your Lambda function. Since the use case talks about a job that runs for 30 minutes, Lambda is\n            not an option here.</p>\n<p><strong>Provision Amazon EC2 instances in an Auto Scaling group to process the data</strong> - The given\n            requirement is for a serverless solution to process the data. Hence, provisioning an Amazon EC2 instance is\n            clearly not the right solution.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/\">https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 15",
        "question": "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and\n          the company now wants to explore an alternate solution on AWS.\nAs a solutions architect, which of the following AWS services would you recommend that can provide support\n          for quick and easy migration from RabbitMQ?",
        "skipped": true,
        "choices": [
            "Amazon SQS FIFO (First-In-First-Out)",
            "Amazon Simple Notification Service (Amazon SNS)",
            "Amazon Simple Queue Service (Amazon SQS) Standard",
            "Amazon MQ"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon MQ</strong></p>\n<p>Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate\n            message brokers in the cloud. Message brokers allow different software systems–often using different\n            programming languages, and on different platforms–to communicate and exchange information. If an\n            organization is using messaging with existing applications and wants to move the messaging service to the\n            cloud quickly and easily, AWS recommends Amazon MQ for such a use case. So this is the correct option.</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service\n            (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables\n            you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics\n            for high-throughput, push-based, many-to-many messaging. SNS does not provide support for migration from\n            RabbitMQ as its a fully managed pub/sub messaging service. Hence this option is incorrect.</p>\n<p><strong>Amazon Simple Queue Service (Amazon SQS) Standard</strong> - Amazon SQS Standard offers a reliable,\n            highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you\n            easily move data between distributed application components and helps you build applications in which\n            messages are processed independently (with message-level ack/fail semantics), such as automated workflows.\n            SQS Standard does not provide support for migration from RabbitMQ. Hence this option is incorrect.</p>\n<p><strong>Amazon SQS FIFO (First-In-First-Out)</strong> - Amazon SQS FIFO (First-In-First-Out) has all the\n            capabilities of the standard queue. They are used when the order of operations and events is critical, or\n            where duplicates can't be tolerated. SQS FIFO does not provide support for migration from RabbitMQ. Hence\n            this option is incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/amazon-mq/\">https://aws.amazon.com/amazon-mq/</a></p>\n<p><a href=\"https://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/\">https://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 16",
        "question": "A photo-sharing company is storing user profile pictures in an Amazon S3 bucket and an image analysis\n          application is deployed on four Amazon EC2 instances. A solutions architect would like to trigger an image\n          analysis procedure only on one of the four Amazon EC2 instances for each photo uploaded.\nWhat do you recommend?",
        "skipped": true,
        "choices": [
            "Create an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the\n                    Amazon EC2 instances to the Amazon SNS topic",
            "Subscribe the Amazon EC2 instances to the Amazon S3 Inventory stream",
            "Create an Amazon EventBridge event that reacts to objects uploads in Amazon S3 and invokes one of\n                    the Amazon EC2 instances",
            "Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon\n                    EC2 instances read from the Amazon SQS queue"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create an Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon\n              EC2 instances read from the Amazon SQS queue</strong></p>\n<p>The Amazon S3 event notification feature enables you to receive notifications when certain events happen in\n            your bucket. To enable notifications, you must first add a notification configuration that identifies the\n            events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the\n            notifications.</p>\n<p>Amazon S3 supports the following destinations where it can publish events:</p>\n<p>Amazon Simple Notification Service (Amazon SNS) topic</p>\n<p>Amazon Simple Queue Service (Amazon SQS) queue</p>\n<p>AWS Lambda</p>\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple\n            and scale microservices, distributed systems, and serverless applications. SQS offers two types of message\n            queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO\n            queues are designed to guarantee that messages are processed exactly once, in the exact order that they are\n            sent.</p>\n<p>Here we have to use Amazon S3 Event Notifications (which can send a message to either AWS Lambda, Amazon\n            SNS, or Amazon SQS) to send a message to the Amazon SQS queue. By using Amazon SQS, we know only one Amazon\n            EC2 instance among the four will pick up a message and process it.</p>\n<p>Incorrect options:</p>\n<p><strong>Subscribe the Amazon EC2 instances to the Amazon S3 Inventory stream</strong> - Amazon S3 Inventory\n            is a distractor. If you're curious - Amazon S3 inventory helps you manage your storage by creating lists of\n            the objects in an Amazon S3 bucket on a defined schedule.</p>\n<p><strong>Create an Amazon EventBridge event that reacts to objects uploads in Amazon S3 and invokes one of\n              the Amazon EC2 instances</strong>- Amazon EventBridge events cannot invoke applications on Amazon EC2\n            instances, so we have to rule out that answer.</p>\n<p><strong>Create an Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the\n              Amazon EC2 instances to the Amazon SNS topic</strong>- Amazon Simple Notification Service (Amazon SNS) is\n            a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple\n            microservices, distributed systems, and serverless applications.</p>\n<p>Using Amazon SNS would send a message to each Amazon EC2 instance via the Amazon SNS topic, therefore\n            making all of them work for each upload. This is not the intended behavior.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 17",
        "question": "A financial services company stores confidential data on an Amazon Simple Storage Service (S3) bucket. The\n          compliance guidelines require that files be stored with server-side encryption. The encryption used must be\n          Advanced Encryption Standard (AES-256) and the company does not want to manage the encryption keys.\nWhich of the following options represents the most cost-optimal solution for the given use case?",
        "skipped": true,
        "choices": [
            "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
            "Server-side encryption with AWS KMS keys (SSE-KMS)",
            "Client Side Encryption",
            "Server-side encryption with customer-provided keys (SSE-C)"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Server-side encryption with Amazon S3 managed keys (SSE-S3)</strong></p>\n<p>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique\n            key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a\n            master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block\n            ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. There are no\n            additional fees for using server-side encryption with Amazon S3-managed keys (SSE-S3).</p>\n<p>Incorrect options:</p>\n<p><strong>Server-side encryption with customer-provided keys (SSE-C)</strong> - You manage the encryption\n            keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.\n          </p>\n<p><strong>Client Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to\n            Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n<p><strong>Server-side encryption with AWS KMS keys (SSE-KMS)</strong> - Similar to SSE-S3 and also provides\n            you with an audit trail of when your key was used and by whom. Additionally, you have the option to create\n            and manage encryption keys yourself. Although SSE-KMS provides an option where AWS manages the encryption\n            key on your behalf, however, this entails a usage fee for the KMS key. So this option is not the best fit\n            for the given use case.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 18",
        "question": "An engineering team wants to orchestrate multiple Amazon ECS task types running on Amazon EC2 instances that\n          are part of the Amazon ECS cluster. The output and state data for all tasks need to be stored. The amount of\n          data output by each task is approximately 20 megabytes and there could be hundreds of tasks running at a time.\n          As old outputs are archived, the storage size is not expected to exceed 1 terabyte.\nAs a solutions architect, which of the following would you recommend as an optimized solution for\n          high-frequency reading and writing?",
        "skipped": true,
        "choices": [
            "Use an Amazon EBS volume mounted to the Amazon ECS cluster instances",
            "Use Amazon DynamoDB table that is accessible by all ECS cluster instances",
            "Use Amazon EFS with Provisioned Throughput mode",
            "Use Amazon EFS with Bursting Throughput mode"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p>Amazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed\n            data storage design enables file systems to grow elastically to petabyte scale. It also enables massively\n            parallel access from compute instances, including Amazon EC2, Amazon ECS, and AWS Lambda, to your data.</p>\n<p><strong>Use Amazon EFS with Provisioned Throughput mode</strong></p>\n<p>Provisioned Throughput mode is available for applications with high throughput to storage (MiB/s per TiB)\n            ratios, or with requirements greater than those allowed by the Bursting Throughput mode. For example, say\n            you're using Amazon EFS for development tools, web serving, or content management applications where the\n            amount of data in your file system is low relative to throughput demands. Your file system can now get the\n            high levels of throughput your applications require without having to pad your file system.</p>\n<p>If your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of\n            your file system as often as you want. You can decrease your file system throughput in Provisioned\n            Throughput mode as long as it's been more than 24 hours since the last decrease. Additionally, you can\n            change between Provisioned Throughput mode and the default Bursting Throughput mode as long as it’s been\n            more than 24 hours since the last throughput mode change.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q33-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Use Amazon EFS with Bursting Throughput mode</strong> - With Bursting Throughput mode, a file\n            system's throughput scales as the amount of data stored in the standard storage class grows. File-based\n            workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels\n            of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput\n            levels for periods of time. By default, AWS recommends that you run your application in the Bursting\n            Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider\n            switching to Provisioned Throughput mode.</p>\n<p>The use-case mentions that the solution should be optimized for high-frequency reading and writing even\n            when the old outputs are archived, therefore Provisioned Throughput mode is a better fit as it guarantees\n            high levels of throughput your applications require without having to pad your file system.</p>\n<p><strong>Use an Amazon EBS volume mounted to the Amazon ECS cluster instances</strong> - Amazon EFS has a\n            higher throughput than Amazon EBS. In addition, Amazon EBS can be attached to multiple Amazon EC2 instances\n            when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use-case does not provide any\n            such details, so this option is ruled out.</p>\n<p><strong>Use Amazon DynamoDB table that is accessible by all ECS cluster instances</strong> - Amazon\n            DynamoDB is not a fit for this scenario as each task output is 20 MB but the storage limit for each item in\n            a Amazon DynamoDB table is 400 KB. You could write custom code to split the task output data into multiple\n            items but it is not an optimal solution compared to using Amazon EFS in Provisioned Throughput mode.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 19",
        "question": "A Big Data analytics company is using a fleet of Amazon EC2 instances to ingest Internet-of-Things (IoT) data\n          from various data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an\n          EC2 instance is restarted, the in-flight data is lost. The analytics team at the company wants to store as\n          well as query the ingested data in near-real-time.\nWhich of the following solutions provides near-real-time data querying that is scalable with minimal data\n          loss?",
        "skipped": true,
        "choices": [
            "Capture data in an Amazon EBS volume and then publish this data to Amazon ElastiCache for Redis.\n                    Subscribe to the Redis channel to query the data",
            "Capture data in Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon\n                    Redshift to query the data",
            "Capture data in an Amazon EC2 instance store and then publish this data to Amazon Kinesis Data\n                    Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data",
            "Capture data in Amazon Kinesis Data Streams. Use Amazon Kinesis Data Analytics to query and analyze\n                    this streaming data in real-time"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Capture data in Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon\n              Redshift to query the data</strong></p>\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data\n            stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon\n            Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New\n            Relic, MongoDB, and Splunk.</p>\n<p>Amazon Kinesis Data Firehose is the easiest way to capture, transform, and load streaming data into\n            Redshift for near real-time analytics. It is also an auto-scaling solution as there is no need to provision\n            any shards like Kinesis Data Streams.</p>\n<p>Amazon Redshift allows you to run complex analytic queries against petabytes of structured data, using\n            sophisticated query optimization, columnar storage on high-performance local disks, and massively parallel\n            query execution. Most results come back in seconds.</p>\n<p>Incorrect options:</p>\n<p><strong>Capture data in an Amazon EC2 instance store and then publish this data to Amazon Kinesis Data\n              Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</strong> - Instance store\n            is a temporary storage available on Amazon EC2 instances. The in-flight data (that is, data arriving from\n            the source) being processed by a specific Amazon EC2 instance will be lost in case that instance is\n            restarted. Hence, this cannot be the option for the given use case.</p>\n<p><strong>Capture data in an Amazon EBS volume and then publish this data to Amazon ElastiCache for Redis.\n              Subscribe to the Redis channel to query the data</strong> - Amazon EBS volumes cannot be used to store\n            high volume data. Amazon EBS can be used to store cache data if a database is hosted on an Amazon EC2\n            instance. However, Amazon EBS cannot be used in place of a database. Amazon ElastiCache is a caching\n            service. It is not relevant to the given use case.</p>\n<p><strong>Capture data in Amazon Kinesis Data Streams. Use Amazon Kinesis Data Analytics to query and analyze\n              this streaming data in real-time</strong> - For Amazon Kinesis Data Streams, you have to manually allocate\n            the shards for scaling the data ingestion process. Amazon Kinesis Data Streams (KDS) and Amazon Kinesis Data\n            Analytics are for real-time processing of data and cannot provide long-term storage of data unlike a\n            database or a data warehouse. So, this option is not right for the current use case.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/redshift/features/\">https://aws.amazon.com/redshift/features/</a></p>\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 20",
        "question": "The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for\n          its MySQL Amazon RDS database in order to automate its database replication and augment data durability. The\n          DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend.\nWhich of the following is the correct outcome during the maintenance window?",
        "skipped": true,
        "choices": [
            "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment\n                    triggers both the primary and standby database instances to be upgraded at the same time. However,\n                    this does not cause any downtime until the upgrade is complete",
            "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment\n                    triggers the standby database instance to be upgraded which is then followed by the upgrade of the\n                    primary database instance. This does not cause any downtime for the duration of the upgrade",
            "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment\n                    triggers both the primary and standby database instances to be upgraded at the same time. This\n                    causes downtime until the upgrade is complete",
            "Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment\n                    triggers the primary database instance to be upgraded which is then followed by the upgrade of the\n                    standby database instance. This does not cause any downtime for the duration of the upgrade"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment\n              triggers both the primary and standby database instances to be upgraded at the same time. This causes\n              downtime until the upgrade is complete</strong></p>\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational\n            database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming\n            administration tasks such as hardware provisioning, database setup, patching, and backups.</p>\n<p>Upgrades to the database engine level require downtime. Even if your Amazon RDS DB instance uses a Multi-AZ\n            deployment, both the primary and standby DB instances are upgraded at the same time. This causes downtime\n            until the upgrade is complete, and the duration of the downtime varies based on the size of your database\n            instance.</p>\n<p>Amazon RDS DB Engine Maintenance:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q8-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment\n              triggers both the primary and standby database instances to be upgraded at the same time. However, this\n              does not cause any downtime until the upgrade is complete</strong> - For Amazon RDS database engine level\n            upgrade, primary and standby database instances are upgraded at the same time and it causes downtime until\n            the upgrade is complete, hence this option is incorrect.</p>\n<p><strong>Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment\n              triggers the standby database instance to be upgraded which is then followed by the upgrade of the primary\n              database instance. This does not cause any downtime for the duration of the upgrade</strong> - For Amazon\n            RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and\n            it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n<p><strong>Any database engine level upgrade for an Amazon RDS database instance with Multi-AZ deployment\n              triggers the primary database instance to be upgraded which is then followed by the upgrade of the standby\n              database instance. This does not cause any downtime for the duration of the upgrade</strong> - For Amazon\n            RDS database engine level upgrade, primary and standby database instances are upgraded at the same time and\n            it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 21",
        "question": "The DevOps team at an e-commerce company has deployed a fleet of Amazon EC2 instances under an Auto Scaling\n          group (ASG). The instances under the ASG span two Availability Zones (AZ) within the us-east-1\n          region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests\n          to the Amazon EC2 instances under the Auto Scaling Group. As part of a test run, two instances (instance 1 and\n          2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones (AZ) to have\n          unbalanced resources. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the\n          Application Load Balancer's health check.\nCan you identify the correct outcomes for these events? (Select two)",
        "skipped": true,
        "choices": [
            "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate\n                    by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new\n                    instances before terminating the old ones, so that rebalancing does not compromise the performance\n                    or availability of your application",
            "As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate\n                    by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old\n                    instances before launching new instances, so that rebalancing does not cause extra instances to be\n                    launched",
            "Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the\n                    unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating\n                    the unhealthy instance and then terminates it",
            "Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and\n                    launch the new instance simultaneously",
            "Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and\n                    then terminates it. Later, another scaling activity launches a new instance to replace the\n                    terminated instance"
        ],
        "correct_answer_indices": [
            0,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate\n              by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances\n              before terminating the old ones, so that rebalancing does not compromise the performance or availability\n              of your application</strong></p>\n<p>Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available\n            to handle the load for your application. You create collections of EC2 instances, called Auto Scaling\n            groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto\n            Scaling ensures that your group never goes below this size.\n            Actions such as changing the Availability Zones (AZ) for your group or explicitly terminating or detaching\n            instances can lead to the Auto Scaling group becoming unbalanced between Availability Zones. Amazon EC2 Auto\n            Scaling compensates by rebalancing the Availability Zones.</p>\n<p>When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that\n            rebalancing does not compromise the performance or availability of your application. Therefore, this option\n            is correct.</p>\n<p>Availability Zone Rebalancing Overview:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q6-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a>\n</p>\n<p><strong>Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and\n              then terminates it. Later, another scaling activity launches a new instance to replace the terminated\n              instance</strong></p>\n<p>However, the scaling activity of Auto Scaling works in a different sequence compared to the rebalancing\n            activity. Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then\n            terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.\n          </p>\n<p>Incorrect options:</p>\n<p><strong>Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the\n              unhealthy instance. Later, Amazon EC2 Auto Scaling creates a new scaling activity for terminating the\n              unhealthy instance and then terminates it</strong> - This option contradicts the correct sequence of\n            events outlined earlier for scaling activity created by Amazon EC2 Auto Scaling. Actually, Auto Scaling\n            first terminates the unhealthy instance and then launches a new instance. Hence this is incorrect.</p>\n<p><strong>As the resources are unbalanced in the Availability Zones, Amazon EC2 Auto Scaling will compensate\n              by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances\n              before launching new instances, so that rebalancing does not cause extra instances to be launched</strong>\n            - This option contradicts the correct sequence of events outlined earlier for rebalancing activity. When\n            rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones. Hence this is\n            incorrect.</p>\n<p><strong>Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and\n              launch the new instance simultaneously</strong> - This is a made-up option as both the terminate and\n            launch activities can't happen simultaneously. This option has been added as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 22",
        "question": "An e-commerce company uses Amazon RDS MySQL DB to store the data. The analytics department at the company\n          runs its reports on the same database. The engineering team has noticed sluggish performance on the database\n          when the analytics reporting process is in progress.\nAs an AWS Certified Solutions Architect - Associate, which of the following would you suggest as the MOST\n          cost-optimal solution to improve the performance?",
        "skipped": true,
        "choices": [
            "Create a read-replica with the same compute capacity and the same storage capacity as the primary.\n                    Point the reporting queries to run against the read replica",
            "Create a standby instance in a multi-AZ configuration with half compute capacity and half storage\n                    capacity as the primary. Point the reporting queries to run against the standby instance",
            "Create a standby instance in a multi-AZ configuration with the same compute capacity and the same\n                    storage capacity as the primary. Point the reporting queries to run against the standby instance",
            "Create a read-replica with half compute capacity and half storage capacity as the primary. Point\n                    the reporting queries to run against the read replica"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create a read-replica with the same compute capacity and the same storage capacity as the primary.\n              Point the reporting queries to run against the read replica</strong></p>\n<p>Amazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in\n            replication functionality to create a special type of database instance called a read replica from a source\n            database instance. The source database instance becomes the primary database instance. Updates made to the\n            primary database instance are asynchronously copied to the read replica. You can reduce the load on your\n            primary DB instance by routing read queries from your applications to the read replica.</p>\n<p>Amazon RDS Read Replicas:\n            <img src=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/read-replica.png\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a>\n</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q46-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a>\n</p>\n<p>You can use read replicas to improve the performance of your Amazon RDS MySQL DB by handling business\n            reporting or data warehousing scenarios where you might want business reporting queries to run against your\n            read replica, rather than your production database instance.</p>\n<p>You can create up to five read replicas from one DB instance. For replication to operate effectively, each\n            read replica should have the same amount of compute and storage resources as the source database instance.\n            If you scale the source database instance, also scale the read replicas.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q46-i2.jpg\"/>\n            via - <a href=\"https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html\">https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Create a read-replica with half compute capacity and half storage capacity as the primary. Point\n              the reporting queries to run against the read replica</strong> - As mentioned in the explanation above,\n            you should create a read-replica with the same compute capacity and the same storage capacity as the\n            primary.</p>\n<p><strong>Create a standby instance in a multi-AZ configuration with the same compute capacity and the same\n              storage capacity as the primary. Point the reporting queries to run against the standby instance</strong>\n</p>\n<p><strong>Create a standby instance in a multi-AZ configuration with half compute capacity and half storage\n              capacity as the primary. Point the reporting queries to run against the standby instance</strong></p>\n<p>Multi-AZ deployments are not a read scaling solution, so you cannot use a standby to serve read traffic.\n            The standby is there just for failover. Hence both these options are incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a>\n</p>\n<p><a href=\"https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html\">https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 23",
        "question": "The CTO of an online home rental marketplace wants to re-engineer the caching layer of the current\n          architecture for its relational database. The CTO wants the caching layer to have replication and archival\n          support built into the architecture.\nWhich of the following AWS service offers the capabilities required for the re-engineering of the caching\n          layer?",
        "skipped": true,
        "choices": [
            "Amazon ElastiCache for Redis",
            "Amazon DocumentDB",
            "Amazon DynamoDB Accelerator (DAX)",
            "Amazon ElastiCache for Memcached"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon ElastiCache for Redis</strong></p>\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency\n            to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time\n            transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards,\n            geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache\n            for Redis supports replication and archival snapshots right out of the box. Hence this is the correct\n            option.</p>\n<p>Exam Alert:</p>\n<p>Please review this comparison sheet for Redis vs Memcached features:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q10-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon ElastiCache for Memcached</strong> - Amazon ElastiCache for Memcached is a\n            Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon\n            ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency,\n            increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to\n            create with Amazon ElastiCache for Memcached. ElastiCache for Memcached does not support replication and\n            archival snapshots, so this option is ruled out.</p>\n<p><strong>Amazon DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB is a key-value and document database\n            that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region,\n            multi-master, durable database with built-in security, backup and restore, and in-memory caching for\n            internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from\n            fast in-memory performance for demanding applications. DAX cannot be used as a caching layer for a\n            relational database.</p>\n<p><strong>Amazon DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully\n            managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB\n            makes it easy to store, query, and index JSON data. DocumentDB cannot be used as a caching layer for a\n            relational database.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 24",
        "question": "A startup uses a fleet of Amazon EC2 servers to manage its CRM application. These Amazon EC2 servers are\n          behind Elastic Load Balancing (ELB). Which of the following configurations are NOT allowed for Elastic Load\n          Balancing?",
        "skipped": true,
        "choices": [
            "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four\n                    instances are deployed in Availability Zone B of us-west-1 region",
            "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four\n                    instances are deployed in Availability Zone A of us-east-1 region",
            "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four\n                    instances are deployed across two Availability Zones of us-east-1 region",
            "Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these\n                    instances are deployed in Availability Zone A of us-east-1 region and the other two instances are\n                    deployed in Availability Zone B of us-west-1 region"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. Two of these\n              instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed\n              in Availability Zone B of <code>us-west-1</code> region</strong></p>\n<p>Elastic Load Balancer automatically distributes incoming traffic across multiple targets – Amazon EC2\n            instances, containers, IP addresses, and Lambda functions – in multiple Availability Zones and ensures only\n            healthy targets receive traffic.\n            ELB cannot distribute incoming traffic for targets deployed in different regions. This configuration is NOT\n            allowed for the Elastic Load Balancer and therefore this is the correct option.</p>\n<p>Incorrect options:</p>\n<p><strong>Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four\n              instances are deployed across two Availability Zones of <code>us-east-1</code> region</strong></p>\n<p><strong>Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four\n              instances are deployed in Availability Zone A of <code>us-east-1</code> region</strong></p>\n<p><strong>Use the Elastic Load Balancing to distribute traffic for four Amazon EC2 instances. All the four\n              instances are deployed in Availability Zone B of <code>us-west-1</code> region</strong></p>\n<p>These three options are valid configurations for the Elastic Load Balancing to distribute traffic (either\n            within an Availability Zone or between two Availability Zones).</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/elasticloadbalancing/\">https://aws.amazon.com/elasticloadbalancing/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 25",
        "question": "An e-commerce application uses a relational database that runs several queries that perform joins on multiple\n          tables. The development team has found that these queries are slow and expensive, therefore these are a good\n          candidate for caching. The application needs to use a caching service that supports multi-threading.\nAs a solutions architect, which of the following services would you recommend for the given use case?",
        "skipped": true,
        "choices": [
            "Amazon DynamoDB Accelerator (DAX)",
            "Amazon ElastiCache for Redis",
            "Amazon ElastiCache for Memcached",
            "AWS Global Accelerator"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon ElastiCache for Memcached</strong></p>\n<p>Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data\n            store and cache in the cloud. The service improves the performance of web applications by allowing you to\n            retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower\n            disk-based databases.</p>\n<p>Memcached is an open-source, distributed, in-memory key-value store that can retrieve data in milliseconds.\n            Caching site information with Memcached can help you improve the performance and scalability of your site\n            while controlling cost.</p>\n<p>Choose Memcached if the following apply to you:</p>\n<p>You need the simplest model possible.</p>\n<p>You need to run large nodes with multiple cores or threads (support for multi-threading).</p>\n<p>You need the ability to scale out and in, adding and removing nodes as demand on your system increases and\n            decreases.</p>\n<p>You need to cache objects.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q47-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon ElastiCache for Redis</strong> - Redis, which stands for Remote Dictionary Server, is a\n            fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue.\n            Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time\n            applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for\n            caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing,\n            chat/messaging, media streaming, and pub/sub apps.</p>\n<p>Redis does not support multi-threading, so this option is not the right fit for the given use case.</p>\n<p><strong>Amazon DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed,\n            highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases.</p>\n<p><strong>AWS Global Accelerator</strong> - AWS Global Accelerator is a networking service that helps you\n            improve the availability and performance of the applications that you offer to your global users. This\n            option has been added as a distractor, it has nothing to do with database caching.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/caching/aws-caching/\">https://aws.amazon.com/caching/aws-caching/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 26",
        "question": "A company has noticed several provisioned throughput exceptions on its Amazon DynamoDB database due to major\n          spikes in the writes to the database. The development team wants to decouple the application layer from the\n          database layer and dedicate a worker process to writing the data to Amazon DynamoDB.\nWhich middleware do you recommend on using that can scale infinitely and meet these requirements in the most\n          cost effective way?",
        "skipped": true,
        "choices": [
            "Amazon Simple Queue Service (Amazon SQS)",
            "Amazon DynamoDB DAX",
            "Amazon Simple Notification Service (Amazon SNS)",
            "Amazon Kinesis Data Streams"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon Simple Queue Service (Amazon SQS)</strong></p>\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple\n            and scale microservices, distributed systems, and serverless applications. Amazon SQS offers two types of\n            message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery.\n            Amazon SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact\n            order that they are sent.</p>\n<p>Using Amazon SQS as a middleware will help us sustain the write throughput during write peaks and therefore\n            this option is the best fit for the given use-case.</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon DynamoDB DAX</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly\n            available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from\n            milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting\n            required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache\n            invalidation, data population, or cluster management.</p>\n<p>DAX is used for caching reads, not to help with writes. So this option is ruled out.</p>\n<p><strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable\n            and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from\n            hundreds of thousands of sources such as website clickstreams, database event streams, financial\n            transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis\n            data stream is designed to scale without limits via increasing the number of shards within a data stream.\n            Kinesis is used to process consistent real-time data and does not scale as cost effectively as SQS to handle\n            spikes in traffic.</p>\n<p><strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service\n            (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables\n            you to decouple microservices, distributed systems, and serverless applications. Amazon SNS won't keep our\n            data if it cannot be delivered, so this option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 27",
        "question": "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection\n          does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround\n          time to set up the connection.\nWhat is the MOST cost-effective way to establish such a connection?",
        "skipped": true,
        "choices": [
            "Set up an AWS Site-to-Site VPN connection",
            "Set up AWS Direct Connect",
            "Set up a bastion host on Amazon EC2",
            "Set up an Internet Gateway between the on-premises data center and AWS cloud"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up an AWS Site-to-Site VPN connection</strong></p>\n<p>By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network.\n            You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site\n            VPN) connection, and configuring routing to pass traffic through the connection. A VPN connection refers to\n            the connection between your VPC and your own on-premises network.</p>\n<p>An AWS Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit\n            gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises)\n            side.</p>\n<p>A virtual private gateway (VGW) is the VPN concentrator on the Amazon side of the AWS Site-to-Site VPN\n            connection. You create a virtual private gateway and attach it to the VPC from which you want to create the\n            AWS Site-to-Site VPN connection.</p>\n<p>How virtual private gateway works:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q25-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a>\n</p>\n<p>An AWS transit gateway is a transit hub that you can use to interconnect your virtual private clouds (VPC)\n            and on-premises networks. For more information, see Amazon VPC Transit Gateways. You can create a\n            Site-to-Site VPN connection as an attachment on a transit gateway.</p>\n<p>How AWS transit gateway works:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q25-i2.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Set up a bastion host on Amazon EC2</strong> - A bastion host is a server whose purpose is to\n            provide access to a private network from an external network, such as the Internet. The bastion host runs on\n            an Amazon EC2 instance that is typically in a public subnet of your Amazon VPC. Other Amazon EC2 instances\n            can be in a subnet that is not publicly accessible, and they are set up with a security group that allows\n            SSH access from the security group attached to the underlying Amazon EC2 instance running the bastion host.\n            A bastion host cannot be used to set up a connection between its on-premises data center and AWS Cloud.</p>\n<p><strong>Set up AWS Direct Connect</strong> - AWS Direct Connect is a network service that provides an\n            alternative to using the Internet to utilize AWS cloud services. AWS Direct Connect enables customers to\n            have low latency, secure and private connections to AWS for workloads that require higher speed or lower\n            latency than the internet. A Dedicated Connection is made through a 1 Gbps, 10 Gbps, or 100 Gbps Ethernet\n            port dedicated to a single customer. AWS Direct Connect takes about a month to provision the connection, so\n            this option is ruled out for the given use case.</p>\n<p><strong>Set up an Internet Gateway between the on-premises data center and AWS cloud</strong> - An Internet\n            Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication\n            between your VPC and the internet. An Internet Gateway cannot be used to set up a connection between its\n            on-premises data center and AWS Cloud.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 28",
        "question": "You are looking to build an index of your files in Amazon S3, using Amazon RDS PostgreSQL. To build this\n          index, it is necessary to read the first 250 bytes of each object in Amazon S3, which contains some metadata\n          about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50\n          terabytes of data.\nHow can you build this index efficiently?",
        "skipped": true,
        "choices": [
            "Create an application that will traverse the Amazon S3 bucket, read all the files one by one,\n                    extract the first 250 bytes, and store that information in Amazon RDS",
            "Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL\n                    query to build the index",
            "Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250\n                    bytes, and store that information in Amazon RDS",
            "Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch\n                    parameter to get the first 250 bytes, and store that information in Amazon RDS"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250\n              bytes, and store that information in Amazon RDS</strong></p>\n<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading\n            scalability, data availability, security, and performance.</p>\n<p>Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object,\n            transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different\n            byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single\n            whole-object request. Fetching smaller ranges of a large object also allows your application to improve\n            retry times when requests are interrupted.</p>\n<p>A byte-range request is a perfect way to get the beginning of a file and ensuring we remain efficient\n            during our scan of our Amazon S3 bucket. So this is the correct option.</p>\n<p>Incorrect options:</p>\n<p><strong>Use the Amazon RDS Import feature to load the data from Amazon S3 to PostgreSQL, and run a SQL\n              query to build the index</strong> - You cannot import data from Amazon S3 into Amazon RDS, so this option\n            is incorrect.</p>\n<p><strong>Create an application that will traverse the Amazon S3 bucket, read all the files one by one,\n              extract the first 250 bytes, and store that information in Amazon RDS</strong> - If you build an\n            application that loads all the files from Amazon S3, that would work, but you would read 50TB of data and\n            that may be very expensive and slow. So this option is incorrect.</p>\n<p><strong>Create an application that will traverse the Amazon S3 bucket, then use S3 Select Byte Range Fetch\n              parameter to get the first 250 bytes, and store that information in Amazon RDS</strong> - Amazon S3 Select\n            is a new Amazon S3 capability designed to pull out only the data you need from an object, which can\n            dramatically improve the performance and reduce the cost of applications that need to access data in Amazon\n            S3. You cannot use Byte Range Fetch parameter with S3 Select to traverse the Amazon S3 bucket and get the\n            first bytes of a file. So this option is incorrect.</p>\n<p>Exam Alert:</p>\n<p>Please note that with Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes\n            to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by\n            splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use\n            the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte).</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q20-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html</a>\n</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 29",
        "question": "A company has media files that need to be shared internally. Users are first authenticated using Active\n          Directory and then they access files on a Microsoft Windows platform. The engineering manager wants to keep\n          the same user permissions but wants the company to migrate the storage layer to AWS Cloud as the company is\n          reaching its storage capacity limit on the on-premises infrastructure.\nWhat should a solutions architect recommend to meet this requirement?",
        "skipped": true,
        "choices": [
            "Create a corporate Amazon S3 bucket and move all media files",
            "Provision Amazon EC2 with Windows OS, attach multiple Amazon EBS volumes, and move all media files",
            "Set up Amazon EFS and move all media files",
            "Set up Amazon FSx for Windows File Server and move all the media files"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up Amazon FSx for Windows File Server and move all the media files</strong></p>\n<p>Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that\n            is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server,\n            delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft\n            Active Directory (AD) integration. To support a wide spectrum of workloads, Amazon FSx provides high levels\n            of throughput and IOPS and consistent sub-millisecond latencies.</p>\n<p>Amazon FSx file storage is accessible from Windows, Linux, and macOS compute instances and devices running\n            on AWS or on-premises. Thousands of compute instances and devices can access a file system concurrently.\n            Amazon FSx for Windows File Server supports Microsoft Active Directory (AD) integration so the same user\n            permissions and access credentials can be used to access the files on FSx Windows File Server.</p>\n<p>Incorrect options:</p>\n<p><strong>Create a corporate Amazon S3 bucket and move all media files</strong> - Amazon S3 is object-based\n            storage and it does not support file storage. Hence S3 is not the correct option.</p>\n<p><strong>Set up Amazon EFS and move all media files</strong> - Amazon EFS provides scalable file storage for\n            use with Amazon EC2. You can use an EFS file system as a common data source for workloads and applications\n            running on multiple instances. EFS is not compatible with the Windows platform, so this option is ruled out.\n          </p>\n<p><strong>Provision Amazon EC2 with Windows OS, attach multiple Amazon EBS volumes, and move all media\n              files</strong> - Multi-attach Amazon EBS volumes are supported only for Nitro EC2 instances which are\n            Linux-based. So this option is ruled out.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 30",
        "question": "An application running on an Amazon EC2 instance needs to access a Amazon DynamoDB table in the same AWS\n          account.\nWhich of the following solutions should a solutions architect configure for the necessary permissions?",
        "skipped": true,
        "choices": [
            "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table.\n                    Store the access credentials in the local storage and read them from within the application code\n                    directly",
            "Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table.\n                    Store the access credentials in an Amazon S3 bucket and read them from within the application code\n                    directly",
            "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB\n                    table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance",
            "Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB\n                    table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance\n                    can assume the role"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB\n              table. Configure an instance profile to assign this IAM role to the Amazon EC2 instance</strong></p>\n<p>A service role is an IAM role that a service assumes to perform actions on your behalf. Service roles\n            provide access only within your account and cannot be used to grant access to services in other accounts. An\n            IAM administrator can create, modify, and delete a service role from within IAM. When you create the service\n            role, you define the <code>trusted entity</code> in the definition.</p>\n<p>If you are going to use the role with Amazon EC2 or another AWS service that uses Amazon EC2, you must\n            store the role in an instance profile. An instance profile is a container for a role that can be attached to\n            an Amazon EC2 instance when launched. An instance profile can contain only one role, and that limit cannot\n            be increased. If you create the role using the AWS Management Console, the instance profile is created for\n            you with the same name as the role.</p>\n<p>Incorrect options:</p>\n<p><strong>Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table.\n              Store the access credentials in an Amazon S3 bucket and read them from within the application code\n              directly</strong></p>\n<p><strong>Set up an IAM user with the appropriate permissions to allow access to the Amazon DynamoDB table.\n              Store the access credentials in the local storage and read them from within the application code\n              directly</strong></p>\n<p>You should never store the IAM access credentials for a user in Amazon S3 or local storage or a database.\n            It's a security bad practice. It is always recommended to use IAM roles to configure access to other AWS\n            resources from Amazon EC2 instances. Therefore both these options are incorrect.</p>\n<p><strong>Set up an IAM service role with the appropriate permissions to allow access to the Amazon DynamoDB\n              table. Add the Amazon EC2 instance to the trust relationship policy document so that the instance can\n              assume the role</strong> - There is no need for this option because when you create an IAM service role\n            for Amazon EC2, the role automatically has Amazon EC2 identified as a trusted entity. Therefore this option\n            is not correct.</p>\n<p>Configuring a Service Role:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q34-i1.jpg\"/>\n</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 31",
        "question": "A company is developing a document management application on AWS. The application runs on Amazon EC2\n          instances in multiple Availability Zones (AZs). The company requires the document store to be highly available\n          and the documents need to be returned immediately when requested. The engineering team has configured the\n          application to use Amazon Elastic Block Store (Amazon EBS) to store the documents but the team is willing to\n          consider other options to meet the availability requirement.\nAs a solutions architect, which of the following will you recommend?",
        "skipped": true,
        "choices": [
            "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use\n                    Amazon S3 as the document store",
            "Provision at least three Provisioned IOPS Amazon EBS volumes for the Amazon EC2 instances and then\n                    mount these volumes to the Amazon EC2 instances in a RAID 5 configuration",
            "Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use\n                    Amazon S3 Glacier as the document store",
            "Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those\n                    snapshots in additional Availability Zones"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use\n              Amazon S3 as the document store</strong></p>\n<p>Instances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. When\n            you launch an Amazon EBS-backed instance, AWS creates an Amazon EBS volume for each Amazon EBS snapshot\n            referenced by the AMI you use. An Amazon EBS-backed instance can be stopped and later restarted without\n            affecting data stored in the attached volumes.</p>\n<p>Amazon S3 provides access to reliable, fast, and inexpensive data storage infrastructure. It is designed to\n            make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from\n            within Amazon EC2 or anywhere on the web. S3 is highly available and can be configured to work as a document\n            store for the given use case.</p>\n<p>Incorrect options:</p>\n<p><strong>Set up Amazon EBS as the Amazon EC2 instance root volume and then configure the application to use\n              Amazon S3 Glacier as the document store</strong> - As the documents need to be returned immediately when\n            requested, Amazon S3 Glacier is not the right fit, since there is a lag of several minutes/hours when you\n            want to read data from Glacier.</p>\n<p><strong>Create snapshots for the Amazon EBS volumes regularly and then build new volumes using those\n              snapshots in additional Availability Zones</strong> - You can back up the data on your Amazon EBS volumes\n            to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the\n            blocks on the device that have changed after your most recent snapshot are saved. Hence, using Amazon EBS\n            volumes as a primary storage solution is ineffective, and creating recurring snapshots is a management\n            nightmare for the current use case.</p>\n<p><strong>Provision at least three Provisioned IOPS Amazon EBS volumes for the Amazon EC2 instances and then\n              mount these volumes to the Amazon EC2 instances in a RAID 5 configuration</strong> - RAID 5 and RAID 6 are\n            not recommended for Amazon EBS because the parity write operations of these RAID modes consume some of the\n            IOPS available to your volumes. Depending on the configuration of your RAID array, these RAID modes provide\n            20-30% fewer usable IOPS than a RAID 0 configuration. So this option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 32",
        "question": "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage\n          firewall rules across its accounts and applications using AWS Organizations.\nWhich of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
        "skipped": true,
        "choices": [
            "Amazon GuardDuty",
            "Amazon Inspector",
            "AWS Shield Advanced",
            "VPC Security Groups",
            "AWS Web Application Firewall (AWS WAF)",
            "Network access control list (network ACL)"
        ],
        "correct_answer_indices": [
            2,
            3,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>AWS Web Application Firewall (AWS WAF)</strong></p>\n<p><strong>AWS Shield Advanced</strong></p>\n<p><strong>VPC Security Groups</strong></p>\n<p>AWS Firewall Manager is a security management service which allows you to centrally configure and manage\n            firewall rules across your accounts and applications in AWS Organizations. As new applications are created,\n            Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common\n            set of security rules. Now you have a single service to build firewall rules, create security policies, and\n            enforce them in a consistent, hierarchical manner across your entire infrastructure.</p>\n<p>Using AWS Firewall Manager, you can centrally configure AWS WAF rules, AWS Shield Advanced protection,\n            Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS\n            Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of\n            today.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q4-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/firewall-manager/faqs/\">https://aws.amazon.com/firewall-manager/faqs/</a></p>\n<p>Incorrect options:</p>\n<p><strong>Amazon GuardDuty</strong> - Amazon GuardDuty offers threat detection that enables you to\n            continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. Amazon\n            GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in\n            AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs.</p>\n<p>How Amazon GuardDuty Works:\n            <img src=\"https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png\"/>\n</p>\n<p><strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps\n            you test the network accessibility of your Amazon EC2 instances and the security state of your applications\n            running on the instances.</p>\n<p><strong>Network access control list (network ACL)</strong> - A network access control list (ACL) is an\n            optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or\n            more subnets.</p>\n<p>These three options are not in the list of AWS resources supported by AWS Firewall Manager, so these\n            options are incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/firewall-manager/faqs/\">https://aws.amazon.com/firewall-manager/faqs/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 33",
        "question": "An e-commerce website is migrating towards a microservices-based approach for their website and plans to\n          expose their website from the same load balancer, linked to different target groups with different URLs:\n          checkout.mycorp.com, www.mycorp.com, mycorp.com/products, and mycorp.com/orders. The website would like to use\n          Amazon ECS on the backend to manage these microservices and possibly host the same container of the\n          application multiple times on the same Amazon EC2 instance.\nWhich feature can help you achieve this with minimal effort?",
        "skipped": true,
        "choices": [
            "Classic Load Balancer + dynamic port mapping",
            "Application Load Balancer + Reverse Proxy running as a Docker daemon on each Amazon ECS host",
            "Network Load Balancer + dynamic port mapping",
            "Application Load Balancer + dynamic port mapping"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Application Load Balancer + dynamic port mapping</strong></p>\n<p>Application Load Balancer can automatically distribute incoming application traffic across multiple\n            targets, such as Amazon EC2 instances, containers, IP addresses, and AWS Lambda functions. It can handle the\n            varying load of your application traffic in a single Availability Zone or across multiple Availability Zones\n            (AZs).</p>\n<p>Dynamic port mapping with an Application Load Balancer makes it easier to run multiple tasks on the same\n            Amazon ECS service on an Amazon ECS cluster.</p>\n<p>Incorrect option:</p>\n<p><strong>Application Load Balancer + Reverse Proxy running as a Docker daemon on each Amazon ECS\n              host</strong> - Dynamic Port Mapping is available for the Application Load Balancer. A reverse proxy\n            solution would work but would be too much work to manage. Here the Application Load Balancer has a feature\n            that provides a direct dynamic port mapping feature and integration with the Amazon ECS service so we will\n            leverage that.</p>\n<p><strong>Classic Load Balancer + dynamic port mapping</strong> - Classic Load Balancer provides basic load\n            balancing across multiple Amazon EC2 instances and operates at both the request level and connection level.\n            Classic Load Balancer is intended for applications that were built within the Amazon EC2-Classic network.\n          </p>\n<p>With the Classic Load Balancer, you must statically map port numbers on a container instance. The Classic\n            Load Balancer does not allow you to run multiple copies of a task on the same instance because of the ports\n            conflict. An Application Load Balancer uses dynamic port mapping so that you can run multiple tasks from a\n            single service on the same container instance.</p>\n<p><strong>Network Load Balancer + dynamic port mapping</strong> - Network Load Balancer is best suited for\n            use-cases involving low latency and high throughput workloads that involve scaling to millions of requests\n            per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets\n            - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC)\n            based on IP protocol data.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 34",
        "question": "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage\n          system on AWS Cloud. The archived data would be accessed for just about a week in a year.\nAs a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal\n          solution?",
        "skipped": true,
        "choices": [
            "Amazon S3 Standard-IA",
            "Amazon EFS Infrequent Access",
            "Amazon EFS Standard",
            "Amazon S3 Standard"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon EFS Infrequent Access</strong></p>\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file\n            system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a\n            storage class that provides price/performance that is cost-optimized for files not accessed every day, with\n            storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only\n            $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by\n            selecting a lifecycle policy that matches your needs.</p>\n<p>How Amazon EFS Infrequent Access Works:\n            <img src=\"https://d1.awsstatic.com/EFS/product-page-diagram-Amazon-EFS-Infrequent-Access-How-It-Works.83f88e30a40c27f38abae1ff157712a336dd1320.png\"/>\n            via - <a href=\"https://aws.amazon.com/efs/features/infrequent-access/\">https://aws.amazon.com/efs/features/infrequent-access/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon EFS Standard</strong> - Amazon EFS Infrequent Access is more cost-effective than EFS\n            Standard for the given use-case, therefore this option is incorrect.</p>\n<p><strong>Amazon S3 Standard</strong></p>\n<p><strong>Amazon S3 Standard-IA</strong></p>\n<p>Both these options are object-based storage, whereas the given use-case requires a POSIX compliant file\n            storage solution. Hence these two options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/efs/features/infrequent-access/\">https://aws.amazon.com/efs/features/infrequent-access/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 35",
        "question": "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read\n          access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets.\nHow can you provide these users access in the least possible time, with minimal changes?",
        "skipped": true,
        "choices": [
            "Create a group, attach the policy to the group and place the users in the group",
            "Update the Amazon S3 bucket policy",
            "Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM\n                    with AWS MFA",
            "Create a policy and assign it manually to the 50 users"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create a group, attach the policy to the group and place the users in the group</strong></p>\n<p>An IAM group is a collection of IAM users. You can use groups to specify permissions for a collection of\n            users, which can make those permissions easier to manage for those users. For example, you could have a\n            group called Admins and give that group the types of permissions that administrators typically need. Any\n            user in that group automatically has the permissions that are assigned to the group. If a new user joins\n            your organization and should have administrator privileges, you can assign the appropriate permissions by\n            adding the user to that group.</p>\n<p>Here creating a group, assigning users to that group and attaching policies to that group is the best way.\n          </p>\n<p>Incorrect options:</p>\n<p><strong>Update the Amazon S3 bucket policy</strong> - Updating the Amazon S3 bucket policy could work but\n            would not scale, as the size of the S3 bucket policy is limited (Bucket policies are limited to 20 KB in\n            size).</p>\n<p><strong>Create a policy and assign it manually to the 50 users</strong> - An IAM user is an entity that you\n            create in AWS. The IAM user represents the person or service who uses the IAM user to interact with AWS.\n            Primary use for IAM users is to give people the ability to sign in to the AWS Management Console for\n            interactive tasks and to make programmatic requests to AWS services using the API or CLI. A user in AWS\n            consists of a name, a password to sign in to the AWS Management Console, and up to two access keys that can\n            be used with the API or CLI.</p>\n<p>A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.\n            AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the\n            policies determine whether the request is allowed or denied.</p>\n<p>Identity-based policies – Attach managed and inline policies to IAM identities (users, groups to which\n            users belong, or roles). Identity-based policies grant permissions to an identity.</p>\n<p>Resource-based policies – Attach inline policies to resources. The most common examples of resource-based\n            policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant\n            permissions to the principal that is specified in the policy. Principals can be in the same account as the\n            resource or in other accounts.</p>\n<p>Creating a policy and assigning it manually to users would work but would be hard to scale and manage.</p>\n<p><strong>Create an AWS Multi-Factor Authentication (AWS MFA) user with read / write access and link 50 IAM\n              with AWS MFA</strong> - AWS MFA adds extra security because it requires users to provide unique\n            authentication from an AWS supported MFA mechanism in addition to their regular sign-in credentials when\n            they access AWS websites or services. AWS MFA cannot help in terms of granting read/write access to only 50\n            of the IAM users.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 36",
        "question": "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application that\n          needs to support relational queries. The application will have unpredictable spikes of usage that the team\n          does not know in advance.\nWhich database would you recommend using?",
        "skipped": true,
        "choices": [
            "Amazon ElastiCache",
            "Amazon Aurora Serverless",
            "Amazon DynamoDB with On-Demand Capacity",
            "Amazon DynamoDB with Provisioned Capacity and Auto Scaling"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon Aurora Serverless</strong></p>\n<p>Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible\n            and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale\n            capacity up or down based on your application's needs. It enables you to run your database in the cloud\n            without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent,\n            or unpredictable workloads. The database design for an OLTP application fits the relational model, therefore\n            you can infer an OLTP system as a Relational Database.</p>\n<p>Amazon Aurora Serverless is the perfect way to create a database that can scale down to 0 servers, and\n            scale up to many servers, as an OLTP database. So this is the correct option.</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon DynamoDB with Provisioned Capacity and Auto Scaling</strong></p>\n<p><strong>Amazon DynamoDB with On-Demand Capacity</strong></p>\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at\n            any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup\n            and restore, and in-memory caching for internet-scale applications.</p>\n<p>Amazon DynamoDB is a NoSQL database and doesn't do relational queries, therefore it's a choice we have to\n            eliminate, even though the two modes proposed here help us cope with an unpredictable amount of usage. So\n            both these options are incorrect.</p>\n<p><strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale\n            popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the\n            performance of your existing databases by retrieving data from high throughput and low latency in-memory\n            data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores,\n            Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Amazon Elasticache is used as a caching layer\n            in front of relational databases. Amazon ElastiCache is a NoSQL database and doesn't facilitate relational\n            queries, so this option is ruled out.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/rds/aurora/serverless/\">https://aws.amazon.com/rds/aurora/serverless/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/rds/\">https://aws.amazon.com/rds/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 37",
        "question": "A company uses a legacy on-premises reporting application that operates on gigabytes of .json files and\n          represents years of data. The legacy application cannot handle the growing size of .json files. New .json\n          files are added daily from various data sources to a central on-premises storage location. The company wants\n          to continue to support the legacy application. The company has hired you as a solutions architect to build a\n          solution that can manage ongoing data updates from your on-premises application to Amazon S3.\nWhich of the following solutions would you suggest to address the given requirement?",
        "skipped": true,
        "choices": [
            "Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume\n                    gateway. Point the legacy analytics application to the volume gateway. The volume gateway should\n                    replicate data to Amazon S3",
            "Set up an on-premises file gateway. Configure data sources to write the .json files to the file\n                    gateway. Point the legacy analytics application to the file gateway. The file gateway should\n                    replicate the .json files to Amazon S3",
            "Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files\n                    between the company's on-premises storage and the company's Amazon S3 bucket",
            "Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files\n                    between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS\n                    to the company's Amazon S3 bucket"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up an on-premises file gateway. Configure data sources to write the .json files to the file\n              gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the\n              .json files to Amazon S3</strong></p>\n<p>A file gateway provides a simple solution for presenting one or more Amazon S3 buckets and their objects as\n            a mountable NFS or SMB file share to one or more clients on-premises.</p>\n<p>The file gateway is deployed as a virtual machine in VMware ESXi or Microsoft Hyper-V environments\n            on-premises, or in an Amazon Elastic Compute Cloud (Amazon EC2) instance in AWS. File gateway can also be\n            deployed in data center and remote office locations on a Storage Gateway hardware appliance. When deployed,\n            file gateway provides a seamless connection between on-premises NFS (v3.0 or v4.1) or SMB (v1 or v2)\n            clients—typically applications—and Amazon S3 buckets hosted in a given AWS Region. The file gateway employs\n            a local read/write cache to provide low-latency access to data for file share clients in the same local area\n            network (LAN) as the file gateway.</p>\n<p>A bucket share consists of a file share hosted from a file gateway across a single Amazon S3 bucket. The\n            file gateway virtual machine appliance currently supports up to 10 bucket shares.</p>\n<p>File Gateway Architecture:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q31-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html\">https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume\n              gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate\n              data to Amazon S3</strong> - The Volume Gateway provides block storage to your on-premises applications\n            using iSCSI connectivity. Data on the volumes is stored in Amazon S3 and you can take point in time copies\n            of volumes that are stored in AWS as Amazon EBS snapshots. Volume Gateway is for block storage and not for\n            file storage, so it is not the right option.</p>\n<p><strong>Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files\n              between the company's on-premises storage and the company's Amazon S3 bucket</strong></p>\n<p><strong>Set up AWS DataSync on-premises. Configure AWS DataSync to continuously replicate the .json files\n              between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the\n              company's Amazon S3 bucket</strong></p>\n<p>AWS recommends that you should use AWS DataSync to migrate existing data to Amazon S3, and subsequently use\n            the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing\n            updates from your on-premises file-based applications. Therefore, both these options are incorrect, as they\n            use DataSync for ongoing replication.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html\">https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 38",
        "question": "A digital media streaming company wants to use Amazon CloudFront to distribute its content only to its\n          service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver\n          restricted content to the bona fide end users? (Select two)",
        "skipped": true,
        "choices": [
            "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers",
            "Require HTTPS for communication between Amazon CloudFront and your S3 origin",
            "Use Amazon CloudFront signed cookies",
            "Use Amazon CloudFront signed URLs",
            "Require HTTPS for communication between Amazon CloudFront and your custom origin"
        ],
        "correct_answer_indices": [
            2,
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Use Amazon CloudFront signed URLs</strong></p>\n<p>Many companies that distribute content over the internet want to restrict access to documents, business\n            data, media streams, or content that is intended for selected users, for example, users who have paid a fee.\n          </p>\n<p>To securely serve this private content by using Amazon CloudFront, you can do the following:</p>\n<p>Require that your users access your private content by using special Amazon CloudFront signed URLs or\n            signed cookies.</p>\n<p>A signed URL includes additional information, for example, expiration date and time, that gives you more\n            control over access to your content. So this is a correct option.</p>\n<p><strong>Use Amazon CloudFront signed cookies</strong></p>\n<p>Amazon CloudFront signed cookies allow you to control who can access your content when you don't want to\n            change your current URLs or when you want to provide access to multiple restricted files, for example, all\n            of the files in the subscribers' area of a website. So this is also a correct option.</p>\n<p>Incorrect options:</p>\n<p><strong>Require HTTPS for communication between Amazon CloudFront and your custom origin</strong></p>\n<p><strong>Require HTTPS for communication between Amazon CloudFront and your S3 origin</strong></p>\n<p>Requiring HTTPS for communication between Amazon CloudFront and your custom origin (or S3 origin) only\n            enables secure access to the underlying content. You cannot use HTTPS to restrict access to your private\n            content. So both these options are incorrect.</p>\n<p><strong>Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers</strong> - This\n            option is just added as a distractor. You cannot use HTTPS to restrict access to your private content.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 39",
        "question": "A startup wants to create a highly available architecture for its multi-tier application. Currently, the\n          startup manages a single Amazon EC2 instance along with a single Amazon RDS MySQL DB instance. The startup has\n          hired you as an AWS Certified Solutions Architect - Associate to build a solution that meets these\n          requirements while minimizing the underlying infrastructure maintenance effort.\nWhat will you recommend?",
        "skipped": true,
        "choices": [
            "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a\n                    single Availability Zone. Configure an Application Load Balancer having a target group of these\n                    Amazon EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration",
            "Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS\n                    MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming\n                    traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs",
            "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across\n                    two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon\n                    EC2 instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone",
            "Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across\n                    two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon\n                    EC2 instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across\n              two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2\n              instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration</strong></p>\n<p>Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances\n            automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the\n            load for your application.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q53-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a>\n</p>\n<p>Application Load Balancer automatically distributes your incoming traffic across multiple targets, such as\n            Amazon EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the\n            health of its registered targets, and routes traffic only to the healthy targets.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q53-i2.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a>\n</p>\n<p>In a multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica\n            in a different Availability Zone. Updates to your DB Instance are synchronously replicated across\n            Availability Zones to the standby to keep both in sync and protect your latest database updates against DB\n            instance failure.</p>\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q53-i3.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n<p>To create a highly available architecture for the given use case, you need to set up an Auto-Scaling group\n            with a desired capacity of a total of two Amazon EC2 instances across two Availability Zones and then point\n            the Application Load Balancer to the target group having the Amazon EC2 instances.</p>\n<p>Incorrect options:</p>\n<p><strong>Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances across\n              two Availability Zones. Configure an Application Load Balancer having a target group of these Amazon EC2\n              instances. Set up a read replica of the Amazon RDS MySQL DB in another Availability Zone</strong> - A read\n            replica cannot be used to enhance the availability of an Amazon RDS MySQL DB. You must use the multi-AZ\n            configuration of Amazon RDS MySQL for this use case.</p>\n<p><strong>Create an Auto-Scaling group with a desired capacity of a total of two Amazon EC2 instances in a\n              single Availability Zone. Configure an Application Load Balancer having a target group of these Amazon EC2\n              instances. Set up Amazon RDS MySQL DB in a multi-AZ configuration</strong> - Having the Amazon EC2\n            instances in a single Availability Zone will not create a highly available solution. In the case of an\n            outage for the entire Availability Zone, the Amazon EC2 instances would be unreachable. Hence this option is\n            incorrect.</p>\n<p><strong>Provision a second Amazon EC2 instance in another Availability Zone. Provision a second Amazon RDS\n              MySQL DB in another Availabililty Zone. Leverage Amazon Route 53 for equal distribution of incoming\n              traffic to the Amazon EC2 instances. Use a custom script to sync data across the two MySQL DBs</strong> -\n            This option has been added as a distractor. It requires significant monitoring and development effort to\n            keep the Amazon EC2 instances highly available as well as keep the MySQL DBs in sync.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 40",
        "question": "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used\n          for cryptocurrency mining. The Amazon EC2 instance does not host any authorized application related to\n          cryptocurrency mining.\nWhich AWS service can be used to protect the Amazon EC2 instances from such unauthorized behavior in the\n          future?",
        "skipped": true,
        "choices": [
            "AWS Shield Advanced",
            "AWS Web Application Firewall (AWS WAF)",
            "Amazon GuardDuty",
            "AWS Firewall Manager"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon GuardDuty</strong></p>\n<p>Amazon GuardDuty continuously monitors for malicious or unauthorized behavior to help protect your AWS\n            resources, including your AWS accounts and access keys. Amazon GuardDuty identifies any unusual or\n            unauthorized activity, like cryptocurrency mining or infrastructure deployments in a region that has never\n            been used. Powered by threat intelligence and machine learning, GuardDuty is continuously evolving to help\n            you protect your AWS environment.</p>\n<p>The cryptocurrency finding expands the service’s ability to detect Amazon EC2 instances querying IP\n            addresses associated with the cryptocurrency-related activity. The finding type is:\n            CryptoCurrency:EC2/BitcoinTool.B, CryptoCurrency:EC2/BitcoinTool.B!DNS.</p>\n<p>This finding informs you that the listed Amazon EC2 instance in your AWS environment is querying a domain\n            name that is associated with Bitcoin or other cryptocurrency-related activity. Bitcoin is a worldwide\n            cryptocurrency and digital payment system that can be exchanged for other currencies, products, and\n            services. Bitcoin is a reward for bitcoin mining and is highly sought after by threat actors.</p>\n<p>If you use the Amazon EC2 instance to mine or manage cryptocurrency, or this instance is otherwise involved\n            in blockchain activity, this finding could represent expected activity for your environment. If this is the\n            case in your AWS environment, AWS recommends that you set up a suppression rule for this finding.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS Web Application Firewall (AWS WAF)</strong> - AWS WAF is a web application firewall that helps\n            protect your web applications or APIs against common web exploits and bots that may affect availability,\n            compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your\n            applications by enabling you to create security rules that control bot traffic and block common attack\n            patterns, such as SQL injection or cross-site scripting.</p>\n<p><strong>AWS Shield Advanced</strong> - For higher levels of protection against attacks targeting your\n            applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront,\n            AWS Global Accelerator, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition\n            to the network and transport layer protections that come with Standard, AWS Shield Advanced provides\n            additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility\n            into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you\n            24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS-related spikes in your Amazon\n            Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and\n            Amazon Route 53 charges.</p>\n<p><strong>AWS Firewall Manager</strong> - AWS Firewall Manager is a security management service that allows\n            you to centrally configure and manage firewall rules across your accounts and applications in AWS\n            Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and\n            resources into compliance by enforcing a common set of security rules. Now you have a single service to\n            build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across\n            your entire infrastructure, from a central administrator account.</p>\n<p>None of these three services can detect unauthorized cryptocurrency mining activity on EC2 instances, so\n            these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 41",
        "question": "A company is deploying a publicly accessible web application. To accomplish this, the engineering team has\n          designed the VPC with a public subnet and a private subnet. The application will be hosted on several Amazon\n          EC2 instances in an Auto Scaling group. The team also wants Transport Layer Security (TLS) termination to be\n          offloaded from the Amazon EC2 instances.\nWhich solution should a solutions architect implement to address these requirements in the most secure\n          manner?",
        "skipped": true,
        "choices": [
            "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private\n                    subnet and associate it with the Network Load Balancer",
            "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public\n                    subnet and associate it with the Network Load Balancer",
            "Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private\n                    subnet and associate it with the Network Load Balancer",
            "Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public\n                    subnet and associate it with the Network Load Balancer"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private\n              subnet and associate it with the Network Load Balancer</strong></p>\n<p>A load balancer serves as the single point of contact for clients. The load balancer distributes incoming\n            traffic across multiple targets, such as Amazon EC2 instances. This increases the availability of your\n            application. You add one or more listeners to your load balancer.</p>\n<p>With a Network Load Balancer, you can offload the decryption/encryption of Transport Layer Security (TLS)\n            traffic from your application servers to the Network Load Balancer, which helps you optimize the performance\n            of your backend application servers while keeping your workloads secure. Additionally, Network Load\n            Balancers preserve the source IP of the clients to the back-end applications, while terminating Transport\n            Layer Security (TLS) on the load balancer.</p>\n<p>An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping\n            for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon\n            EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the\n            number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon\n            EC2 Auto Scaling service.</p>\n<p>The NLB has to be accessible over the internet and hence has to be in a public subnet and will act as a\n            single point-of-contact for all incoming traffic. NLB will forward the incoming traffic to the Amazon EC2\n            instances managed by the ASG in the private subnet.</p>\n<p>Exam Alert:</p>\n<p>You should note that the Application Load Balancer also supports Transport Layer Security (TLS) offloading.\n            The Classic Load Balancer supports SSL offloading.</p>\n<p>Incorrect options:</p>\n<p><strong>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public\n              subnet and associate it with the Network Load Balancer</strong> - The Auto Scaling group with its target\n            EC2 instances should be in the private subnet to avoid access to EC2 instances over the public internet.\n            Having EC2 instances in the public subnet would weaken the security posture of the application. Hence, this\n            option is incorrect.</p>\n<p><strong>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public\n              subnet and associate it with the Network Load Balancer</strong></p>\n<p><strong>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private\n              subnet and associate it with the Network Load Balancer</strong></p>\n<p>NLB should be in the public subnet as it represents the internet-facing component of the web tier.\n            Therefore, both these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/\">https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 42",
        "question": "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should\n          also support configuring a trust relationship with any existing on-premises Microsoft Active Directory.\nWhich AWS Directory Service is the best fit for this requirement?",
        "skipped": true,
        "choices": [
            "AWS Transit Gateway",
            "AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)",
            "Simple Active Directory (Simple AD)",
            "Active Directory Connector"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD)</strong></p>\n<p>AWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. AWS Directory\n            Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, is powered by Windows\n            Server 2012 R2. When you select and launch this directory type, it is created as a highly available pair of\n            domain controllers connected to your virtual private cloud (VPC).</p>\n<p>With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, including Microsoft\n            SharePoint and custom .NET and SQL Server-based applications. You can also configure a trust relationship\n            between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory,\n            providing users and groups with access to resources in either domain, using single sign-on (SSO).</p>\n<p>AWS Managed Microsoft AD is your best choice if you need actual Active Directory features to support AWS\n            applications or Windows workloads, including Amazon Relational Database Service for Microsoft SQL Server.\n            It's also best if you want a standalone AD in the AWS Cloud that supports Office 365 or you need an LDAP\n            directory to support your Linux applications.</p>\n<p>Incorrect options:</p>\n<p><strong>Active Directory Connector</strong> - AD Connector is a directory gateway with which you can\n            redirect directory requests to your on-premises Microsoft Active Directory without caching any information\n            in the cloud. AD Connector is your best choice when you want to use your existing on-premises directory with\n            compatible AWS services.</p>\n<p><strong>Simple Active Directory (Simple AD)</strong> - Simple AD is a standalone directory in the cloud,\n            where you create and manage user identities and manage access to applications. Simple AD provides a subset\n            of the features offered by AWS Managed Microsoft AD. However, note that Simple AD does not support features\n            such as multi-factor authentication (MFA), trust relationships with other domains, Active Directory\n            Administrative Center, PowerShell support, Active Directory recycle bin, group managed service accounts, and\n            schema extensions for POSIX and Microsoft applications.</p>\n<p><strong>AWS Transit Gateway</strong> - AWS Transit Gateway connects VPCs and on-premises networks through a\n            central hub. Transit Gateway is not an Active Directory service.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 43",
        "question": "A company manages a High Performance Computing (HPC) application that needs to be deployed on Amazon EC2\n          instances. The application requires high levels of inter-node communications and high network traffic between\n          the instances.\nAs a solutions architect, which of the following options would you recommend to the engineering team at the\n          company? (Select two)",
        "skipped": true,
        "choices": [
            "Deploy Amazon EC2 instances behind a Network Load Balancer",
            "Deploy Amazon EC2 instances in a partition placement group",
            "Deploy Amazon EC2 instances in a spread placement group",
            "Deploy Amazon EC2 instances with Elastic Fabric Adapter (EFA)",
            "Deploy Amazon EC2 instances in a cluster placement group"
        ],
        "correct_answer_indices": [
            3,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Deploy Amazon EC2 instances with Elastic Fabric Adapter (EFA)</strong></p>\n<p>Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run\n            applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating\n            system (OS) bypass hardware interface enhances the performance of inter-instance communications, which is\n            critical to scaling these applications. Therefore this option is correct.</p>\n<p><strong>Deploy Amazon EC2 instances in a cluster placement group</strong></p>\n<p>Cluster placement groups pack instances close together inside an Availability Zone. They are recommended\n            when the majority of the network traffic is between the instances in the group. These are also recommended\n            for applications that benefit from low network latency, high network throughput, or both. Therefore this\n            option is one of the correct answers.</p>\n<p>Incorrect options:</p>\n<p><strong>Deploy Amazon EC2 instances in a spread placement group</strong> - A spread placement group is a\n            group of instances that are each placed on distinct racks, with each rack having its own network and power\n            source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can\n            have a maximum of seven running instances per Availability Zone per group. Since the spread placement group\n            can span across multiple Availability Zones in the same Region, it cannot support high levels of inter-node\n            communications and high network traffic. So this option is incorrect.</p>\n<p><strong>Deploy Amazon EC2 instances in a partition placement group</strong> - A partition placement group\n            spreads your instances across logical partitions such that groups of instances in one partition do not share\n            the underlying hardware with groups of instances in different partitions. This strategy is typically used by\n            large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement\n            group can have a maximum of seven partitions per Availability Zone. Since the partition placement group can\n            have partitions in multiple Availability Zones in the same Region, it cannot support high levels of\n            inter-node communications and high network traffic. So this option is incorrect.</p>\n<p><strong>Deploy Amazon EC2 instances behind a Network Load Balancer</strong> - A load balancer serves as the\n            single point of contact for clients. The load balancer distributes incoming traffic across multiple targets,\n            such as Amazon EC2 instances. A Network Load Balancer functions at the fourth layer of the Open Systems\n            Interconnection (OSI) model. Network Load Balancer cannot facilitate high network traffic between instances.\n            Network Load Balancer cannot support high levels of inter-node communication between EC2 instances. This\n            option just serves as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 44",
        "question": "A retail company's procurement application becomes slow when traffic spikes. The application has a three-tier\n          architecture (web, application and database tier) that uses synchronous transactions. The engineering team at\n          the company has identified certain bottlenecks in the application tier but it does not want to change the\n          underlying application architecture.\nAs a solutions architect, which of the following solutions would you suggest to meet the required application\n          response times while accounting for any traffic spikes?",
        "skipped": true,
        "choices": [
            "Leverage Amazon SQS with asynchronous AWS Lambda calls to decouple the application and data tiers",
            "Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS",
            "Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance\n                    size",
            "Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and\n                    Application Load Balancer"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and\n              Application Load Balancer</strong></p>\n<p>A horizontally scalable system is one that can increase capacity by adding more computers to the system.\n            This is in contrast to a vertically scalable system, which is constrained to running its processes on only\n            one computer; in such systems, the only way to increase performance is to add more resources into one\n            computer in the form of faster (or more) CPUs, memory or storage.</p>\n<p>Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling\n            parallel execution of workloads and distributing those across many different computers.</p>\n<p>Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the\n            Amazon EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by\n            optimally routing traffic so that no one instance is overwhelmed.</p>\n<p>To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto\n            Scaling group to register the group with the load balancer. Your load balancer acts as a single point of\n            contact for all incoming web traffic to your Auto Scaling group.</p>\n<p>When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual\n            Amazon EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are\n            automatically registered with the load balancer. Likewise, instances that are terminated by your Auto\n            Scaling group are automatically deregistered from the load balancer.</p>\n<p>This option will require fewer design changes, it's mostly configuration changes and the ability for the\n            web/application tier to be able to communicate across instances. Hence, this is the right solution for the\n            current use case.</p>\n<p>Incorrect options:</p>\n<p><strong>Leverage Amazon SQS with asynchronous AWS Lambda calls to decouple the application and data\n              tiers</strong> - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses\n            synchronous transactions. The question says there should be no change in the application architecture.</p>\n<p><strong>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on\n              AWS</strong> - The issue is not with the persistence layer at all. This option has only been used as a\n            distractor.</p>\n<p>You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images\n            (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that\n            allows a single database (a set of data files) to be concurrently accessed and served by one or many\n            database server instances.</p>\n<p><strong>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance\n              size</strong> - Vertical scaling is just a band-aid solution and will not work long term.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 45",
        "question": "A Big Data company wants to optimize its daily Extract-Transform-Load (ETL) process that migrates and\n          transforms data from its Amazon S3 based data lake to an Amazon Redshift cluster. The team wants to manage\n          this daily job in a serverless environment.\nWhich AWS service is the best fit to manage this process without the need to configure or manage the\n          underlying compute resources?",
        "skipped": true,
        "choices": [
            "Amazon EMR",
            "AWS Data Pipeline",
            "AWS Database Migration Service (DMS)",
            "AWS Glue"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>AWS Glue</strong></p>\n<p>AWS Glue provides a managed ETL service that runs on a serverless Apache Spark environment. This allows you\n            to focus on your ETL job and not worry about configuring and managing the underlying compute resources. AWS\n            Glue takes a data-first approach and allows you to focus on the data properties and data manipulation to\n            transform the data to a form where you can derive business insights. It provides an integrated data catalog\n            that makes metadata available for ETL as well as querying via Amazon Athena and Amazon Redshift Spectrum.\n          </p>\n<p>Create a unified catalog to find data across multiple data stores using AWS Glue:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q48-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a>\n</p>\n<p>AWS Glue automates much of the effort required for data integration. AWS Glue crawls your data sources,\n            identifies data formats, and suggests schemas to store your data. It automatically generates the code to run\n            your data transformations and loading processes. You can use AWS Glue to easily run and manage thousands of\n            ETL jobs or to combine and replicate data across multiple data stores using SQL.</p>\n<p>AWS Glue runs in a serverless environment. There is no infrastructure to manage, and AWS Glue provisions,\n            configures, and scales the resources required to run your data integration jobs. You pay only for the\n            resources your jobs use while running.</p>\n<p>AWS Glue is the right fit since the company is looking at a managed ETL service without having the overhead\n            of configuring, maintaining, or managing any servers.</p>\n<p><img src=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/images/pattern-img/1f854a3e-44d4-4d70-9cd2-d61f852e3231/images/f26e2ee3-74be-49f1-8290-cd81e4ef9465.png\"/>\n            via - <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>AWS Data Pipeline</strong> - AWS Data Pipeline provides a managed orchestration service that gives\n            you greater flexibility in terms of the execution environment, access and control over the compute resources\n            that run your code, as well as the code itself that does data processing. AWS Data Pipeline launches compute\n            resources in your account allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters. As\n            this option provides access to the underlying EC2 instances so it's not a serverless solution. Therefore\n            this option is incorrect for the given use case.</p>\n<p><strong>Amazon EMR</strong> - EMR is a web service to easily and cost-effectively process vast amounts of\n            data. EMR utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic\n            Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). As this option provides access to\n            the underlying Amazon EC2 instances so it's not a serverless solution. Therefore this option is incorrect\n            for the given use case.</p>\n<p><strong>AWS Database Migration Service (DMS)</strong> - AWS Database Migration Service (DMS) helps you\n            migrate databases to AWS easily and securely. For use cases that require a database migration from\n            on-premises to AWS or database replication between on-premises sources and sources on AWS, AWS recommends\n            you use AWS DMS. Once your data is in AWS, you can use AWS Glue to move, combine, replicate, and transform\n            data from your data source into another database or data warehouse, such as Amazon Redshift. As the use-case\n            talks about data migration and transformation between AWS services, so AWS Glue is a better fit than DMS.\n          </p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/glue/faqs/\">https://aws.amazon.com/glue/faqs/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 46",
        "question": "A company has moved its business critical data to Amazon Elastic File System (Amazon EFS) which will be\n          accessed by multiple Amazon EC2 instances.\nAs an AWS Certified Solutions Architect - Associate, which of the following would you recommend to exercise\n          access control such that only the permitted Amazon EC2 instances can read from the Amazon EFS file system?\n          (Select two)",
        "skipped": true,
        "choices": [
            "Use an IAM policy to control access for clients who can mount your file system with the required\n                    permissions",
            "Set up the IAM policy root credentials to control and configure the clients accessing the Amazon\n                    EFS file system",
            "Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system",
            "Use network access control list (network ACL) to control the network traffic to and from your\n                    Amazon EC2 instance",
            "Use VPC security groups to control the network traffic to and from your file system"
        ],
        "correct_answer_indices": [
            0,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Use VPC security groups to control the network traffic to and from your file system</strong></p>\n<p><strong>Use an IAM policy to control access for clients who can mount your file system with the required\n              permissions</strong></p>\n<p>You control which Amazon EC2 instances can access your Amazon EFS file system by using VPC security group\n            rules and AWS Identity and Access Management (IAM) policies. Use VPC security groups to control the network\n            traffic to and from your file system. Attach an IAM policy to your file system to control which clients can\n            mount your file system and with what permissions, and you may use Amazon EFS Access Points to manage\n            application access. Control access to files and directories with POSIX-compliant user and group-level\n            permissions.</p>\n<p>Files and directories in an Amazon EFS file system support standard Unix-style read, write, and execute\n            permissions based on the user ID and group IDs. When an NFS client mounts an Amazon EFS file system without\n            using an access point, the user ID and group ID provided by the client is trusted. You can also use Amazon\n            EFS access points to override user ID and group IDs used by the NFS client. When users attempt to access\n            files and directories, Amazon EFS checks their user IDs and group IDs to verify that each user has\n            permission to access the objects.</p>\n<p>Incorrect options:</p>\n<p><strong>Use network access control list (network ACL) to control the network traffic to and from your\n              Amazon EC2 instance</strong> - Network ACLs operate at the subnet level and not at the instance level.</p>\n<p><strong>Set up the IAM policy root credentials to control and configure the clients accessing the Amazon\n              EFS file system</strong> - There is no such thing as an IAM policy root credentials and this statement has\n            been added as a distractor.</p>\n<p><strong>Use Amazon GuardDuty to curb unwanted access to Amazon EFS file system</strong> - Amazon GuardDuty\n            is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to\n            protect your AWS accounts, workloads, and data stored in Amazon S3. It cannot be used for access control to\n            the Amazon EFS file system.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html\">https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html\">https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 47",
        "question": "The systems administrator at a company wants to set up a highly available architecture for a bastion host\n          solution.\nAs a solutions architect, which of the following options would you recommend as the solution?",
        "skipped": true,
        "choices": [
            "Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts\n                    managed by an Auto Scaling Group",
            "Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto\n                    Scaling Group",
            "Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts\n                    managed by an Auto Scaling Group",
            "Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts\n                    managed by an Auto Scaling Group"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create a public Network Load Balancer that links to Amazon EC2 instances that are bastion hosts\n              managed by an Auto Scaling Group</strong></p>\n<p>Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that\n            involve scaling to millions of requests per second. Network Load Balancer operates at the connection level\n            (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within\n            Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.</p>\n<p>Including bastion hosts in your VPC environment enables you to securely connect to your Linux instances\n            without exposing your environment to the Internet. After you set up your bastion hosts, you can access the\n            other instances in your VPC through Secure Shell (SSH) connections on Linux. Bastion hosts are also\n            configured with security groups to provide fine-grained ingress control.</p>\n<p>You need to remember that Bastion Hosts are using the SSH protocol, which is a TCP based protocol on port\n            22. They must be publicly accessible.</p>\n<p>Here, the correct answer is to use a Network Load Balancer, which supports TCP traffic, and will\n            automatically allow you to connect to the Amazon EC2 instance in the backend.</p>\n<p>Incorrect options:</p>\n<p><strong>Create an elastic IP address (EIP) and assign it to all Amazon EC2 instances that are bastion hosts\n              managed by an Auto Scaling Group</strong> - An elastic IP address (EIP) can only be attached to one Amazon\n            EC2 instance at a time, so it won't provide you a highly available setup on its own. Note that if we had two\n            Elastic IPs and two Bastion Hosts, this would work.</p>\n<p><strong>Create a VPC Endpoint for a fleet of Amazon EC2 instances that are bastion hosts managed by an Auto\n              Scaling Group</strong> - A VPC endpoint enables you to privately connect your VPC to supported AWS\n            services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT\n            device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP\n            addresses to communicate with resources in the service. Traffic between your VPC and the other service does\n            not leave the Amazon network.</p>\n<p>VPC Endpoints are not used on top of Amazon EC2 instances. They're a way to access AWS services privately\n            within your VPC (without using the public internet). This is a distractor.</p>\n<p><strong>Create a public Application Load Balancer that links to Amazon EC2 instances that are bastion hosts\n              managed by an Auto Scaling Group</strong> - Application Load Balancer (ALB) operates at the request level\n            (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses and AWS Lambda\n            functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic,\n            Application Load Balancer provides advanced request routing targeted at delivery of modern application\n            architectures, including microservices and container-based applications.</p>\n<p>An Application Load Balancer only supports HTTP traffic, which is layer 7, while the SSH protocol is based\n            on TCP and is layer 4. So, the Application Load Balancer doesn't work.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html\">https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 48",
        "question": "You are using AWS Lambda to implement a batch job for a big data analytics workflow. Based on historical\n          trends, a similar job runs for 30 minutes on average. The AWS Lambda function pulls data from Amazon S3,\n          processes it, and then writes the results back to Amazon S3. When you deployed your AWS Lambda function, you\n          noticed an issue where the AWS Lambda function abruptly failed after 15 minutes of execution.\nAs a solutions architect, which of the following would you identify as the root cause of the issue?",
        "skipped": true,
        "choices": [
            "The AWS Lambda function chosen runtime is wrong",
            "The AWS Lambda function is timing out",
            "The AWS Lambda function is missing IAM permissions",
            "The AWS Lambda function is running out of memory"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time\n            you consume.</p>\n<p>With AWS Lambda, you can run code for virtually any type of application or backend service - all with zero\n            administration. Just upload your code and Lambda takes care of everything required to run and scale your\n            code with high availability. You can set up your code to automatically trigger from other AWS services or\n            call it directly from any web or mobile app. AWS Lambda functions can be configured to run up to 15 minutes\n            per execution. You can set the timeout to any value between 1 second and 15 minutes.</p>\n<p><strong>The AWS Lambda function is timing out</strong></p>\n<p>AWS Lambda functions time out after 15 minutes, and are not usually meant for long-running jobs.</p>\n<p>Incorrect options:</p>\n<p><strong>The AWS Lambda function is running out of memory</strong> - Memory errors will not result in the\n            abrupt termination of the function with no error message.</p>\n<p><strong>The AWS Lambda function chosen runtime is wrong</strong> - AWS Lambda function execution will fail\n            if there is an issue with runtime. So, this is not the issue in the current case.</p>\n<p><strong>The AWS Lambda function is missing IAM permissions</strong> - Without enough permissions, AWS\n            Lambda would not have been able to start its execution at all. So, permissions are not an issue here.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/lambda/faqs/\">https://aws.amazon.com/lambda/faqs/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 49",
        "question": "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as\n          internaldomainexample.com. The team wants to use the private hosted zones feature of Amazon Route 53 to\n          accomplish this.\nWhich of the following settings of the VPC need to be enabled? (Select two)",
        "skipped": true,
        "choices": [
            "enableDnsSupport",
            "enableVpcHostnames",
            "enableDnsDomain",
            "enableVpcSupport",
            "enableDnsHostnames"
        ],
        "correct_answer_indices": [
            0,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>enableDnsHostnames</strong></p>\n<p><strong>enableDnsSupport</strong></p>\n<p>A private hosted zone is a container for records for a domain that you host in one or more Amazon virtual\n            private clouds (VPCs). You create a hosted zone for a domain (such as example.com), and then you create\n            records to tell Amazon Route 53 how you want traffic to be routed for that domain within and among your\n            VPCs.</p>\n<p>For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to\n            true:</p>\n<p>enableDnsHostnames</p>\n<p>enableDnsSupport</p>\n<p>Incorrect options:</p>\n<p><strong>enableVpcSupport</strong></p>\n<p><strong>enableVpcHostnames</strong></p>\n<p><strong>enableDnsDomain</strong></p>\n<p>The options enableVpcSupport, enableVpcHostnames and enableDnsDomain have been added as distractors.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 50",
        "question": "Your e-commerce application is using an Amazon RDS PostgreSQL database and an analytics workload also runs on\n          the same database. When the analytics workload is run, your e-commerce application slows down which further\n          affects your sales.\nWhich of the following is the MOST cost-optimal solution to fix this issue?",
        "skipped": true,
        "choices": [
            "Migrate the analytics application to AWS Lambda",
            "Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby database",
            "Create a Read Replica in another Region as the Master database and point the analytics workload\n                    there",
            "Create a Read Replica in the same Region as the Master database and point the analytics workload\n                    there"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create a Read Replica in the same Region as the Master database and point the analytics workload\n              there</strong></p>\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They\n            make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy\n            database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS\n            creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native\n            asynchronous replication to update the read replica whenever there is a change to the source database\n            instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.</p>\n<p>Creating a Read Replica is the answer. As we want to minimize the costs, we need to launch the Read Replica\n            in the same Region as you are not charged for the data transfer incurred in replicating data between your\n            source database instance and read replica within the same AWS Region.</p>\n<p>Exam Alert:</p>\n<p>Please review this comparison vis-a-vis Multi-AZ vs Read Replica for Amazon RDS:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q19-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Enable Multi-AZ for the Amazon RDS database and run the analytics workload on the standby\n              database</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS\n            database (DB) instances, making them a natural fit for production database workloads. When you provision a\n            Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates\n            the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two\n            Availability Zones within a single region.</p>\n<p>Enabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and\n            cannot be used for reads or write. It's just a database that will become primary when the other database\n            encounters a failure. So this option is not correct.</p>\n<p><strong>Migrate the analytics application to AWS Lambda</strong>- AWS Lambda lets you run code without\n            provisioning or managing servers. You pay only for the compute time you consume.</p>\n<p>Running the application on AWS Lambda will not help, as it will still run against the main database and\n            slow down our e-commerce application.</p>\n<p><strong>Create a Read Replica in another Region as the Master database and point the analytics workload\n              there</strong> - This is incorrect because we have to pay for inter-Region data replication charges for\n            the Read Replica, whereas the replication of data within a single Region is free.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a>\n</p>\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 51",
        "question": "You have deployed a database technology that has a synchronous replication mode to survive disasters in data\n          centers. The database is therefore deployed on two Amazon EC2 instances in two Availability Zones (AZs). The\n          database must be publicly available so you have deployed the Amazon EC2 instances in public subnets. The\n          replication protocol currently uses the Amazon EC2 public IP addresses.\nWhat can you do to decrease the replication cost?",
        "skipped": true,
        "choices": [
            "Use an Elastic Fabric Adapter (EFA)",
            "Use the Amazon EC2 instances private IP for the replication",
            "Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the replication",
            "Create a Private Link between the two Amazon EC2 instances"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use the Amazon EC2 instances private IP for the replication</strong></p>\n<p>The source of the cost is that traffic between two EC2 instances is going over the public internet, thus\n            incurring high costs. Here, the correct answer is to use Private IP, so that the network remains private,\n            for a minimal cost.</p>\n<p>Incorrect options:</p>\n<p><strong>Assign elastic IP address (EIP) to the Amazon EC2 instances and use them for the\n              replication</strong> - Using Elastic IPs will not solve the problem as the traffic will still be going\n            over the public internet.</p>\n<p><strong>Create a Private Link between the two Amazon EC2 instances</strong> - AWS PrivateLink simplifies\n            the security of data shared with cloud-based applications by eliminating the exposure of data to the public\n            Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises\n            applications, securely on the Amazon network.</p>\n<p>Private Link is a distractor in this question. Private Link is leveraged to create a private connection\n            between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in\n            another account, without the need of VPC peering and allowing the connections between the two to remain\n            within the AWS network.</p>\n<p><strong>Use an Elastic Fabric Adapter (EFA)</strong> - The Elastic Fabric Adapter (EFA) is a network\n            interface for Amazon EC2 instances that enables customers to run HPC applications requiring high levels of\n            inter-instance communications, like computational fluid dynamics, weather modeling, and reservoir\n            simulation, at scale on AWS. This option is not relevant to the given use-case.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/privatelink/\">https://aws.amazon.com/privatelink/</a></p>\n<p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 52",
        "question": "The engineering team at a startup is evaluating the most optimal block storage volume type for the Amazon EC2\n          instances hosting its flagship application. The storage volume should support very low latency but it does not\n          need to persist the data when the instance terminates. As a solutions architect, you have proposed using\n          Instance Store volumes to meet these requirements.\nWhich of the following would you identify as the key characteristics of the Instance Store volumes? (Select\n          two)",
        "skipped": true,
        "choices": [
            "Instance store is reset when you stop or terminate an instance. Instance store data is preserved\n                    during hibernation",
            "An instance store is a network storage type",
            "You can specify instance store volumes for an instance when you launch or restart it",
            "If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store\n                    volumes isn't preserved",
            "You can't detach an instance store volume from one instance and attach it to a different instance"
        ],
        "correct_answer_indices": [
            3,
            4
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>You can't detach an instance store volume from one instance and attach it to a different\n              instance</strong></p>\n<p>You can specify instance store volumes for an instance only when you launch it. You can't detach an\n            instance store volume from one instance and attach it to a different instance. The data in an instance store\n            persists only during the lifetime of its associated instance. If an instance reboots (intentionally or\n            unintentionally), data in the instance store persists.</p>\n<p><strong>If you create an Amazon Machine Image (AMI) from an instance, the data on its instance store\n              volumes isn't preserved</strong></p>\n<p>If you create an AMI from an instance, the data on its instance store volumes isn't preserved and isn't\n            present on the instance store volumes of the instances that you launch from the AMI.</p>\n<p>Incorrect options:</p>\n<p><strong>Instance store is reset when you stop or terminate an instance. Instance store data is preserved\n              during hibernation</strong> - When you stop, hibernate, or terminate an instance, every block of storage\n            in the instance store is reset. Therefore, this option is incorrect.</p>\n<p><strong>You can specify instance store volumes for an instance when you launch or restart it</strong> - You\n            can specify instance store volumes for an instance only when you launch it.</p>\n<p><strong>An instance store is a network storage type</strong> - An instance store provides temporary\n            block-level storage for your instance. This storage is located on disks that are physically attached to the\n            host computer.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 53",
        "question": "A security consultant is designing a solution for a company that wants to provide developers with individual\n          AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the\n          individual developers will have AWS account root user-level access to their own accounts, the consultant wants\n          to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not\n          modified.\nWhich of the following actions meets the given requirements?",
        "skipped": true,
        "choices": [
            "Configure a new trail in AWS CloudTrail from within the developer accounts with the organization\n                    trails option enabled",
            "Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to\n                    the developer accounts",
            "Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only\n                    from an Amazon Resource Name (ARN) in the master account",
            "Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root user"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Set up a service control policy (SCP) that prohibits changes to AWS CloudTrail, and attach it to\n              the developer accounts</strong></p>\n<p>Service control policy (SCP) is a type of organization policy that you can use to manage permissions in\n            your organization. SCPs offer central control over the maximum available permissions for all accounts in\n            your organization. SCPs help you to ensure your accounts stay within your organization’s access control\n            guidelines.</p>\n<p>An SCP restricts permissions for IAM users and roles in member accounts, including the member account's\n            root user. Any account has only those permissions permitted by every parent above it. If a permission is\n            blocked at any level above the account, either implicitly (by not being included in an Allow policy\n            statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected\n            account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM\n            policy with <em>/</em> permissions to the user.</p>\n<p>SCPs don't affect users or roles in the management account. They affect only the member accounts in your\n            organization.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure a new trail in AWS CloudTrail from within the developer accounts with the organization\n              trails option enabled</strong> - Configuring each developer account individually is not a viable solution\n            to start with. In addition, any configuration changes can be undone by the user once they are logged into\n            their individual accounts as root users.</p>\n<p><strong>Set up an IAM policy that prohibits changes to AWS CloudTrail and attach it to the root\n              user</strong> - The root user can modify this IAM policy itself, so this option is not correct.</p>\n<p><strong>Set up a service-linked role for AWS CloudTrail with a policy condition that allows changes only\n              from an Amazon Resource Name (ARN) in the master account</strong> - A service-linked role is a unique type\n            of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service\n            and include all the permissions that the service requires to call other AWS services on your behalf. The\n            linked service also defines how you create, modify, and delete a service-linked role.</p>\n<p>The linked service defines the permissions of its service-linked roles, and unless defined otherwise, only\n            that service can assume the roles. The defined permissions include the trust policy and the permissions\n            policy, and that permissions policy cannot be attached to any other entity such as the ARN in the master\n            account.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 54",
        "question": "A company maintains its business-critical customer data on an on-premises system in an encrypted format. Over\n          the years, the company has transitioned from using a single encryption key to multiple encryption keys by\n          dividing the data into logical chunks. With the decision to move all the data to an Amazon S3 bucket, the\n          company is now looking for a technique to encrypt each file with a different encryption key to provide maximum\n          security to the migrated on-premises data.\nHow will you implement this requirement without adding the overhead of splitting the data into logical\n          groups?",
        "skipped": true,
        "choices": [
            "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3\n                    managed keys (SSE-S3) to encrypt the data",
            "Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique\n                    keys for each file of data",
            "Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS\n                    (SSE-KMS) and use encryption context to generate a different key for each file/object that you store\n                    in the S3 bucket",
            "Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with\n                    Amazon S3 managed keys (SSE-S3) to encrypt the data"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3\n              managed keys (SSE-S3) to encrypt the data</strong></p>\n<p>Server-side encryption is the encryption of data at its destination by the application or service that\n            receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers\n            and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys\n            (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself\n            with a root key that it regularly rotates.</p>\n<p>Note: Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level\n            of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3\n            will be automatically encrypted at no additional cost and with no impact on performance.</p>\n<p>Incorrect options:</p>\n<p><strong>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with\n              Amazon S3 managed keys (SSE-S3) to encrypt the data</strong> - Server-side encryption with Amazon S3\n            managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional\n            overhead of splitting data. Multiple S3 buckets are redundant for this requirement.</p>\n<p><strong>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique\n              keys for each file of data</strong> - Server-side encryption is the encryption of data at its destination\n            by the application or service that receives it. The requirement is about server-side encryption and not\n            about client-side encryption, hence this choice is incorrect.</p>\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS\n              (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in\n              the S3 bucket</strong> - An encryption context is a set of key-value pairs that contain additional\n            contextual information about the data. When an encryption context is specified for an encryption operation,\n            Amazon S3 must specify the same encryption context for the decryption operation. The encryption context\n            offers another level of security for the encryption key. However, it is not useful for generating unique\n            keys.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 55",
        "question": "A company has multiple Amazon EC2 instances operating in a private subnet which is part of a custom VPC.\n          These instances are running an image processing application that needs to access images stored on Amazon S3.\n          Once each image is processed, the status of the corresponding record needs to be marked as completed in a\n          Amazon DynamoDB table.\nHow would you go about providing private access to these AWS resources which are not part of this custom VPC?",
        "skipped": true,
        "choices": [
            "Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the\n                    custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using\n                    the private IP address",
            "Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target\n                    entries for these two gateway endpoints in the route table of the custom VPC",
            "Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these\n                    services using the private IP address",
            "Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom\n                    VPC. Create an interface endpoint for Amazon DynamoDB and then connect to the DynamoDB service using\n                    the private IP address"
        ],
        "correct_answer_indices": [
            1
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Create a separate gateway endpoint for Amazon S3 and Amazon DynamoDB each. Add two new target\n              entries for these two gateway endpoints in the route table of the custom VPC</strong></p>\n<p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC\n            components. They allow communication between instances in your VPC and services without imposing\n            availability risks or bandwidth constraints on your network traffic.</p>\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint\n            services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or\n            AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with\n            resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.\n          </p>\n<p>There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is\n            an elastic network interface with a private IP address from the IP address range of your subnet that serves\n            as an entry point for traffic destined to a supported service.</p>\n<p>A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic\n            destined to a supported AWS service. The following AWS services are supported:</p>\n<p>Amazon S3</p>\n<p>Amazon DynamoDB</p>\n<p>Incorrect options:</p>\n<p><strong>Create a gateway endpoint for Amazon S3 and add it as a target in the route table of the custom\n              VPC. Create an interface endpoint for Amazon DynamoDB and then connect to the DynamoDB service using the\n              private IP address</strong></p>\n<p><strong>Create a separate interface endpoint for Amazon S3 and Amazon DynamoDB each. Then connect to these\n              services using the private IP address</strong></p>\n<p>Amazon DynamoDB does not support interface endpoints, so these two options are incorrect.</p>\n<p><strong>Create a gateway endpoint for Amazon DynamoDB and add it as a target in the route table of the\n              custom VPC. Create an Origin Access Identity for Amazon S3 and then connect to the S3 service using the\n              private IP address</strong> - Origin Access Identity (OAI) is used within the context of Amazon\n            CloudFront. To restrict access to content that you serve from Amazon S3 buckets, you can create a special\n            Amazon CloudFront user called an origin access identity (OAI) and associate it with your distribution. You\n            cannot use OAI to facilitate access to Amazon S3 from a VPC.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 56",
        "question": "To support critical production workloads that require maximum resiliency, a company wants to configure\n          network connections between its Amazon VPC and the on-premises infrastructure. The company needs AWS Direct\n          Connect connections with speeds greater than 1 Gbps.\nAs a solutions architect, which of the following will you suggest as the best architecture for this\n          requirement?",
        "skipped": true,
        "choices": [
            "Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency",
            "Opt for one AWS Direct Connect connection at each of the multiple Direct Connect locations",
            "Opt for at least two AWS Direct Connect connections terminating on different devices at a single\n                    Direct Connect location",
            "Opt for two separate AWS Direct Connect connections terminating on separate devices in more than\n                    one Direct Connect location"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Opt for two separate AWS Direct Connect connections terminating on separate devices in more than\n              one Direct Connect location</strong></p>\n<p>Maximum resilience is achieved by separate connections terminating on separate devices in more than one\n            location. This configuration offers customers maximum resilience to failure. As shown in the figure above,\n            such a topology provides resilience to device failure, connectivity failure, and complete location failure.\n            You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS\n            Direct Connect locations.</p>\n<p>Maximum Resiliency for Critical Workloads:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i1.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Opt for one AWS Direct Connect connection at each of the multiple Direct Connect locations</strong>\n            - For critical production workloads that require high resiliency, it is recommended to have one connection\n            at multiple locations. As shown in the figure below, such a topology ensures resilience to connectivity\n            failure due to a fiber cut or a device failure as well as a complete location failure. You can use Direct\n            Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect location.\n          </p>\n<p>High Resiliency for Critical Workloads:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i2.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a>\n</p>\n<p><strong>Opt for at least two AWS Direct Connect connections terminating on different devices at a single\n              Direct Connect location</strong> - For non-critical production workloads and development workloads that do\n            not require high resiliency, it is recommended to have at least two connections terminating on different\n            devices at a single location. As shown in the figure above, such a topology helps in the case of the device\n            failure at a location but does not help in the event of a total location failure.</p>\n<p>Non Critical Production Workloads or Development Workloads:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i3.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a>\n</p>\n<p><strong>Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum\n              resiliency</strong> - It is important to understand that AWS Managed VPN supports up to 1.25 Gbps\n            throughput per VPN tunnel and does not support Equal Cost Multi-Path (ECMP) for egress data path in the case\n            of multiple AWS Managed VPN tunnels terminating on the same VGW. Thus, AWS does not recommend customers use\n            AWS Managed VPN as a backup for AWS Direct Connect connections with speeds greater than 1 Gbps.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 57",
        "question": "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone (AZ). The engineering\n          team wants to set up shared data access for these Amazon EC2 instances using Amazon EBS Multi-Attach volumes.\n        \nWhich Amazon EBS volume type is the correct choice for these Amazon EC2 instances?",
        "skipped": true,
        "choices": [
            "Throughput Optimized HDD Amazon EBS volumes",
            "Cold HDD Amazon EBS volumes",
            "General-purpose SSD-based Amazon EBS volumes",
            "Provisioned IOPS SSD Amazon EBS volumes"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Provisioned IOPS SSD Amazon EBS volumes</strong></p>\n<p>Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple\n            instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an\n            instance or set of instances. Each instance to which the volume is attached has full read and write\n            permission to the shared volume. Multi-Attach makes it easier for you to achieve higher application\n            availability in clustered Linux applications that manage concurrent write operations.</p>\n<p>Multi-Attach is supported exclusively on Provisioned IOPS SSD volumes.</p>\n<p>Incorrect options:</p>\n<p><strong>General-purpose SSD-based Amazon EBS volumes</strong> - These SSD-backed Amazon EBS volumes provide\n            a balance of price and performance. AWS recommends these volumes for most workloads. These volume types are\n            not supported for Multi-Attach functionality.</p>\n<p><strong>Throughput Optimized HDD Amazon EBS volumes</strong> - These HDD-backed volumes provide a low-cost\n            HDD designed for frequently accessed, throughput-intensive workloads. These volume types are not supported\n            for Multi-Attach functionality.</p>\n<p><strong>Cold HDD Amazon EBS volumes</strong> - These HDD-backed volumes provide a lowest-cost HDD design\n            for less frequently accessed workloads. These volume types are not supported for Multi-Attach functionality.\n          </p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 58",
        "question": "Your company has created a data warehouse using Amazon Redshift that is used to analyze data from Amazon S3.\n          From the usage pattern, you have detected that after 30 days, the data is rarely queried in Amazon Redshift\n          and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get\n          the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum\n          amount of cost on Amazon Redshift.\nWhat do you recommend? (Select two)",
        "skipped": true,
        "choices": [
            "Move the data to Amazon S3 Standard IA after 30 days",
            "Create a smaller Amazon Redshift Cluster with the cold data",
            "Analyze the cold data with Amazon Athena",
            "Move the data to Amazon S3 Glacier after 30 days",
            "Migrate the Amazon Redshift underlying storage to Amazon S3 IA"
        ],
        "correct_answer_indices": [
            0,
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct options:</p>\n<p><strong>Move the data to Amazon S3 Standard IA after 30 days</strong></p>\n<p>Amazon S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed.\n            Amazon S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low\n            per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3\n            Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The\n            minimum storage duration charge is 30 days.</p>\n<p><strong>Analyze the cold data with Amazon Athena</strong></p>\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3\n            using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers\n            pay only for the queries they run. You can use Amazon Athena to process logs, perform ad-hoc analysis, and\n            run interactive queries.</p>\n<p>Moving the data to Amazon S3 glacier will prevent us from being able to query it. Therefore, we should\n            migrate the data to Amazon S3 Standard IA and use Amazon Athena to analyze the cold data.</p>\n<p>Incorrect options:</p>\n<p><strong>Migrate the Amazon Redshift underlying storage to Amazon S3 IA</strong> - Amazon Redshift is a\n            fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage\n            and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which\n            are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or\n            more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more\n            compute nodes. The leader node receives queries from client applications, parses the queries, and develops\n            query execution plans. The leader node then coordinates the parallel execution of these plans with the\n            compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results\n            to the client applications.</p>\n<p>Redshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also\n            ruled out.</p>\n<p><strong>Create a smaller Amazon Redshift Cluster with the cold data</strong> - Creating a smaller cluster\n            with the cold data would not decrease the storage cost of Amazon Redshift, which will only increase with\n            time as we keep on creating data. Therefore this option is ruled out.</p>\n<p><strong>Move the data to Amazon S3 Glacier after 30 days</strong> - Amazon S3 Glacier and Amazon S3 Glacier\n            Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving\n            and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive\n            security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 59",
        "question": "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines.\n        \nWhich of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single\n          tenant?",
        "skipped": true,
        "choices": [
            "Spot Instances",
            "Dedicated Hosts",
            "On-Demand Instances",
            "Dedicated Instances"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Dedicated Instances</strong></p>\n<p>Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's\n            dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically\n            isolated at a hardware level, even if those accounts are linked to a single-payer account. However,\n            Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated\n            Instances.</p>\n<p>A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have\n            visibility and control over how instances are placed on the server.</p>\n<p>Differences between Dedicated Hosts and Dedicated Instances:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q21-i1.jpg\"/>\n            via - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Spot Instances</strong> - A Spot Instance is an unused Amazon EC2 instance that is available for\n            less than the On-Demand price. Your Spot Instance runs whenever capacity is available and the maximum price\n            per hour for your request exceeds the Spot price. Any instance present with unused capacity will be\n            allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of\n            the client and hence is not the correct option.</p>\n<p><strong>Dedicated Hosts</strong> - An Amazon EC2 Dedicated Host is a physical server with Amazon EC2\n            instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software\n            licenses on Amazon EC2 instances. With a Dedicated Host, you have visibility and control over how instances\n            are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right\n            choice for the current requirement.</p>\n<p><strong>On-Demand Instances</strong> - With On-Demand Instances, you pay for the compute capacity by the\n            second with no long-term commitments. You have full control over its lifecycle—you decide when to launch,\n            stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of\n            the costliest instance charges and hence is not the correct answer for current requirements.</p>\n<p>High Level Overview of Amazon EC2 Instance Purchase Options:\n            <img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q21-i2.jpg\"/>\n            via - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a>\n</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 60",
        "question": "A development team has noticed that one of the Amazon EC2 instances has been incorrectly configured with the\n          'DeleteOnTermination' attribute set to True for its root EBS volume.\nAs a Solution's Architect, can you suggest a way to disable this flag while the instance is still running?",
        "skipped": true,
        "choices": [
            "Set the DeleteOnTermination attribute to False using the command line",
            "Set the DisableApiTermination attribute of the instance using the API",
            "The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2\n                    console and then update the flag",
            "Update the attribute using AWS management console. Select the Amazon EC2 instance and then uncheck\n                    the DeleteOnTermination check box for the root EBS volume"
        ],
        "correct_answer_indices": [
            0
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p>When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume\n            determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to\n            True for the root volume and is set to False for all other volume types.</p>\n<p><strong>Set the <code>DeleteOnTermination</code> attribute to False using the command line</strong></p>\n<p>If the instance is already running, you can set <code>DeleteOnTermination</code> to False using the command\n            line.</p>\n<p>Incorrect options:</p>\n<p><strong>Update the attribute using AWS management console. Select the Amazon EC2 instance and then uncheck\n              the <code>DeleteOnTermination</code> check box for the root EBS volume</strong> - You can set the\n            <code>DeleteOnTermination</code> attribute to False when you launch a new instance. It is not possible to\n            update this attribute of a running instance from the AWS console.\n          </p>\n<p><strong>Set the <code>DisableApiTermination</code> attribute of the instance using the API</strong> - By\n            default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To\n            prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination\n            protection for the instance. The <code>DisableApiTermination</code> attribute controls whether the instance\n            can be terminated using the console, CLI, or API. This option cannot be used to control the delete status\n            for the EBS volume when the instance terminates.</p>\n<p><strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2\n              console and then update the flag</strong> - This statement is wrong and given only as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/\">https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 61",
        "question": "A development team is looking for a solution that saves development time and deployment costs for an\n          application that uses a high-throughput request-response message pattern.\nWhich of the following Amazon SQS queue types is the best fit to meet this requirement?",
        "skipped": true,
        "choices": [
            "Amazon Simple Queue Service (Amazon SQS) FIFO queues",
            "Amazon Simple Queue Service (Amazon SQS) delay queues",
            "Amazon Simple Queue Service (Amazon SQS) temporary queues",
            "Amazon Simple Queue Service (Amazon SQS) dead-letter queues"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Amazon Simple Queue Service (Amazon SQS) temporary queues</strong></p>\n<p>Temporary queues help you save development time and deployment costs when using common message patterns\n            such as request-response. You can use the Temporary Queue Client to create high-throughput, cost-effective,\n            application-managed temporary queues.</p>\n<p>The client maps multiple temporary queues—application-managed queues created on demand for a particular\n            process—onto a single Amazon SQS queue automatically. This allows your application to make fewer API calls\n            and have a higher throughput when the traffic to each temporary queue is low. When a temporary queue is no\n            longer in use, the client cleans up the temporary queue automatically, even if some processes that use the\n            client aren't shut down cleanly.</p>\n<p>The following are the benefits of temporary queues:</p>\n<ol>\n<li>\n<p>They serve as lightweight communication channels for specific threads or processes.</p>\n</li>\n<li>\n<p>They can be created and deleted without incurring additional costs.</p>\n</li>\n<li>\n<p>They are API-compatible with static (normal) Amazon SQS queues. This means that existing code that\n                sends and receives messages can send messages to and receive messages from virtual queues.</p>\n</li>\n</ol>\n<p>To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary\n            Queue Client. This client makes it easy to create and delete many temporary messaging destinations without\n            inflating your AWS bill. The key concept behind the client is the virtual queue. Virtual queues let you\n            multiplex many low-traffic queues onto a single Amazon SQS queue. Creating a virtual queue only instantiates\n            a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs\n            associated with creating a virtual queue.</p>\n<p>End-to-end process for sending messages through virtual queues:\n            <img src=\"https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2019/07/26/Selection_015.png\"/>\n            via - <a href=\"https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/\">https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/</a>\n</p>\n<p>Incorrect options:</p>\n<p><strong>Amazon Simple Queue Service (Amazon SQS) dead-letter queues</strong> - Amazon SQS supports\n            dead-letter queues, which other queues (source queues) can target for messages that can't be processed\n            (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system\n            because they let you isolate problematic messages to determine why their processing doesn't succeed. Amazon\n            SQS does not create the dead-letter queue automatically. You must first create the queue before using it as\n            a dead-letter queue.</p>\n<p><strong>Amazon Simple Queue Service (Amazon SQS) FIFO queues</strong> - Amazon SQS FIFO\n            (First-In-First-Out) queues are designed to enhance messaging between applications when the order of\n            operations and events is critical, or where duplicates can't be tolerated. FIFO queues also provide\n            exactly-once processing but have a limited number of transactions per second (TPS).</p>\n<p><strong>Amazon Simple Queue Service (Amazon SQS) delay queues</strong> - Delay queues let you postpone the\n            delivery of new messages to a queue for a number of seconds, for example, when your consumer application\n            needs additional time to process messages. If you create a delay queue, any messages that you send to the\n            queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a\n            queue is 0 seconds. The maximum is 15 minutes.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/\">https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 62",
        "question": "Your application is deployed on Amazon EC2 instances fronted by an Application Load Balancer. Recently, your\n          infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users\n          only make about 5 requests per second.\nHow can you efficiently prevent attackers from overwhelming your application?",
        "skipped": true,
        "choices": [
            "Define a network access control list (network ACL) on your Application Load Balancer",
            "Configure Sticky Sessions on the Application Load Balancer",
            "Use AWS Shield Advanced and setup a rate-based rule",
            "Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Use an AWS Web Application Firewall (AWS WAF) and setup a rate-based rule</strong></p>\n<p>AWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect your web\n            applications or APIs against common web exploits that may affect availability, compromise security, or\n            consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by\n            enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site\n            scripting, and rules that filter out specific traffic patterns you define.</p>\n<p>The correct answer is to use WAF (which has integration on top of your ALB) and define a rate-based rule.\n          </p>\n<p>Incorrect options:</p>\n<p><strong>Configure Sticky Sessions on the Application Load Balancer</strong> - Application Load Balancer\n            (ALB) operates at the request level (layer 7), routing traffic to targets – Amazon EC2 instances,\n            containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load\n            balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at\n            delivery of modern application architectures, including microservices and container-based applications.</p>\n<p>Sticky Sessions on your Application Load Balancer is a distractor here. Sticky sessions are a mechanism to\n            route requests from the same client to the same target. Application Load Balancer supports sticky sessions\n            using load balancer generated cookies. If you enable sticky sessions, the same target receives the request\n            and can use the cookie to recover the session context.</p>\n<p><strong>Define a network access control list (network ACL) on your Application Load Balancer</strong> - A\n            network access control list (network ACL) does not work, as this only helps to block specific IPs. On top of\n            things, network access control list (network ACL) is defined at the subnet level, and not for an Application\n            Load Balancer.</p>\n<p><strong>Use AWS Shield Advanced and setup a rate-based rule</strong> - AWS Shield is a managed Distributed\n            Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides\n            always-on detection and automatic inline mitigations that minimize application downtime and latency, so\n            there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield -\n            Standard and Advanced.</p>\n<p>AWS Shield Advanced provides enhanced resource-specific detection and employs advanced mitigation and\n            routing techniques for sophisticated or larger attacks.</p>\n<p>AWS Shield Advanced will give you DDoS protection overall, and you cannot set up rate-based rules in\n            Shield.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html</a>\n</p>\n<p><a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 63",
        "question": "The development team at a company manages a Python based nightly process with a runtime of 30 minutes. The\n          process can withstand any interruptions in its execution and start over again. The process currently runs on\n          the on-premises infrastructure and it needs to be migrated to AWS.\nWhich of the following options do you recommend as the MOST cost-effective solution?",
        "skipped": true,
        "choices": [
            "Run on Amazon EMR",
            "Run on AWS Lambda",
            "Run on an Application Load Balancer",
            "Run on a Spot Instance with a persistent request type"
        ],
        "correct_answer_indices": [
            3
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>Run on a Spot Instance with a persistent request type</strong></p>\n<p>A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price.\n            Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower\n            your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The\n            request type (one-time or persistent) determines whether the request is opened again when Amazon EC2\n            interrupts a Spot Instance or if you stop a Spot Instance. If the request is persistent, the request is\n            opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot\n            Instance, the request only opens after you start your Spot Instance.</p>\n<p>Incorrect options:</p>\n<p><strong>Run on an Application Load Balancer</strong> - Application Load Balancer operates at the request\n            level (layer 7), routing traffic to targets – Amazon EC2 instances, containers, IP addresses, and AWS Lambda\n            functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic,\n            Application Load Balancer provides advanced request routing targeted at delivery of modern application\n            architectures, including microservices and container-based applications.</p>\n<p>Application Load Balancer helps distribute load for HTTP(S) requests. This option has been added as a\n            distractor.</p>\n<p><strong>Run on Amazon EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for\n            processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase,\n            Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your\n            data and processing across a resizable cluster of Amazon EC2 instances.</p>\n<p>Amazon EMR is to run Big Data load that is meant to be run on Hadoop, this is also a distractor.</p>\n<p><strong>Run on AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers.\n            You pay only for the compute time you consume.</p>\n<p>AWS Lambda would be the perfect fit if our script could run in less than 15 minutes, as this is the maximum\n            timeout for AWS Lambda.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html</a>\n</p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 64",
        "question": "A Hollywood production studio is looking at transferring their existing digital media assets of around 20\n          petabytes to AWS Cloud in the shortest possible timeframe.\nWhich of the following is an optimal solution for this requirement, given that the studio's data centers are\n          located at a remote location?",
        "skipped": true,
        "choices": [
            "AWS Snowball",
            "AWS Storage Gateway",
            "AWS Snowmobile",
            "AWS Direct Connect"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p><strong>AWS Snowmobile</strong></p>\n<p>AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to\n            AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a\n            semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video\n            libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile\n            is more secure, fast, and cost-effective. AWS recommends using Snowmobile to migrate large datasets of 10PB\n            or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should\n            use Snowball.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS Snowball</strong> - The AWS Snowball service uses physical storage devices to transfer large\n            amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location\n            at faster-than-internet speeds. Snowball provides powerful interfaces that you can use to create jobs, track\n            data, and track the status of your jobs through to completion. AWS recommends snowball only if you want to\n            transfer greater than 10 TB of data between your on-premises data centers and Amazon S3.</p>\n<p><strong>AWS Storage Gateway</strong> - AWS Storage Gateway is a hybrid cloud storage service that gives you\n            on-premises access to virtually unlimited cloud storage. Used for key hybrid storage solutions that include\n            moving tape backups to the cloud, reducing on-premises storage with cloud-backed file shares, providing low\n            latency access to data in AWS for on-premises applications, as well as various migration, archiving,\n            processing, and disaster recovery use cases. This is not an optimal solution since the studio's data centers\n            are in remote locations where internet speed may not optimal, thereby increasing both cost and time for\n            migrating 20TB of data.</p>\n<p><strong>AWS Direct Connect</strong> - AWS Direct Connect is a network service that provides an alternative\n            to using the Internet to connect a customer’s on-premises sites to AWS. Data is transmitted through a\n            private network connection between AWS and a customer’s datacenter or corporate network. Direct Connect\n            connection takes significant cost as well as time to provision. This is not the correct solution since the\n            studio wants the data transfer to be done in the shortest possible time.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/snowmobile/\">https://aws.amazon.com/snowmobile/</a></p>\n</div>\n</div>"
    },
    {
        "question_number": "Question 65",
        "question": "As a Solutions Architect, you have set up a database on a single Amazon EC2 instance that has an Amazon EBS\n          volume of type gp2. You currently have 300 gigabytes of space on the gp2 device. The Amazon EC2 instance is of\n          type m5.large. The database performance has recently been poor and upon looking at Amazon CloudWatch, you\n          realize the IOPS on the Amazon EBS volume is maxing out. The disk size of the database must not change because\n          of a licensing issue.\nHow do you troubleshoot this issue?",
        "skipped": true,
        "choices": [
            "Convert the Amazon EC2 instance to an i3.4xlarge",
            "Stop the Amazon CloudWatch agent to improve performance",
            "Convert the gp2 volume to an io1",
            "Increase the IOPS on the gp2 volume"
        ],
        "correct_answer_indices": [
            2
        ],
        "explanation_html": "<div class=\"mc-quiz-question--explanation--Thrjf\">\n<h4 class=\"ud-heading-md\">Explanation</h4>\n<div class=\"rt-scaffolding\" data-purpose=\"safely-set-inner-html:rich-text-viewer:html\" id=\"question-explanation\">\n<p>Correct option:</p>\n<p>Amazon EBS provides the following volume types, which differ in performance characteristics and price so\n            that you can tailor your storage performance and cost to the needs of your applications. The volumes types\n            fall into two categories:</p>\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with\n            small I/O size, where the dominant performance attribute is IOPS</p>\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better\n            performance measure than IOPS</p>\n<p><strong>Convert the gp2 volume to an io1</strong></p>\n<p>Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly\n            database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a\n            bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate\n            when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.\n          </p>\n<p>The only solution is to convert the volume into an io1 volume. This will allow us to keep the same disk\n            size while independently increasing the IOPS for that volume.</p>\n<p>Incorrect options:</p>\n<p><strong>Stop the Amazon CloudWatch agent to improve performance</strong> - The Amazon CloudWatch agent does\n            not have any impact on the performance of the instance.</p>\n<p><strong>Increase the IOPS on the gp2 volume</strong> - General Purpose SSD (gp2) volumes offer\n            cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit\n            millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100\n            IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance\n            scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver their provisioned\n            performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.</p>\n<p>IOPS cannot be directly increased on a gp2 volume without increasing its size, which is not possible due to\n            the question's constraints.</p>\n<p><strong>Convert the Amazon EC2 instance to an i3.4xlarge</strong> - Converting the Amazon EC2 instance to\n            i3.4xlarge won't improve the Amazon EBS drive's performance.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2</a>\n</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops</a>\n</p>\n</div>\n</div>"
    }
]